\chapter{Technical Background}
\label{sec:background}
%
In this chapter bla bla
\todo{Describe Chapter}


% ===========================================
% ===========================================
% Docker
% ===========================================
% ===========================================
\section{Docker}
\todo{Mehr zum Thema DevOps}
% Short docker intro
Docker is an open-source platform that enables containerization of applications. Containerization is a technology to package, ship and run applications and their environment in individual containers.
% Make clear
Docker is not a container technology itself, it hides the complexity of working with container technologies directly and instead provides an abstraction and the tools to work with containers \cite{Nickoloff2019Docker, Bullington2020Docker, Potdar2020Docker}.


\subsection{Docker architecture}

% Architecture figure
\begin{figure}[h]%
\centering
\includegraphics[scale=0.5]{images/04_technical_background/docker_architecture}%
\caption{Docker architecture - Source: Authors own model, based on \cite{Docker2020Docs}.}%
\label{fig:spark_docker_architecture}%
\end{figure}

% Explain figure
\Fig{fig:spark_docker_architecture} illustrates the client server architecture of Docker which consists of of a Docker client, the Docker deamon and a registry.


\paragraph{Docker client:} The Docker client is an interface for the user to send commands to the Docker deamon \cite{Docker2020Docs}.


\paragraph{Docker deamon:} The Docker deamon manages all containers running on the host system and handles the containers resources, networks and volumes \cite{Bullington2020Docker}.


\paragraph{Docker Registry:} A Docker registry stores images. Images can be pushed to a public or private registry and pulled from it to build a container \cite{Docker2020Docs}.


\subsection{Docker I mage}
% What is an image
An Image is a snapshot of the environment that is needed to run an application in a Docker container. The environment consists of all files, libraries and configurations that are needed for the application to run properly \cite{Docker2020Docs, Nickoloff2019Docker}.
% How to create an image
Images can be created from existing containers or from executing a build script called Dockerfile. A Dockerfile is a text file consisting of instructions for building an image. The Docker image builder executes the instructions of a Dockerfile from top to bottom \cite{Nickoloff2019Docker}.

% Dockerfile example
\begin{lstlisting}[frame=single, label=lst:docker_dockerfile, caption=Example of a Dockerfile, captionpos=b]
FROM ubuntu:latest

RUN apt-get update && apt-get install -y git

ENTRYPOINT ["git"]
\end{lstlisting}

% Example Dockerfile example
\Lst{lst:docker_dockerfile} provides an example of a Dockerfile with three instructions.
\begin{enumerate}
\item \texttt{FROM ubuntu:latest} - This image is build on top of the latest Ubuntu image. Dockerfiles have to start with a \texttt{FROM} instruction \cite{Nickoloff2019Docker}.
\item \texttt{RUN apt-get update \&\& apt-get install -y git} - Update the package manager and install Git.
\item \texttt{ENTRYPOINT ["git"]} - Set the git command as the entrypoint of this image.
\end{enumerate}

% How to build an image
\todo{Vielleicht noch erkl√§ren wie Images mit Docker build erstellt werden.}


\subsection{Docker Container}
% What are containers
A container is an execution environment running on the host-system kernel.

% Container image
\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{images/04_technical_background/docker_basic_container_structure}
\caption{Docker basic container structure - Source: Authors own model, based on \cite{Bullington2020Docker}.}
\label{fig:docker_container_struct}
\end{figure}

% Container advantages
The advantage of a container is its lightweight nature. As illustrated in \Fig{fig:docker_container_struct}, containers take advantage of OS-level virtualization instead of hardware-virtualization without the need of a hypervisor \cite{Docker2020Docs, Nickoloff2019Docker}. Containers share the resources of the host-system instead of using reserved resources \cite{Bullington2020Docker}. Multiple containers can run on the host-system kernel and are by default isolated from each other \cite{Docker2020Docs}.
% Docker container
In Docker, a container is a runnable unit of an image and is used for distributing and testing applications. A container can be configured to expose certain resources to the host system, e.g. network ports \cite{Bullington2020Docker}.


\subsection{Docker Swarm Mode}
\label{subsec:04_docker_swarm}
% What is Docker swarm
Docker Swarm mode is the native cluster orchestration and management tool embedded in the Docker engine.
% What is a swarm
In Docker Swarm mode, a cluster of multiple nodes is called a swarm. All nodes run in Swarm mode and act as managers or workers.
% Whats the purpose
In a swarm, multiple services can be deployed. The manager node is responsible to maintain the desired state of a service \cite{Docker2020Docs}.


% Why
Docker Swarm mode will be used to create a self-healing and a self-adapting environment as described in \Sec{sec:02_autonomic_computing}.
% 
Many properties of Docker Swarm mode contribute the fact that it is an ideal candidate to create self-healing and self-adapting environment:
\begin{itemize}
\item \textbf{Desired state:} The manager node monitors the state of each service in the swarm and adapts the environment to maintain the desired state \cite{Docker2020Docs}.

\item \textbf{Cluster management and orchestration:} Docker Swarm mode is integrated with the Docker engine. A swarm can be created and managed using the Docker CLI \cite{Docker2020Docs}.

\item \textbf{Service model:} The Docker engine allows to define the desired state of a service. The manager node maintains the desired state of all services in the swarm \cite{Docker2020Docs}.

\item \textbf{Scaling:} The number of replicas can be defined for each service. The manager node will automatically adapt the number of replicas for a service to keep the desired state \cite{Docker2020Docs}.

\item \textbf{Multi-host networking:} A swarm runs all services in an overlay network. New services will automatically be added to the overlay network \cite{Docker2020Docs}.
\end{itemize}


\subsubsection{Nodes}
% What is a node
A Docker engine participating in the swarm is called a node.
% What about the node role
Nodes can act as manager nodes, worker nodes or both \cite{Docker2020Docs}.


% What about the manager
The manager node is responsible for cluster orchestration an management. It maintains the desired state of all services and tasks in the swarm. In addition, the manager node dispatches tasks to worker nodes when service definitions will be submitted to the manager node \cite{Docker2020Docs}.


% What about worker
Worker nodes are responsible to execute the tasks receives by the manager node. While performing the tasks, the worker node notifies the manager node about the tasks state \cite{Docker2020Docs}.


\subsubsection{Services and Tasks}
% What is a service
A services defines the desired state of a task. The state is defined by the number of replicas of a service and the configuration for the Docker container, e.g. Docker Image, resources, network, and more \cite{Docker2020Docs}.


%What is a node
A task is a running Docker container. The task is defined by the corresponding service and will be managed by the manager node. A task can be performed on worker and manager nodes \cite{Docker2020Docs}.


% ===========================================
% ===========================================
% Apache Spark
% ===========================================
% ===========================================
\section{Apache Spark}
\label{sec:04_spark}
Apache Spark is an open-source computing framework for parallel data processing on a large computer cluster. Spark manages the available resources and distributes computation tasks across a cluster to perform big-data processing operations at large scale \cite{Chambers2018Spark}. Before Spark was developed, Hadoop MapReduce \cite{Dean2010MapReduce} was the framework of choice for parallel operations on a computer cluster \cite{Zaharia2010Spark}. Spark accomplished to outperform Hadoop by 10x for iterative Machine Learning \cite{Zaharia2010Spark}. It is implemented in Scala\footnote{Scala programming language. https://www.scala-lang.org/}, a JVM-based language and provides a programming interface for Scala, Java\footnote{Java programming language. https://www.oracle.com/java/}, Python\footnote{Python programming language. https://www.python.org/} and R\footnote{R programming language. https://www.r-project.org/}. In addition, Spark includes an interactive SQL shell and libraries to implement Machine Learning and streaming applications \cite{Chambers2018Spark}.
It was developed in 2009 as the Spark research project at UC Berkeley and became an Apache Software Foundation project in 2013 \cite{Chambers2018Spark}. 


\subsection{Spark programming model}
\label{subsec:04_spark_pr-model}
% Explain RDDs
Spark provides resilient distributed datasets (RDDs) as the main abstraction for parallel operations \cite{Zaharia2010Spark}. Core types of Spark`s higher-level structured API are built on top of RDDs \cite{Chambers2018Spark} and will automatically be optimized by Spark`s Catalyst optimizer to run operations quick and efficient \cite{Hien2018Spark}.
\todo{Master-Slave Architektur + Bild}


\subsubsection{Resilient distributed datasets}
% What are RDDs
Resilient distributed datasets are fault-tolerant, parallel data structures to enable data sharing across cluster applications \cite{Zaharia2012RDDs}. They allow to express different cluster programming models like MapReduce, SQL and batched stream processing \cite{Zaharia2012RDDs}. RDDs have been implemented in Spark and serve as the underlying data structure for higher level APIs (Spark structured API) \cite{Zaharia2012RDDs}.
% How can RDDs be used in applications    !! HIER MAL BEISPIEL WIE IN RDD ERZEUGT WIRD
RDD`s are a immutable, partitioned collection of records and can only be initiated through transformations (e.g. map, filter) on data or other RDD`s.
% What are the advantages of RDDs    !!! HIER NOCHMAL BEISPIEL WIE DAS AUSSEHEN KANN
An advantage of RDDs is, that they can be recovered through lineage. Lost partitions of an RDD can be recomputed from other RDDs in parallel on different nodes \cite{Zaharia2012RDDs}. 
% Better use structured API
RDDs are lower level APIs and should only be used in applications if custom data partitioning is needed \cite{Chambers2018Spark}. It is recommended to use Sparks structured API objects instead. Optimizations for RDDs have to be implemented manually while Spark automatically optimize the execution for structured API operations \cite{Chambers2018Spark}.


\subsubsection{Spark structured API}
% What is the structured API
Spark provides high level structured APIs for manipulating all kinds of data. The three distributed core types are Datasets, DataFrames and SQL Tables and Views \cite{Chambers2018Spark}.
% About DataFrames, Datasets and SQL
Datasets and DataFrames are immutable, lazy evaluated collections that provide execution plans for operations \cite{Chambers2018Spark}. SQL Tables and Views work the same way as DataFrames, except that SQL is used as the interface instead of using the DataFrame programming interface \cite{Chambers2018Spark}.
% Differences between Datasets ad Dataframes
Datasets use JVM types and are therefore only available for JVM based languages. DataFrames are Datasets of type Row, which is the Spark internal optimized format for computations. This has advantages over JVM types which comes with garbage collection and object instantiation \cite{Chambers2018Spark}.


\subsubsection{Spark Catalyst}
\label{subsubsec:04_spark_pr-model_catalyst}
% What is the Catalyst optimizer
Spark also provides a query optimizer engine called Spark Catalyst. \Fig{fig:04_spark_pr-model_catalyst_process} illustrates how the Spark Catalyst optimizer automatically optimizes Spark applications to run quickly and efficient.
% Short How
Before executing the user`s code, the Catalyst optimizer translates the data-processing logic into a logical plan and optimizes the plan using heuristics \cite{Hien2018Spark}. After that, the Catalyst optimizer converts the logical plan into a physical plan to create code that can be executed \cite{Hien2018Spark}.


% What are logical plans
Logical plans get created from a DataFrame or a SQL query. A logical plan represents the data-processing logic as a tree of operators and expressions where the Catalyst optimizer can apply sets of rule-based and cost-based optimizations \cite{Hien2018Spark}.
For example, the Catalyst can position a filter transformation in front of a join operation \cite{Hien2018Spark}.

% What are physical plans
From the logical plan, the Catalyst optimizer creates one ore more physical plans which consist of RDD operations \cite{Chambers2018Spark}. The cheapest physical will be generated into Java bytecode for execution across the cluster \cite{Hien2018Spark}.

% Spark Catalyst figure
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{images/04_technical_background/spark_catalyst}
\caption{Optimization process of the Spark Catalyst - Source: Authors own model, based on \cite{Hien2018Spark}.}
\label{fig:04_spark_pr-model_catalyst_process}
\end{figure}
\todo{Bild nochmal machen mit abstand	+ QUELLE anpassen}


\subsection{Spark application architecture}

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{images/04_technical_background/cluster_overview}
\caption{Overview of a Spark cluster architecture - Source: Authors own model, based on \cite{Apache2020Spark}.}
\label{fig:spark_cluster_overview}
\end{figure}

\todo{Lieber Doch Master/Slave ???. Was ist mit dem CLuster-Manager}
% Explain image
\Fig{fig:spark_cluster_overview} illustrates the main architecture of a Spark cluster. The architecture follows the master-worker model where the Spark driver is the master and the Spark executors are the worker \cite{Hien2018Spark}.
\todo{Nicht ganz richtig, Master und Worker sind machines und driver und executor sind prozesse}

\paragraph{Spark driver:}
The Spark driver is a JVM process on a physical machine and responsible to maintain the execution of a Spark application \cite{Chambers2018Spark}. It coordinates the application tasks onto each available executor \cite{Hien2018Spark}. To get launch executors and get physical resources, the Spark driver interacts with the cluster manager \cite{Chambers2018Spark, Hien2018Spark}.


\paragraph{Spark Executor:}
A Spark executor performs the tasks given by the Spark driver \cite{Chambers2018Spark}. It runs as a JVM process and runs until the Spark application finishes \cite{Hien2018Spark}. After the executor finishes, it reports back to the Spark driver \cite{Chambers2018Spark}. Each task will be performed on a separate CPU core to enable parallel processing  \cite{Hien2018Spark}.


\paragraph{Cluster manager:}
The cluster manager is an external service that orchestrates the work between the machines in the cluster \cite{Hien2018Spark, Apache2020Spark}. The cluster manager knows about the resources of each worker and decides on which machine the Spark driver process and the executor processes run \cite{Hien2018Spark, Chambers2018Spark}.
% Different cluster types
Spark supports different services that can run as the cluster manager: Standalone, Apache Mesos\footnote{Apache Mesos. https://mesos.apache.org/}, Hadoop YARN\cite{Murthy2013Yarn} and Kubernetes\footnote{Kubernetes. https://kubernetes.io/} \cite{Apache2020Spark}.
% What is standalone
\todo{Explain standalone}
% Explain different cluster deploy modes
The cluster manager provides three different deploy modes for acquiring resources in the cluster.
\begin{itemize}
\item Cluster mode
\item Client mode
\item Local mode
\end{itemize}

% Cluster mode
To run an application in cluster mode, the user has to submit a precompiled JAR, python script or R script to the cluster manager \cite{Chambers2018Spark}. After that, the cluster manager starts the driver process and executor processes exclusively for the Spark application on machines inside the cluster \cite{Chambers2018Spark, Hien2018Spark}.
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{images/04_technical_background/cluster_mode}
\caption{Spark`s cluster mode - Source: Authors own model, based on \cite{Chambers2018Spark}.}
\label{fig:spark_cluster_mode}
\end{figure}

% Client mode
The difference between the client mode and the cluster mode is that, the driver process runs on the client machine outside of the Spark cluster \cite{Chambers2018Spark}.
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{images/04_technical_background/client_mode}
\caption{Spark`s client mode - Source: Authors own model, based on \cite{Chambers2018Spark}.}
\label{fig:spark_client_mode}
\end{figure}

% Local mode
The local mode starts a Spark application on a single computer \cite{Chambers2018Spark}. It is not recommended to use the local mode in production, instead it should be used for testing Spark applications or learning the Spark framework \cite{Chambers2018Spark}.


\subsection{Spark application implementation}
% Short intro
The concept of a Spark application consists of calling transformations and actions. A transformation creates a DataFrame or a Dataset, the logical data structures of a Spark application. The computation of a Spark application gets processed when an action gets called in the application. The transformations of a Spark application build up a directed acyclic graph (DAG) of instructions. By calling an action, the DAG will break down into stages and tasks to create a single job for execution \cite{Chambers2018Spark}.
% Show example application
\begin{lstlisting}[frame=single, label=lst:spark_python_example, caption=Example of a Python3 Spark application, captionpos=b]
# Initialize a SparkSession
sparkSession = SparkSession\
    .builder\
    .getOrCreate()

# Create a dataframe with a transformation
dataframe = sparkSession.range(1, 1000)
# Apply another transformation
dataframe = dataframe.filter(dataframe.id % 2 == 0)
# Call an action
count = dataframe.count()
\end{lstlisting}
% Explain a spark application on top of an example
\Lst{lst:spark_python_example} demonstrates an example implementation of a Spark application. At first a SparkSession gets initialized. Each Spark application must include a SparkSession to initialize the application driver and executors  \cite{Chambers2018Spark}. In Addition, the SparkSession provides an API for data-processing logic and configuration of the Spark application \cite{Hien2018Spark}. After that, a DataFrame gets created with the range transformation to include each number from 1 to 1000 in the DataFrame. Next, a filter transformation is applied on the DataFrame to sort out any odd number. At the end, the number of rows gets saved in a variable with the count action.
\todo{Create listing macro}



% How to submit
\todo{Tabelle von Luu18 vll als Anhang(Table 2-1 Subdirectories Inside the spark-2.1.1-bin-hadoop2.7 Directory)}
The Spark framework provides a spark-submit executable to launch a Spark application inside a cluster.
% Show example submit
\begin{lstlisting}[frame=single, label=lst:spark_submit_example, caption=Execution of a Spark Python application using the spark-submit executable, captionpos=b]
$SPARK_HOME/bin/spark-submit \
    --master spark://spark-master:7077 \
    application.py
\end{lstlisting}
% Explain
\Lst{lst:spark_submit_example} provides an example how the spark-submit executable can be used to launch a Spark Python application.


\subsection{Spark standalone cluster deployment}
% Why only standalone
\todo{Why only standalone}
% What is a standalone cluster
The standalone mode is a basic cluster-manager build specifically for Spark. It is developed to only run Spark but supports workloads at large scale \cite{Chambers2018Spark}.

% How can we create a cluster
Spark provides build-in scripts to start a master node and worker nodes in standalone mode. ABC demonstrates how a master node and worker node gets launched using the build-in scripts.
% NE ALTER !, schon erw√§hnen start-master und start-worker


% ===========================================
% ===========================================
% NVIDIA RAPIDS
% ===========================================
% ===========================================
\section{RAPIDS accelerator for Apache Spark}
\label{sec:04_rapids}
% Explain
RAPIDS accelerator for Apache Spark is a plugin suite that aims to accelerate computing operation for Apache Spark using GPUs. It is available for Apache Spark 3 \cite{SparkRapids2020Docs}.
% A little how
The plugin uses the RAPIDS\footnote{Open GPU Data Science - \url{https://rapids.ai/}, accessed: 01-01-2021} libraries to extend the Apache Spark programming model, introduced in \SubSec{subsec:04_spark_pr-model} \cite{SparkRapids2020Docs, Mcdonald2020SparkRapids}.


\subsection{Extension of the Spark programming model}
\label{subsec:04_rapids_ext}
% How does it extends DataFrame and SQL
\paragraph{}The plugin suite extends the Spark programming model with a new DataFrame based on Apache Arrow\footnote{Arrow. A cross-language development platform for in-memory data - \url{https://arrow.apache.org/}, accessed: 2020-12-03} data structures. In addition, the Catalyst optimizer (described in \SubSec{subsubsec:04_spark_pr-model_catalyst}) is extended to generate GPU-aware query plans \cite{Mcdonald2020SparkRapids}.
% What is Apache Arrow
Apache arrow is a data platform to build high performance applications that work with large dataset`s and to improve analytic algorithms. A component of Apache Arrow is the Arrow Columnar Format, an in-memory data structure specification for efficient analytic operations on GPUs and CPUs \cite{ApacheArrow2020Docs}.


% RAPIDS query plan image
\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{images/04_technical_background/rapids_query_plan}
\caption{Catalyst optimization with RAPIDS accelerator for Apache Spark - Source: Authors own model, based on \cite{Mcdonald2020SparkRapids}.}
\label{fig:04_rapids_ext_query-plan}
\end{figure}
\paragraph{}\Fig{fig:04_rapids_ext_query-plan} illustrates how the RAPIDS plugin suite extends the Catalyst optimization process illustrated previously in \Fig{fig:04_spark_pr-model_catalyst_process}.
% Identifiy operators
% How does the Catalyst create GPU aware plans
The Spark Catalyst optimizer identifies operator in a query plan that are supported by the RAPIDS APIs. To execute the query plan, these operators can be scheduled on a GPU within the Spark cluster \cite{Mcdonald2020SparkRapids}. If operators are not supported by the RAPIDS APIs, a physical plan for CPUs will be generated by the Catalyst optimizer to execute RDD operations \cite{Mcdonald2020SparkRapids}.


\subsection{Installation}



% ===========================================
% ===========================================
% PROMETHEUS
% ===========================================
% ===========================================
\section{Prometheus}
% What is Prometheus
Prometheus is an open-source monitoring and alerting system \cite{Prom2020Docs}.
% Time-series format + PromQL
To collect and store data, Prometheus supports a multi-dimensional key-value pair based data model, according to \SubSec{subsec:02_monitoring_db_multi-metrics}, which can be analyzed in real-time using the PromQL query language \cite{Pandey2020Monitoring}.
% Pull-based approach
Prometheus follows the pull-based approach, as described in detail in \SubSec{subsec:02_monitoring_push-pull}, to scrape metrics from hosts and services \cite{Bastos2019Prom}.


\subsection{Prometheus architecture}

\todo{Bilder f√ºr components}

% Architecture image
\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{images/04_technical_background/prom_architecture}
\caption{Prometheus high-level architecture - Source: Authors own model, based on \cite{Prom2020Docs, Bastos2019Prom}.}
\label{fig:prom_architecture}
\end{figure}

% High level architecture
\Fig{fig:prom_architecture} illustrates the high-level architecture of Prometheus.
% Description of components
The Prometheus ecosystem provides multiple components. Components can be optional, depending on the monitoring needs of the environment \cite{Bastos2019Prom}. The main components of a Prometheus system are Prometheus server, Alertmanager, service discovery, exporters, Pushgateway and visualization tools \cite{Prom2020Docs}.


\paragraph{Prometheus server:} 
The Prometheus server is the main component of a Prometheus system. It is responsible to collect metrics as time-series data from targets and stores the collected data in the built-in TSDB \cite{Bastos2019Prom}. Prometheus uses the concept of scraping to collect metrics from a target. A target host has to expose an endpoint to make metrics available in the Prometheus data format \cite{Pandey2020Monitoring}. Additionally, the Prometheus server triggers alerts to the Alertmanager if a configured condition becomes true \cite{Prom2020Docs}.


\paragraph{Alertmanager:}
If an alerting rule becomes true, the Prometheus server generates an alert and pushes it to the Alertmanager. The Alertmanager generates notifications from the received alerts. A notification can take multiple forms like emails or chat messages. Webhooks can be implemented to trigger custom notifications \cite{Bastos2019Prom}.


\paragraph{Service discovery:}
As mentioned before, Prometheus follows a pull-based approach to fetch metrics from a target. To know about all targets, Prometheus needs a list of the corresponding hosts. The service discovery manages the complexity of maintaining a list of hosts manually in an changing infrastructure \cite{Bastos2019Prom}.


\paragraph{Exporters:}
If an application does not support an endpoint for Prometheus, an exporter can be used to fetch metrics and make them available to the Prometheus server. An exporter is a monitoring agent running on a target host that fetches metric from the host and exports them to the Prometheus server \cite{Pandey2020Monitoring}.


\paragraph{Pushgateway:}
If a target is not designed to be scraped, metrics can be pushed against the Pushgateway\cite{Prom2020Docs}. The Pushgateway converts the data into the Prometheus data format and passes them to the Prometheus server \cite{Pandey2020Monitoring}.


\paragraph{Visualization:}
Prometheus supports various tools for virtualization of the scraped data. Grafana\footnote{Grafana: The open observability platform - https://grafana.com/} is one of the widely used tools for this occasion.


% ===========================================
% ===========================================
% CADVISOR
% ===========================================
% ===========================================
\section{cAdvisor}
% What is cAdvisor
Container Advisor (cAdvisor) is a running deamon that collects, aggregates, analyses and exposes performance metrics from running containers.
% How to run
It has native support for Docker container and is deployed as a Docker container.
% How it collects
cAdvisor collects metrics from the container deamon and Linux cgroups.
% How it exposes
Collected metrics will be exposed in the Prometheus file format \cite{Bastos2019Prom, cadvisor2020Docs}.


% ===========================================
% ===========================================
% Gitlab CI/CD
% ===========================================
% ===========================================
\section{GitLab CI/CD}
GitLab CI/CD is a tool integrated into the GitLab platform that enables Continuous Integration (CI), Continuous Delivery (CD) and Continuous Deployment (CD) for software development.
% Gitlab platform
The GitLab platform integrates many development features like Git repository management and CI/CD.
% How it works
By pushing code changes to the codebase, GitLab CI/CD executes a pipeline of scripts to automate CI and CD processes of the software development cycle.
% CI
A CI pipeline will consist of scripts that builds, tests and validate the updated codebase.
% CD
A CD pipeline is responsible to deploy the application for production after the CI pipeline has executed successfully.
% Summary
Adding CI/CD pipelines to the software development cycle of an application, allows to catch bugs an errors early. This ensures that an application deployed to production will conform to established standards \cite{Gitlab2020Docs}.

\subsection{CI/CD Pipeline}

The fundamental component of GitLab CI/CD is called a pipeline. Pipelines will perform based on conditions. A conditions might be a push to the main branch of the repository \cite{Gitlab2020Docs}. A pipeline comprises two components:

\begin{itemize}
\item \textbf{Stages:} A stage consists of one or multiple jobs that run in parallel. Furthermore, a stage defines how jobs will be executed. For example, a build stage only performs after a test stage has performed successfully \cite{Gitlab2020Docs}.

\item \textbf{Jobs:} Jobs are responsible to perform the scripts defined by administrators. The scripts define necessary actions. For example compiling the source code or performing tests \cite{Gitlab2020Docs}.
\end{itemize}


\subsubsection{Configuration}
% How to configurate a pipeline (.gitlab-ci.yml)
GitLab CI/CD is configured by a \textit{.gitlab-ci.yml} file. It is necessary that this file is located in the repositories root directory.
% What does this file
The configuration file will create a pipeline that performs after a push to the codebase \cite{Gitlab2020Docs}.


\subsubsection{Basic Architecture}

\begin{figure}[h]
\centering
\includegraphics[scale=1]{images/04_technical_background/gitlab/basic-pipeline-architecture}
\caption{Basic architecture of a GitLab CI/CD pipeline - Source: Authors own model, based on \cite{Gitlab2020Docs}.}
\label{fig:gitlab-basic_pipeline}
\end{figure}


\subsection{Job Execution}
% Runner pick up jobs
Jobs which are defined in the configuration file will be performed by GitLab runners.
% What is runner
A GitLab runner is an agent that performs the jobs in its own environment and responds the result back to the GitLab instance. A runner is a lightweight and highly scalable application that runs on a server and performs one or multiple executors.
% WHat is an executor
An executor provides the environment where jobs will be executed in. GitLab runner provides multiple variants of executors.
% The docker executor
For example the Docker executor that connects to the underlying Docker engine. In addition, the Docker executor performs a job in a separate and isolated Docker container.
% Runner config
GitLab runner can be set up only for specific projects or be available to all project on the GitLab platform \cite{Gitlab2020Docs}.


% ===========================================
% ===========================================
% K-MEANS
% ===========================================
% ===========================================
\section{K-MEANS}


% ===========================================
% ===========================================
% Bayes
% ===========================================
% ===========================================
\section{Naive Bayes Classifier}


% ===========================================
% ===========================================
% Scaling heat
% ===========================================
% ===========================================
\section{Scaling Heat}


% ===========================================
% ===========================================
% KHP
% ===========================================
% ===========================================
\section{KHP}