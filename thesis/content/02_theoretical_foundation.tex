\chapter{Theoretical Foundation}
\label{chap:02_foundation}


This chapter provides the theoretical foundation about concepts that will be used in this thesis. The theoretical foundation is necessary to understand the choice of tools and the introduced solution in this thesis to create the research objective, answer the research questions, and solve the problem statements.


\section{Scalability}
\label{sec:02_foundations_scalability}
% What is scalability
Scalability defines the ability of a computing system to handle an increasing amount of load \cite{Farcic2017Toolkit21}. 
% Vertical and horizontal
To increase the scalability of a system, different approaches exist. The two main approaches are vertical scaling and horizontal scaling.
% Only horizontal
During this thesis work, the horizontal scaling approach is being used because of its suitability to solve this thesis problems and therefore being introduced in this section.
% Vertical
Vertical scaling will not be used due to its limitations explained in \Sec{subsec:02_foundations_scalability_limits}.


\subsection{Horizontal Scaling}
% What is horizontal scaling
Horizontal scaling is accomplished by duplicating nodes in the computing environment \cite{Wilder2012CloudPatterns}.
% DIstribution
Duplicating the nodes in a computing environment increases the computational capacity of the environment. In addition, the workload can be distributed across all nodes to handle and balance an increasing workload \cite{Wilder2012CloudPatterns, Abbott2015ScalabilityArt}. 
% Homogeneous nodes
Horizontal scaling is more efficient with homogeneous nodes. Homogeneous nodes add the same amount of computing power to the system and are able to perform the same work and response as other nodes. With non-homogeneous nodes, capacity planing and distributing workload becomes more complex \cite{Abbott2015ScalabilityArt}.


\subsection{Limitations of Vertical Scaling}
\label{subsec:02_foundations_scalability_limits}
The limit of scalability is reached, when a computing system is not able to serve the requests of its concurrent users \cite{Wilder2012CloudPatterns}.
% hardware capacity
If the scalability of a computing environment is reached, an option is to add more powerful hardware resources to the system. This approach is called vertical scaling. Adding more powerful hardware can become unaffordable or a point can be reached, where more powerful hardware is not available.
% Downtime
Additionally, increasing the physical capacity of a system requires a restart. Most of the time restarting is not possible, because it will interrupt important services running on the system \cite{Wilder2012CloudPatterns}.


% ===========================================
% ===========================================
\section{Deployment Pipeline}
% Short Abstract
A deployment pipeline is an implementation of the process for getting software from source code to production.
% Based on CI
It is based on the concept of Continuous Integration (CD).
% Involves what
The process involves building, testing and deploying software through automated scripts \cite{Farley2010CI}.


\subsection{Continuous Integration}
% Abtract def
Continuous Integration is a development practice where each change on a primary code base is validated by automated scripts. This ensures that errors are detected and fixed in an early development stage \cite{Duvall2007CI}.
% What its about
The CI process is responsible for building and testing the software to guarantee that the software is in a releasable state at all time \cite{Rossel2017CICD}.
% Advantages
CI contributes with the following advantages to the development life cycle of an application:
\begin{itemize}
\item Reduce risks:
Because the CI process will run tests and validations on each change, errors will be detected in an early stage and can be fixed immediately \cite{Duvall2007CI}.

\item Reduce manual processes:
% Same time
The CI process will perform every time a commit has being made to the code base.
% Same process
Each run is processed the exact same way every time.
% No human
Therefore, no human intervention is needed to start the process which saves time and cost \cite{Duvall2007CI}.

\item Generate deployable software:
If an error occurs during a CI run, developers will be informed and fixes can be applied immediately.
This ensures that the software is in a deployable state at time \cite{Duvall2007CI}.
\end{itemize}


\subsection{Requirements of a Continuous Integration Process}
% Requirements
The implementation of a CI process is based on several requirements:
% The list
\begin{enumerate}
\item Version control repository:
To manage changes to the code base, the source code and all other assets like the build script should be hosted on a single version control repository.
% Changes
Each change on the code base will trigger the CI process on the build server to run against the latest version available \cite{Duvall2007CI}.

\item Build server:
The build server is responsible to monitor the code base for changes.
% Execute
If a change has been committed, the build server automatically executes the CI scripts in order \cite{Rossel2017CICD, Duvall2007CI}.

\item Build script:
This includes all automation scripts to validate the source code \cite{Duvall2007CI}.
\end{enumerate}


\subsection{Continuous Integration Process Implementation Example}
% Explain workflow
\paragraph{}\Fig{fig:02_foundation_deployment_ci_scenario} demonstrates the CI scenario.
% Commit changes
First a developer commits changes to the version control repository.
% CI
The CI server monitors the repository for changes. After the change has been committed, the CI server pulls the latest version of the source code and executes all build scripts in order to integrate the software.
% Feedback
Finally, the CI server sends feedback to inform the developer about the build script status  \cite{Duvall2007CI}.


% The scenario figure
\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{images/02_theoretical_foundation/deployment_pipeline/ci_scenario}
\caption{Continuous Integration Scenario - Source: Authors own model, based on \cite{Duvall2007CI}.}
\label{fig:02_foundation_deployment_ci_scenario}
\end{figure}


% Explain build script order
\paragraph{}A CI run should be executed in a headless automated process. It is not feasible to rely on a manual process.
% Automated scripts
All assets to perform the CI run should be accessed from the repository. Therefore a machine can start the build script process by executing a command script in an automated fashion \cite{Duvall2007CI}.
% The example
An example of a logical build script order is illustrated in \Fig{fig:02_foundation_deployment_ci_script-order}.


% The build script order figure
\begin{figure}[h]
\centering
\includegraphics[scale=1]{images/02_theoretical_foundation/deployment_pipeline/ci_build_script_order}
\caption{An example of a logical build script order for a CI process- Source: Authors own model, based on \cite{Duvall2007CI}.}
\label{fig:02_foundation_deployment_ci_script-order}
\end{figure}


% ===========================================
% ===========================================
\section{Autonomic Computing}
\label{sec:02_ac}
% What is autonomic computing
Autonomic computing is the ability of an IT infrastructure to automatically manage itself in accordance to high level objectives defined by administrators \cite{Kephart2003VisionComputing}.
% Why
Autonomic computing gives an IT infrastructure the flexibility to adapt dynamic requirements quickly and effectively to meet the challenges of modern business needs \cite{Murch2004Autonomic}. Therefore, autonomic computing environments can reduce operating costs, lower failure rates, make systems more secure and quickly respond to business needs \cite{Jacob2004AutonomicSolution}.


% What does it need
Computing systems need to obtain a detailed knowledge of its environment and how to extend its resources to be truly autonomic \cite{Murch2004Autonomic}.
% The 4 basic elements
An autonomic computing system is defined by four elements:
\begin{itemize}
\item Self-configuring:
Self-configuring refers to the ability of an IT environment to adapt dynamically to system changes and to be able to deploy new components automatically. Therefore, the system needs to understand and control the characteristics of a configurable item \cite{Murch2004Autonomic, Sinreich2006AnAB}.

\item Self-optimizing:
To ensure given goals and objectives, a self-optimizing environment has the ability to efficiently maximize resource allocation and utilization \cite{Jacob2004AutonomicSolution}. To accomplish this requirement, the environment has to monitor all resources to determine if an action is needed \cite{Murch2004Autonomic}.

\item Self-healing:
Self-healing environments are able to detect problematic operations and then perform policy-based actions to ensure that the systems health is stable \cite{Sinreich2006AnAB, Jacob2004AutonomicSolution}. The policies of the actions have to be defined and should be executed without disrupting the system \cite{Sinreich2006AnAB, Jacob2004AutonomicSolution}.

\item Self-protecting:
The environment must identify unauthorized access and threats to the system and automatically protect itself taking appropriate actions during its runtime \cite{Sinreich2006AnAB, Jacob2004AutonomicSolution}.
\end{itemize}


\subsection{Autonomic Computing Concept}

% Concept figure
\begin{figure}[h]
\centering
\includegraphics[scale=1]{images/02_theoretical_foundation/autonomic_computing/autonomic_computing_concept}
\caption{Autonomic computing concept - Source: Authors own model, based on \cite{Jacob2004AutonomicSolution}.}
\label{fig:ac_concept}
\end{figure}

% Figure + short description
\Fig{fig:ac_concept} demonstrates the main concept of an autonomic computing environment. The autonomic computing architecture relies on monitoring sensors and an adoption engine (autonomic manager) to manage resources in the environment \cite{Goscinski2011CloudComputing}.
% About the environment
In an autonomic computing environment, all components have to communicate to each other and can manage themselves. Appropriate decisions will be made by an autonomic manager that knows the given policies \cite{Jacob2004AutonomicSolution}.

% Control loop figure
\begin{figure}[h]
\centering
\includegraphics[scale=1]{images/02_theoretical_foundation/autonomic_computing/control_loop}
\caption{The control-loop concept - Source: Authors own model, based on \cite{Murch2004Autonomic}.}
\label{fig:ac_control_loop}
\end{figure}

% Explain control loop
The core element of the autonomic architecture is the control-loop. \Fig{fig:ac_control_loop} illustrates the concept of a control-loop. The control-loop collects details about resources through monitoring and makes decisions based on analysis of the collected details to adjust the system if needed \cite{Murch2004Autonomic}.


\subsection{Managed Resources}

% Figure
\begin{figure}[h]
\centering
\includegraphics[scale=1]{images/02_theoretical_foundation/autonomic_computing/managed_resource}
\caption{Managed resource - Source: Authors own model, based on \cite{Jacob2004AutonomicSolution}.}
\label{fig:ac_managed_resource}
\end{figure}

% Explanation
A managed resource is a single component or a combination of components in the autonomic computing environment \cite{Murch2004Autonomic, Jacob2004AutonomicSolution}. A component can be a hardware or software component, e.g. a database, a server, an application or a different entity \cite{Sinreich2006AnAB}.
% Sensors and Effectors
They are controlled by their sensors and effectors, as illustrated in \Fig{fig:ac_managed_resource}. Sensors are used to collect information about the state of the resource and effectors can be used to change the state of the resource \cite{Jacob2004AutonomicSolution}. The combination of sensors and effectors is called a touchpoint, which provides an interface for communication with the autonomic manager \cite{Sinreich2006AnAB}.
% Scalability
The ability to manage and control managed resources makes them highly scalable \cite{Murch2004Autonomic}.


\subsection{Autonomic Manager}
\label{subsec:02_ac_manager}

% Figure
\begin{figure}[h]
\centering
\includegraphics[scale=1]{images/02_theoretical_foundation/autonomic_computing/autonomic_manager}
\caption{Autonomic manager - Source: Authors own model, based on \cite{Jacob2004AutonomicSolution}.}
\label{fig:ac_manager}
\end{figure}

% WHat is the autonomic manager
The autonomic manager implements the control-loop to collect, aggregate, filter and report system metrics from the managed resources. It can only make adjustments within its own scope and uses predefined policies to make decisions of what actions have to be executed to accommodate the goals and objectives \cite{Murch2004Autonomic, Sinreich2006AnAB}.
% Knowledge
In addition, the autonomic manager gains knowledge through analysing the managed resources \cite{Murch2004Autonomic}.
% MAPE
The autonomic computing concept digests the MAPE model to implement an autonomic manager, as illustrated in \Fig{fig:ac_manager} \cite{Goscinski2011CloudComputing}.

\begin{itemize}
\item \textbf{Monitor:}
The monitor phase is responsible to collect the needed metrics from all managed resources and applies aggregation and filter operations to the collected data \cite{Sinreich2006AnAB}.

\item \textbf{Analyze:}
The autonomic manager has to gain knowledge to determine if changes have to made to the environment \cite{Sinreich2006AnAB}. To predict future situations, the autonomic manager can model complex situation given the collected knowledge \cite{Jacob2004AutonomicSolution}.

\item \textbf{Plan:}
Plans have to be structured to achieve defined goals and objectives. A plan consists of policy-based actions \cite{Jacob2004AutonomicSolution, Sinreich2006AnAB}.

\item \textbf{Execute:}
The execute phase applies all necessary changes to the computing system \cite{Sinreich2006AnAB}.
\end{itemize}

% Multiple manager
Multiple autonomic manager can exist in an autonomic computing environment to perform only certain parts. For example, there can be one autonomic manager which is responsible to monitor and analyse the system and another autonomic manager to plan and execute. To create a complete and closed control-loop, multiple autonomic manager can be composed together \cite{Sinreich2006AnAB}.


% ===========================================
% ===========================================
\section{System Performance}


\subsection{Performance Metrics}
% Short description
Performance metrics are statistics that describe the system performance. These statistics are generated by the system, applications or other tools \cite{Greg2020SysPerf}.
% Common types
The following are examples of common types for performance metrics:
\begin{itemize}
\item \textbf{Throughput:} Volume of data or operations per second \cite{Greg2020SysPerf}.
\item \textbf{Latency:} Time of operation \cite{Greg2020SysPerf}.
\item \textbf{Utilization:} Usage of a component \cite{Greg2020SysPerf}.
\end{itemize}


% Overhead
\paragraph{}It is important to note that measuring performance metrics can cause an overhead. To gather and store performance metrics, CPU cycles must be spent. This can have a negative affect on the target performance \cite{Greg2020SysPerf}.


\subsection{Time-Based Utilization}
% What is utilization
Utilization is a performance metrics that describes the usage of a device, e.g. CPU device usage.
% What about time-based utilization
A time-based utilization describes the usage of a component during a time period where the component was actively performing work \cite{Greg2020SysPerf}.
% 100% utilization
When a resource approaches 100\% utilization, the performance of that resource can degrade. If a component can process operations in parallel, the performance does not have to degrade much at 100\% utilization and the component can process more work \cite{Greg2020SysPerf}.


% ===========================================
% ===========================================
\section{Monitoring}
\label{sec:02_monitoring}
% What is monitoring
Monitoring is a process, that aims to detect and take care of system faults. In a dynamic environment, becoming aware of the system is a trivial process \cite{Ligus2012EffMonitoring}.
% What is a monitoring system
A monitoring system consists of a set of multiple tools. The tools are responsible to perform measurements on components in the computing environment and collect, store and interpret the monitored data \cite{Ligus2012EffMonitoring}. 


% The monitoring Loop/Process
\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{images/02_theoretical_foundation/monitoring/monitoring_system}
\caption{The monitoring process - Source: Authors own model.}
\label{fig:mon_mon-system-process}
\end{figure}
In the monitoring process, illustrated in \Fig{fig:mon_mon-system-process}, data is continuously collected by agents. An agent is a process that continuously gathers metrics. Data can be device statistics, logs or system measurements. Agents will group these data into metrics and submit them to the monitoring system. The monitoring system will store the metrics in its database \cite{Ligus2012EffMonitoring}.


% Requirements of a monitoring system
The requirements for a monitoring system, that is able to monitor a dynamic changing environment, are the following:
\begin{itemize}
\item An efficient database to store metrics \cite{Farcic2017Toolkit21}
\item A push or pull based way of gathering metrics \cite{Farcic2017Toolkit21}
\item Multi-dimensional metrics \cite{Farcic2017Toolkit21}
\item A powerful query language \cite{Farcic2017Toolkit21}
\end{itemize}


\subsection{Database}
% Continuous data
Continuous data needs to be stored in the most efficient way.
% TSDB
Time-series databases (TSDB) are optimized to store and retrieve time-series data.
% Format
In a time-series database, metrics will be stored in a compact and optimized format. This allows the database to store a massive amount of time-series data on a single machine.


\subsection{Push and Pull Based Monitoring Systems}
\label{subsec:02_monitoring_push-pull}
The approach how the monitoring systems gathers metrics to store in the database plays a significant role.
% There are push and pull based systems
Push and pull based systems are the two primary approaches to gather metrics from services.
% About push
Push based monitoring systems expect services to push metrics to their storage.
% About pull
On the other hand, pull based monitoring systems scrape metrics from all defined targets. Targets do not know about the existence of the monitoring system and only need to collect and expose metrics \cite{Farcic2017Toolkit21}.


% When to choose what (Discovery)
Service discovery is an important aspect to decide whenever to use a pull or push based monitoring system \cite{Farcic2017Toolkit21}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{images/02_theoretical_foundation/monitoring/push_based}
\caption{Push-based monitoring approach - Source: Authors own model.}
\label{fig:mon_push-based}
\end{figure}
% Push based discovery
In a push-based environment, services only need to know the address of the monitoring service to push their data to the storage \cite{Farcic2017Toolkit21}.


\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{images/02_theoretical_foundation/monitoring/pull_based}
\caption{Pull-based monitoring approach - Source: Authors own model.}
\label{fig:mon_pull-based}
\end{figure}
% Pull based discovery
A pull-based monitoring tool needs to know the address of each target in the environment.
% Why pull based works best with discovery
The advantage of a pull-based monitoring systems is the simplicity to detect whenever a target has failed or is not available \cite{Farcic2017Toolkit21}.


\subsection{Multi-Dimensional Metrics}
\label{subsec:02_monitoring_db_multi-metrics}
% Why do we need dimensions
For query languages to be effective, metrics need to be dimensional. Metrics without dimensions, are limited in their capabilities.
% Dynamic environment
In a dynamic environment, services are dynamically added and removed. Therefore, a dynamic environment needs dynamic analytics where metrics represent all dimension in the environment \cite{Farcic2018Toolkit22}.
% Example
\begin{lstlisting}[frame=single, label=lst:mon_metr_dimless, caption=Example of a dimensionless-metric, captionpos=b]
container_cpu_user_seconds_total
\end{lstlisting}
\begin{lstlisting}[frame=single, label=lst:mon_metr_withdim, caption=Example of a metric with dimensions, captionpos=b]
container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}
\end{lstlisting}
% Explain
As the example \Lst{lst:mon_metr_dimless} and \Lst{lst:mon_metr_withdim} show, the metric with dimension provides more efficient querying to gather informations about the environment.


\subsection{Query Language}
Another requirement is a powerful and flexible query language. It should have the ability to query a multi-dimensional data model.
% Aggregations
Furthermore, the query language must allow aggregations on time-series data.
