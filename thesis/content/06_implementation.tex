\chapter{Implementation}
\label{chap:06_implementation}
\todo{Describe Chapter}


% ===========================================
% ===========================================
\section{Background}
\todo{NVIDIA RUNTIME MUSS INSTALLIERT SEIN UND ALS DEFAULT}
Mentioned before, the goal is to create a self-healing and self-adapting computing environment. In addition, Apache Spark applications  should be able to deploy automatically via a CI pipeline to the cluster. Therefore, to implement the conceptual design described in SECTION XY, the following requirements need to be implemented:

\begin{enumerate}
% 1
\item Implementing the Auto-Scaler Python module
% 2
\item Dockerizing the Auto-Scaler
% 3
\item Creating the computing environment in a Docker swarm
\begin{itemize}
\item Create an Apache Spark cluster with GPU acceleration
\item Create an autonomic manager with a monitoring system and an Auto-Scaler
\end{itemize}
% 4
\item Implement a GitLab CI pipeline to deploy Apache Spark applications to the cluster
\end{enumerate}


% ===========================================
% ===========================================
\section{Auto-Scaler}
\todo{AUto-Scaler control loop: Periodically checks if the number of desired worker replicas is running, vll auch eher in design}
% Concept
The conceptual design of the Auto-Scaler is described in detail in \Sec{sec:05_auto-scaler}. 
% Python module
The Auto-Scaler will be implemented as a module in Python 3.8.
% Docker
To deploy the Auto-Scaler in a Docker swarm, the Auto-Scaler needs to be dockerized in a Docker Image.


%
% WARUM NICHT GPU MIT IN DIE AUTO-SCALER METRICS?
% weil alle worker sich die GPUs teilen ...
%

% Only horizontal
%During this thesis work, the horizontal scaling approach is being used because of its suitability to solve this thesis problems and therefore being %introduced in this section.
% Vertical
%Vertical scaling will not be used due to its limitations explained in \Sec{subsec:02_foundations_scalability_limits}.

\subsection{Configuration}
% Design
\paragraph{}The configuration parameter for the Auto-Scaler have been introduced in \Sec{subsec:05_auto-scaler_configuration}.
% Its yaml
The configuration for the Auto-Scaler will be specified in a YAML file.
% The yaml file
\Lst{lst:06_auto-scaler_config_example} describes an example of an Auto-Scaler configuration.
% Overall structure
Overall, a configuration file is structured in three sections: General, metrics and worker.
% The table
\Tab{table:06_auto-scaler_config_parameter} lists all available configuration parameters. It describes the value type and the default value of each parameter. Some parameters are required to be defined by the administrator and have no default value.

% General
\paragraph{General:}
The general section defines details about the scaling and heat algorithm and the Prometheus URL.


% Metrics
\paragraph{Metrics:}
\todo{Multi-dimensional metrics/queries}
Metrics is a list of performance metrics configuration parameters. The name of a metric (\textit{cpu} and \textit{gpu} in the example \Lst{lst:06_auto-scaler_config_example}) can be set by the administrator. A performance metric requires a query in the PromQL syntax. Additionally a target utilization is needed and the minimum and maximum utilization of the performance metric.


% Worker
\paragraph{Worker:}
To scale the replicas of the Apache Spark worker service, the name of the Docker service needs to be set. In addition, the minimum and maximum number of concurrent worker nodes needs to be defined to prevent an overhead of running worker nodes.


% Example
\begin{lstlisting}[label=lst:06_auto-scaler_config_example, caption=Auto-Scaler configuration YAML file]
general:
  interval_seconds: 5
  cooldown_period_seconds: 180
  recurrence_factor: 3
  prometheus_url: "http://localhost:9090"
 
metrics:
  cpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.5
    thresholds:
      min: 0.2
      max: 0.6
  
  gpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.3
    thresholds:
      min: 0.2
      max: 0.6
 
worker:
  service_name: "computing_spark-worker"
  thresholds:
    min: 1
    max: 30
\end{lstlisting}


% Parameter table
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Name                      & Type    & Default           \\ \midrule
\multicolumn{3}{l}{\textbf{general}}                    \\ \midrule
interval\_seconds         & Integer & 1                 \\
cooldown\_period\_seconds & Integer & 180               \\
recurrence\_factor        & Integer & 1                 \\
prometheus\_url           & String  & \textit{Required} \\
\multicolumn{3}{l}{\textbf{metrics}}                    \\ \midrule
query                     & String  & \textit{Required} \\
target\_utilization       & Float   & 0.5               \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\
\multicolumn{3}{l}{\textbf{worker}}                     \\ \midrule
service\_name             & String  & \textit{Required} \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\ \bottomrule
\end{tabular}
\caption{Auto-Scaler configuration parameter}
\label{table:06_auto-scaler_config_parameter}
\end{table}


\subsection{Scaling the Apache Worker Service}
docker service apache-worker scale replicas=4
Not possible to use nvidia-runtime because of restart -> scale docker container


\subsubsection{Estimation of Necessary Scaling Actions}

%
% ANALYZE !!!!!
%

The Scaling Heat algorithm, introduced in \Sec{sec:04_scal-heat}, is being used to estimate if a scaling action is necessary.
% Why
The algorithm is being used because it will prevent the Auto-Scaler to perform unnecessary scaling actions.
% For each performance metric
During each interval, after performance metrics have been received from the monitoring system, a heat value will be calculated for each performance metric specified in the configuration under \textit{metrics}.
% Recurrence factor
The algorithm uses a recurrence factor which has to be defined in design time. The Auto-Scaler configuration provides a parameter called \textit{recurrence\_factor} (see \Tab{table:06_auto-scaler_config_parameter} for details). 


% HeatStore
\paragraph{}To store and calculate the heat for each performance metric, a class called \texttt{HeatStore} was created.
% Explain UML
FIGURE XY describes the UML class diagram for the \texttt{HeatStore} Python class.
% For what can it be used
The class can be used to retrieve, update and reset the heat for a list of performance metrics.


\subsubsection{Calculating the Number of Needed Worker Nodes}

%
% PLAN !!!
%


% Intro KHPA
The KHPA algorithm will be used to calculate how many worker are needed to reach the target utilization (see SECTION XY for algorithm details). 
% More metrics then one
In this project, the calculation is done for the CPU and GPU utilization. The highest number of the desired worker node replicas is chosen.


\subsubsection{Performing a Scaling Action}

%
% EXEVUTE !!!
%


% Python Docker SDK
Docker provides a Python library for the Docker Engine\footnote{Docker SDK for Python 4.4.1 Documentation - \url{https://docker-py.readthedocs.io/en/4.4.1/} (Accessed: 2021-01-05)}. This library will be used to perform the swarm scaling action.


% Check if worker is busy
If worker need to be removed, it is necessary to check if the worker are running any applications. Removing a worker while an application is performing will cause the cancellation of the application.
% How to check if applications are running
To check if applications 


% Cooldown
After a scaling action has been performed, a cooldown period will be applied. The cooldown period is needed because the number of desired worker nodes can keep fluctuating due to the dynamic nature of performance metrics.


\subsection{Docker Image}
- Dockerfile hier erklären
- Wie den Auto-Scaler per konsole starten (mit config parameter)


% ===========================================
% ===========================================
\section{Deployment of a Docker Swarm}


\subsection{Hardware}
% DGX erklären


\subsection{Software info}
Hier tabelle mit versionen von eingesetzter software


\subsection{Swarm}
Vielleicht euch einfach das ganze kapitel Swarm nennen?
- Dockerfile erläutern
- GPUs (nur die 2 bestimmten)


\subsection{Build Script}


\subsection{Apache Spark Cluster with GPU Acceleration}
% Standalone
\paragraph{}The Apache Spark cluster is created in standalone mode, see \SubSec{subsec:04_spark_standalone} for deployment details.
% Master and worker
The cluster consists of a single Apache Spark master node and a dynamic number of Apache Spark worker nodes. The master and worker container will run as a service in a swarm (see \Sec{subsec:04_docker_swarm}).
% Submit
For submitting an application to the cluster, an individual container performing the spark-submit executable will be deployed.
% Docker
Each node runs in an independent Docker container.


% How many images are needed
\paragraph{}Overall four Docker images are needed to create the Apache Spark cluster introduced in \SubSec{subsec:05_arch_spark}:
\begin{itemize}
\item Base image
\item Master image
\item Worker image
\item Submit image
\end{itemize}
% The base image
The master, worker and submit image require common packages to be installed and a set of common configuration. Therefore, an additional base image will serve as a base image.

\subsubsection{Apache Spark Base Image Installation Details}
\paragraph{}The base image Dockerfile is available at \Lst{lst:appendix_spark_base_dockerfile}.
% Ubuntu base docker image
As parent, the base image uses the \texttt{nvidia/cuda:11.0-devel-ubuntu16.04} Docker image. The parent image runs Ubuntu\footnote{Enterprise Open Source and Linux - \url{https://www.ubuntu.com/} (Accessed: 2021-01-03)} in version 16.04. Additionally the CUDA Toolkit and the NVIDIA GPU driver are already installed.
% Args for spark and hadoop version
Docker provides the ability to set build arguments. To be able to install a specific Apache Spark version, two arguments,  can be set when building the Docker image.
% Apache Spark
Apache Spark will be installed at \texttt{/opts/Spark}.
% Work dir
This directory will be set as the working directory for the Docker image as well.
% Ubuntu packages
Furthermore, required Ubuntu packages will be installed. This includes the Java Runtime Environment Version 8, which is a requirement for Apache Spark.
% RAPIDS
To enable GPU acceleration on all Apache Spark nodes, the base image will install the compiled Java files for the RAPIDS plugin at \texttt{/opt/sparkRapidsPlugin} (for RAPIDS installation requirements, see \SubSec{subsec:04_rapids_req}). The .jar files can be downloaded in the maven repository.
% Discovery script
To enable Apache Spark to discover available GPUs, a GPU discovery script is needed (see \SubSec{subsubsec:04_spark_standalone_res-alloc} for details about resource allocation). This discovery script will be placed at \texttt{/opt/sparkRapidsPlugin} as well. The discovery script is introduced at \Lst{lst:appendix_spark_gpu-discovery}.

%% HETEROGEN


% Building
%\paragraph{}Listing XY demonstrates how the base image can be build 


% Env var table
%The table \ref{table:1} is an example of referenced \LaTeX elements.

%\begin{table}[h!]
%\centering
%\begin{tabular}{||c c||} 
 %\hline
 %Environment variable & Value \\ [0.5ex] 
 %\hline\hline
 %$SPARK\_RAPIDS\_DIR$ & /opt/sparkRapidsPlugin \\ 
 %$SPARK\_CUDF\_JAR$ & opt/sparkRapidsPlugin/cudf-0.15-cuda11.jar \\
 %$SPARK\_RAPIDS\_PLUGIN\_JAR$ & opt/sparkRapidsPlugin/rapids-4-spark_2.12-0.2.0.jar \\
 %$JAVA\_HOME$ & /usr/lib/jvm/java-8-openjdk-amd64 \\
 %$PYSPARK\_PYTHON$ & python3 \\ [1ex] 
 %\hline
%\end{tabular}
%\caption{Table to test captions and labels}
%\label{table:1}
%\end{table}


\subsubsection{Standalone Master and Worker Image}
% Base image
The master and worker image are build on top of the  Apache Spark base image.
% no additional installation
Therefore, no additional installation steps are required.
% standalone
The master and worker nodes will be launched in standalone mode (see \SubSec{subsec:04_spark_standalone} for standalone mode details).


\paragraph{Master image:}
Implementation of the master node Dockerfile is available at \Lst{lst:appendix_spark_master_dockerfile}.
% Ports
The master node image needs two ports configured: The Apache Spark service port and the port for the web user interface.
% web ui port
The Apache Spark service port ist set to 7077 and the web user interface port to 4040. 
% Start in standalone
To start the master node in standalone mode, the start-master.sh launchable will be set as image entrypoint which requires no additional arguments.


\paragraph{Worker image:}
\Lst{lst:appendix_spark_worker_dockerfile} describes the implementation of the worker node Dockerfile.
% web ui port
The port for the worker web interface will be exposed at 4041.
% Port
To start the worker in standalone mode, the start-slave.sh executable will be set as entrypoint for the image.
% Master uri
The launch script requires the master node URI as a parameter. To keep the configuration simple, the environment variable HIER DIE VAR will be set in the compose file.
% Worker configuration
\Lst{lst:06_computing_spark_worker-env} describes the configuration environment for the worker. As mentioned previously (in \Sec{sec:05_restrictions}), for this project two GPUs are available on the DGX workstation. Furthermore, the worker needs to know where to find the GPU resource discovery script.

% Building script
\begin{lstlisting}[label=lst:06_computing_spark_worker-env, caption=Environment configuration for all worker nodes, language=bash]
SPARK_WORKER_OPTS="-Dspark.worker.resource.gpu.amount=2 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
\end{lstlisting}


\subsubsection{Submit Image}
% Pyspark
As mentioned in \Sec{sec:05_restrictions}, a requirements is, that Apache Spark application will be implemented in Python. Accordingly, the pyspark Python module needs to installed on all submit nodes.
% Application workspace
Apache Spark application will be placed at \textit{/opt/spark-apps}. SECTION CI describes how an Apache Spark application will be copied to a submit node.
% Entrypoint
As entrypoint, the image will perform a custom submit script (available at LISTING AB). This script performs the spark-submit executable (usage described in detail in \SubSec{subsubsec:04_spark_standalone_submit}).


\subsection{Autonomic Manager}
As mentioned in \SubSec{subsec:05_arch_am}, the autonomic manager will consist of a monitoring system and the Auto-Scaler to create a complete control loop.

\paragraph{}The monitoring system conceptual design was introduced in SECTION XY.
It consists of a cAdvisor (SECTION XY) service and a Prometheus (SECTION XY) service.
All modules will run as individual Docker services in the overall swarm.
% Docker images
The following DOcker images will be used for the monitoring system:
% List all images
\begin{itemize}
\item \textbf{cAdvisor:} google/cadvisor
\item \textbf{Prometheus:} prom/prometheus
\end{itemize}

\subsubsection{Prometheus Target Configuration}
% Pull based
As mentioned in \Sec{sec:04_prom}, Prometheus is a pull-based monitoring tool.
% Target list
It requires a list of targets to pull performance metrics from.


% Describe config
\Lst{lst:06_computing_am_prom-config} specifies the scrape configuration of the Prometheus system.
% cAdvisor target
The cAdvisor service is specified as a target. Prometheus will scrape every 5 seconds performance metrics from cAdvisor. All performance metrics will be labeled with the cAdvisor lable. The cAdvisor service is available at  cadvisor:8080.


% Building script
\begin{lstlisting}[label=lst:06_computing_am_prom-config, caption=Prometheus target configuration in YAML syntax]
scrape_configs:
    - job_name: prometheus
      scrape_interval: 5s
      static_configs:
          - targets: ["localhost:9090"]
 
    - job_name: cadvisor
      scrape_interval: 5s
      static_configs:
          - targets: ["cadvisor:8080"]
            labels:
                group: "cadvisor"
\end{lstlisting}


% ===========================================
% ===========================================
\section{Automatic Deployment of Apache Spark Applications}
Vielleicht eher das Kapitel so nennen
- gitlab-ci.yml erklären
- Screenshot von webui output
