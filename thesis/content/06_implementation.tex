\chapter{Implementation}
\label{chap:06_implementation}

This chapter explains the implementation process of the conceptual detail introduced in \Chap{chap:05_design}.


% ===========================================
% ===========================================
\section{Implementation Environment}
% The DGX
For the implementation of the conceptual design this thesis, a NVIDIA DGX\footnote{The Universal System for AI Infrastructure - \url{https://www.nvidia.com/en-us/data-center/dgx-a100/} (Accessed: 2021-01-30)} machine is available. 
% Live system
This machine is a shared live-system. Therefore, multiple application are performing simultaneously on this machine which share the same resources.


\subsection{Technical Details}
% Hardware Resources
The hardware specification of the machine is the following:
\begin{itemize}
\item 11GB RAM
\item 300 TB DIsk space 
\item 8x NVIDIA Tesla GPU a 32GB
\item 500x Intel CPU COre 29876
\end{itemize}


% Available software
The following software is installed on the machine:
\begin{itemize}
\item Ubuntu 30.5
\item Docker 19.5
\item NVIDIA Docker runtime 1
\end{itemize}


\subsection{NVIDIA Docker Runtime}
% nvidia runtime
To enable GPU support for Docker containers, the NVIDIA Container Toolkit\footnote{NVIDIA Cloud Native technologies documentation - \url{https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html} (Accessed: 2021-01-28)} has to be installed on the host machine. This toolkit provides a runtime library which automatically enables Docker containers to leverage NVIDIA GPUs.
% The problem
It is possible to define the runtime of a Docker container with the docker \texttt{run} command. However, this is not supported for Docker services. To deploy Docker services with the NVIDIA runtime enabled, the NVIDIA runtime has to be set as the default runtime for Docker.

\paragraph{Problem statement:}
% Already installed
On the host machine, the NVIDIA Container Toolkit is installed, however the NVIDIA runtime is not set as default runtime.
% The problem
Changing the default runtime requires a restart of the Docker service. Restarting the Docker service is not possible because it requires to quit all running Docker containers on the machine. Therefore, components which require access to GPU resources (Apache Spark worker nodes and the dcgm-exporter) cannot be deployed as Docker services.
% The solution
The solution to this problem is, to deploy these components as Docker container instead of Docker services.
% Finish
Given this problem statement, the \textit{Auto-Scaler} has to be implemented to use the Docker container API instead of the Docker service API to create Apache Spark worker nodes in the environment. Additionally, the dcgm-exporter Docker container has to be created manually and cannot be created with the Docker stack command.


% ===========================================
% ===========================================
%- Screenshots from Output
%- Logging allgemein beschreiben
\section{Auto-Scaler}
% SHort intro 
The \textit{Auto-Scaler} is a main module of the autonomic manager. It is responsible to analyse performance metrics, plan scaling actions in accordance to the performance metrics and execute scaling actions to adapt the number of Apache Spark worker in the computing environment.
% Mention conceptual design
It is implemented as a custom module in Python 3.8 and deployed as a Docker image using a custom Dockerfile to deploy the \textit{Auto-Scaler} running in a Docker container.


% Concept
The \textit{Auto-Scaler} integrates the Analyse, Plan, and Execute phase of the MAPE architecture. Furthermore it scales the Apache Worker horizontally by adding and removing Apache Spark worker Docker containers.
% A Loop
The Auto-Scaler runs as a background service that periodically performs each phase in order.
% Configuration
A file which defines runtime configuration can be set.
% Start the sclaer
The Auto-Scaler can be started with the following command:
\begin{lstlisting}[label=lst:06_auto-scaler_start, caption=Auto-Scaler start command, language=sh]
$ python3 run.py --config=config.yml
\end{lstlisting}


\subsection{Technical Background}
% Technical background
The \textit{Auto-Scaler} is implemented in Python 3.8. It consists of different classes, each having different responsibilities.
% A list of the Python libs
The following Python libraries have been used for the implementation:
% The list
\begin{itemize}
\item aiohttp\footnote{\url{https://pypi.org/project/aiohttp/} (Accessed: 2021-01-26)}
\item APScheduler\footnote{\url{https://pypi.org/project/APScheduler/} (Accessed: 2021-01-26)}
\item docker\footnote{\url{https://pypi.org/project/docker/} (Accessed: 2021-01-26)}
\item PyYAML\footnote{\url{https://pypi.org/project/PyYAML/} (Accessed: 2021-01-26)}
\end{itemize}


\subsection{Configuration}
% Design
\paragraph{}The configuration parameter for the \textit{Auto-Scaler} have been introduced in \Sec{subsubsec:05_am_auto-scaler_config}.
% Its yaml
All parameter are defined in the YAML file format
% The yaml file
\Lst{lst:06_auto-scaler_config_example} provides an example of a configuration.
% Overall structure
Overall, a configuration file is structured in three sections: General, metrics, and worker.
% The sections
\begin{itemize}
% General
\item General:
The general section defines details about the scaling and heat algorithm and the Prometheus URL.

% Metrics
\item Metrics:
Metrics is a list of performance metrics configuration parameters. A performance metric requires a query in the PromQL syntax. Additionally a target utilization is needed and the minimum and maximum utilization of the performance metric.

% Worker
\item Worker:
To scale the replicas of the Apache Spark worker service, the name of the Docker service needs to be set. In addition, the minimum and maximum number of concurrent worker nodes needs to be defined to prevent an overhead of running worker nodes.
\end{itemize}

% The table
\Tab{table:06_auto-scaler_config_parameter} lists all available configuration parameters. It describes the value type and the default value of each parameter. Some parameters are required to be defined by the administrator and have no default value.
% Parameter table
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Name                      & Type    & Default           \\ \midrule
\multicolumn{3}{l}{\textbf{general}}                    \\ \midrule
interval\_seconds         & Integer & 1                 \\
cooldown\_period\_seconds & Integer & 180               \\
recurrence\_factor        & Integer & 1                 \\
prometheus\_url           & String  & \textit{Required} \\
\multicolumn{3}{l}{\textbf{metrics}}                    \\ \midrule
query                     & String  & \textit{Required} \\
target\_utilization       & Float   & 0.5               \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\
\multicolumn{3}{l}{\textbf{worker}}                     \\ \midrule
service\_name             & String  & \textit{Required} \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\ \bottomrule
\end{tabular}
\caption{Auto-Scaler configuration parameter}
\label{table:06_auto-scaler_config_parameter}
\end{table}

\todo{Update weil kein DOcker swarm mehr}
% Example
\begin{lstlisting}[label=lst:06_auto-scaler_config_example, caption=Auto-Scaler configuration YAML file]
general:
  interval_seconds: 5
  cooldown_period_seconds: 180
  recurrence_factor: 3
  prometheus_url: "http://localhost:9090"
 
metrics:
  cpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.5
    thresholds:
      min: 0.2
      max: 0.6
  
  gpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.3
    thresholds:
      min: 0.2
      max: 0.6
 
worker:
  service_name: "computing_spark-worker"
  thresholds:
    min: 1
    max: 30
\end{lstlisting}


\subsection{Scaling Apache Worker Nodes}
% Periodically performing
The \textit{Auto-Scaler} performs periodically. Each period fetches performance metrics, analyses the metrics, plans scaling actions, and executes them if necessary.
% AP jobs
To perform periodically, it uses the \textit{APScheduler}\footnote{\url{https://pypi.org/project/APScheduler/} (Accessed: 2021-01-26)} Python library. The \textit{APScheduler} provides to schedule jobs periodically.
% Explain jobs
In each job, the Analyse, Plan, and Execute phase is performed in order.


\subsubsection{Estimation of Necessary Scaling Actions}
To estimate of a scaling actions is necessary, the Auto-Scaler uses the Scaling Heat algorithm (introduced in \Sec{alg:04_scal-heat_concept_algo}).
% input
The algorithm takes the utilization of a performance metric, the lower- and upper-threshold, and the calculated heat of the last iteration as input parameters.
% utilization
The utilization is received from the Prometheus HTTP API. Furthermore, the \textit{Auto-Scaler} fetches the utilization of all given performance metrics in each iteration. It calculates the heat value for each single performance metric. If the upper-threshod value gets violated, the heat value is increased by 1. Otherwise, if the lower-threshold is violated, the heat is reduced by one.
% the limit
Until the heat value reaches positive or negative of the recurrence-factor a scaling action will be executed.
% cooldown
This phase will not be performed if a cooldown period has been activated in the previous iteration.


\subsubsection{Calculating the Number of Needed Worker Nodes}
To calculate the number of needed Apache Spark worker, the \textit{Auto-Scaler} uses the KHPA algorithm (introduced in \Sec{sec:04_background_khpa}).
As input parameters, the algorithm takes the current number of active worker nodes, the utilization of the performance metric, and target utilization.


% Explain implementation
\Lst{lst:06_auto-scaler_plan_khpa} shows the implementation of the KHPA algorithm in Python.
To receive the number of active worker nodes, the Apache Spark master node has to be defined as a Prometheus target (described in SECTION). This enables to fetch the \texttt{metrics\_master\_aliveWorkers\_Value} metric from the Prometheus HTTP API. This metrics returns the number of alive Apache Spark worker nodes in the cluster.

The utilization of the performance metric, which is used for the KHPA algorithm, has already been received in the previous phase. Lastly, the target utilization for the performance metric is defined in the \textit{Auto-Scaler} configuration. 
% The python impl
\begin{lstlisting}[label=lst:06_auto-scaler_plan_khpa, caption=KHPA implementation using Python 3.8, language=Python]
def calculate_number_of_needed_worker(active_worker: int,
    utilization: float,
    target_utilization: float):
  return math.ceil(
    active_worker * (utilization / target_utilization))
\end{lstlisting}


\subsubsection{Performing a Scaling Action}
After the number of needed Apache Spark worker nodes are calculated, the \textit{Auto-Scaler} is responsible to scale the replicas of the worker service to reach the desired performance goal.
% Docker lib
To scale a Docker service using Python, Docker provides a Python library for the Docker Engine\footnote{Docker SDK for Python 4.4.1 Documentation - \url{https://docker-py.readthedocs.io/en/4.4.1/} (Accessed: 2021-01-05)}. 
% Cooldown
After a scaling action has been executed successfully by the Docker engine, a cooldown period is activated. The duration of the cooldown period can be set in the configuration. During this cooldown no scaling actions are executed.


\subsubsection{Removing Apache Spark worker}
% RDD intro
As being introduced in SECTION RDD, Apache Spark is build on top of a data structure called RDD. A RDD can be compose of bla bla bla.
% Shuffle data
Executors are launched on workers to process data. Therefore, a worker node stores temporary data of an executor containing cache data and shuffle-data.
% No downsscaling
If a worker node store important shuffle data, even if the overall performance is underutilized, the worker cannot be removed.
% How
% Check if worker is busy
Therefore, worker will not be removed if applications are actively performed on the Apache Spark cluster.
% The metric
Prometheus provides a metrics called \texttt{metrics\_master\_apps\_Value} which returns the number of running applications on the cluster. To fetch this metrics, the Apache Spark master node has to be set as Prometheus target.
% auto-scaler
The Auto-Scaler only reduces the number of worker replicas, if this metric returns 0.


\subsection{Docker Image}
% Why dockerfile
A Docker image is needed to deploy the \textit{Auto-Scaler} as a Docker service in the computing environment. Therefore a custom Dockerfile is created to build an \textit{Auto-Scaler} Docker image.
% Explain Dockerfile
\Lst{lst:06_auto-scaler_dockerfile} shows the Auto-Scaler Dockerfile implementation. The python:3 Docker image is set as the parent image. 
Next, the \textit{Auto-Scaler} source code is copied to image. Afterwards, a Python virtual environment is created with all dependencies installed.
As entrypoint, the Docker image starts the \textit{Auto-Scaler} process.
\begin{lstlisting}[label=lst:06_auto-scaler_dockerfile, caption=Auto-Scaler Dockerfile]
FROM python:3
 
WORKDIR /usr/src/auto_scaler
 
# Copy the python module
COPY setup.py .
COPY src src/
 
# Update and install packages
RUN pip3 install -e .
 
ENTRYPOINT [ "python3", "src/run.py" ]
\end{lstlisting}


% Building the Docker image
To build the \textit{Auto-Scaler} Docker image, a build script was implemented. \Lst{lst:06_auto-scaler_build} shows the build script implementation.
The script takes the version of the \textit{Auto-Scaler} module as input attribute and uses the version to tag the Docker image.
It builds two versions of the \textit{Auto-Scaler}, one tagged with the current \textit{Auto-Scaler} version (e.g. \textit{auto-scaler:1.0}) and one tagged as \textit{latest} (e.g. \textit{auto-scaler:latest}).
\begin{lstlisting}[label=lst:06_auto-scaler_build, caption=Auto-Scaler build script, language=sh]
#!/bin/bash
 
if [ $# -ge 1 ]
  then
    VERSION=$1
 
    PWD=$(pwd)
 
    CI_REGISTRY=${CI_REGISTRY-local}
    IMAGE_TAG_PATH="${CI_REGISTRY}/cci/distributed-computing-framework"
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:$VERSION \
      $PWD
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:latest \
      $PWD
  else
    echo "No arguments supplied\n"
      echo "Use the script as follows:"
      echo "build-image.sh <VERSION>"
      exit 1
fi
\end{lstlisting}
% How to use the script
The script can be used from the command line as \texttt{sh build-image.sh "1.0"}.

% ===========================================
% ===========================================
\section{Apache Spark Cluster with GPU Acceleration}
The Apache Spark cluster consists of a single master node, a dynamic number of worker nodes, and \textit{spark-submit} nodes.
A \textit{spark-submit} is deployed, whenever an application is submitted to cluster.
The master worker nodes are deployed in standalone mode.


\subsection{Apache Spark Base Image}
All services in the Apache Spark cluster have the same dependencies but perform different services. To simplify the creation of the Docker images, a base image is created. This base image serves as the parent image for all other images.
Additionally, this contributes to the homogeneity of running services in cluster.


%
To install a specific Apache Spark version on the Docker image, two arguments are provides. The arguments can be set in the Docker build command with the \texttt{--build-arg} flag. LISTING BLA provides 
%
The base image is build from the \texttt{nvidia/cuda:11.0-devel-ubuntu16.04} Docker image. This image is provided by NVIDIA and has already installed all needed dependencies to leverage GPUs to Docker images.
%
The Dockerfile installs all required dependencies to run a standalone Apache Spark node with GPU acceleration. This includes the Apache Spark, JRE 1.8, Python3, GPU discovery script, RAPIDS Java binary, cuDF Java binary, XGBoost4j binary, and XGBoost4j-spark binary.


\subsection{Apache Spark Master Image}
% Intro Dockerfile
The Apache Spark master node Docker image is created from a custom Dockerfile (see \Lst{lst:appendix_spark_master_dockerfile}).
% Base image
This image is build on top the \textit{spark-base} image and therefore has already all dependencies installed.
% Ports
It configures the Apache Spark master port to 7077 and the port for the web user interface to 4040.
% entrypoint
Additionally, the \texttt{start-master.sh} script is set as entrypoint. Therefore, when a service is build from this image, it will automatically start an Apache Spark master node in standalone mode.


\subsection{Apache Spark Worker Image}
% Intro
The Apache Spark worker image is created from a custom Dockerfile as well.
%
This Dockerfile uses the \textit{spark-base} image as parent.
% Homo
As being mentioned in SECTION THEO, the horizontal scaling approach is more effective with homogeneous nodes. Each Apache Spark worker node is being created from the same Docker image given the same resources.
%
Therefore, no additional dependencies have to be installed.
%
To configure the worker nodes resources, the \texttt{spark-env.sh} is copied to the Apache Spark \texttt{conf/} folder. \Lst{lst:06_env_depl_worker-env} shows the configuration file. It sets the number of GPUs for each executor and the path to the GPU discovery script on the node host.
%
The entrypoint is set to \texttt{start-slave.sh} to start an Apache Spark worker in standalone mode. To connect to Apache Spark master node, the URI has to be set as an environment variable.
\begin{lstlisting}[label=lst:06_env_depl_worker-env, caption=Environment configuration for all worker nodes, language=sh]
SPARK_WORKER_OPTS="-Dspark.worker.resource.gpu.amount=1 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
\end{lstlisting}


\subsection{Apache Spark Submit Image}
% Intro
An Apache Spark submit container is deployed by the CI pipeline whenever an application is submitted to the cluster. This container is created from a \textit{spark-submit} Docker image.
% Parent
The spark-submit Dockerfile uses the spark-base image as parent.
% submit
It copies a custom submit script to the image which is used as the image entrypoint. When it a spark-submit container is started, it executes the custom submit script with all given arguments.
% spark-submit
The custom submit script is provided at \Lst{lst:appendix_spark-submit_script}. It provides a simplified interface for the spark-submit executable. Furthermore, it includes all necessary libraries and configuration parameters to perform an application with GPU acceleration enabled. Additionally, if the \texttt{CPU\_ONLY} environment variable is set to \textit{true}, the script will disable GPU acceleration for the given application.
% task-parallelism
This feature is useful when algorithms optimized for task-parallelism are being executed.
% Example
\Lst{st:06_env_depl_submit} demonstrates an usage example of the the custom submit script.
% Exaplin
In the example, the \texttt{train.py} application is executed with a \texttt{--max-depth=5} runtime argument given. All provided arguments are forwarded to the spark-submit executable (explain in SECTION XY). 
% The script
\begin{lstlisting}[label=lst:06_env_depl_submit, caption=Usage of the submit script, language=bash]
$ sh submit.sh train.py --max-depth=5
\end{lstlisting}
% CI pipeline
The script is part of the training state of the CI pipeline which is described in detail in SECTION CI.
\todo{List mit allen envs variablen}


% ===========================================
% ===========================================
\section{Computing Environment}
% SHort intro
The computing environment is deployed as a Docker swarm (described in SECTION AB). It consists of several components which are all deployed as Docker services. The conceptual design is explained in SECTION XY.
% Problem
As mentioned in SECTION XY, components which require GPUs are not created as Docker services. These components are deployed as Docker containers in the same swarm network.
% The whole thing
Overall, the computing environment consists of the following components:
\begin{itemize}
% AM
\item Autonomic Manager
\begin{itemize}
\item Auto-Scaler
\item Prometheus
\item cAdvisor
\item dcgm-exporter
\end{itemize}

% Spark cluster
\item Apache Spark Cluster
\end{itemize}


\subsection{Deployment of the Computing Environment}
%
To simplify the deployment of a stack of services, Docker proved to define all services in a docker-compose file.
% 
The computing environment docker-compose file is defined in \Lst{lst:appendix_env_compose}.
%
It defines all services, except the Apache Spark worker and the dcgm-export because these services require the NVIDIA Docker runtime.
%
To deploy all services defined in the docker-compose file, Docker provides the docker stack command.
%
After the Docker stack has been deployed, the dcgm-exporter container has to be deployed and added to the same network.
%
\Lst{lst:06_env_depl_docker-stack} provides the process how to deploy the stack and the dcgm-exporter container.
\begin{lstlisting}[label=lst:06_env_depl_docker-stack, caption=Auto-Scaler start command, language=sh]
$ docker stack deploy -c docker-compose.yml computing
...
$ docker run -d --rm -p "9400:9400" \
    --runtime=nvidia \
    --name dcgm-exporter \
    --network computing_net \
    nvidia/dcgm-exporter:2.0.13-2.1.1-ubuntu18.04
\end{lstlisting}


\subsection{Autonomic Manager}
The autonomic manager is responsible to monitor the computing environment and scale the number of Apache Spark worker.
It is composed of a monitoring system and the previously mentioned \textit{Auto-Scaler}.
Furthermore, the monitoring system is build of a Prometheus, cadvisor, and dcgm-exporter.
Together, all components build an autonomic manager according to the MAPE architecture.
The monitoring system monitors the components in the computing environment while the \textit{Auto-Scaler} analyses the performance metrics and adapt the number of Apache Spark worker.

% Prometheus:
% - rules
% - targets

% cAdvisor

% dcgm-exporter
% deploy

\subsubsection{Prometheus Target Configuration}

% ==============
% Hands-On Infrastructure Monitoring with Prometheus -> Comparing CPU usage -> PromQL approach
% ==============



% Pull based
As being introduced in \Sec{sec:04_prom}, Prometheus is a pull-based monitoring tool.
% Target list
It requires a list of targets to pull performance metrics from.


% Describe config
\Lst{lst:06_computing_am_prom-config} specifies the scrape configuration of the Prometheus system.
% scrape interval
The \texttt{scrape\_interval} parameter is set to \textit{5s}. This means, that Prometheus scrapes all defined targets every 5 seconds.
% 
Additionally, a the 
% All targets
To scrape all needed performance metrics three different targets are set: cAdvisor, dcgm-exporter, and the Apache Spark master node. The configuration is explained in SECTION TECHNICAL PROM.
% describe
All container related performance metrics are received from the cAdvisor target. The GPU performance metrics are provided by the dcgm-exporter target. As mentioned in SECTION AS, the \textit{Auto-Scaler} needs to know how many Apache Spark worker nodes are active at runtime and how man applications are being executed. This metrics are scraped from the Apache Spark master node which has native support for Prometheus.
% Label
Each target is given a specific label which to identify the source.
% Target config
\begin{lstlisting}[label=lst:06_computing_am_prom-config, caption=Prometheus target configuration in YAML syntax]
global:
  scrape_interval: 5s
 
rule_files:
  - "/etc/prometheus/recording_rules.yml"
 
scrape_configs:
    - job_name: cadvisor
      static_configs:
          - targets: ["cadvisor:8080"]
            labels:
                group: "cadvisor"
 
    - job_name: dcgm-exporter
      static_configs:
          - targets: ["dcgm-exporter:9400"]
            labels:
                group: "dcgm-exporter"
    
    - job_name: spark-master
      metrics_path: /metrics/master/prometheus/
      static_configs:
          - targets: ["spark-master:4040"]
            labels:
              group: "spark"
\end{lstlisting}


\subsubsection{Prometheus Recording Rules Configuration}
% mention the suitable metrics
In SECTION YB, suitable metrics for monitoring the target performance have been introduced.
%
These two metrics are created as recording rules in \Lst{lst:06_env_depl_am_prom-rules}  (see SECTION XY for details about recording rules).
%
\begin{lstlisting}[label=lst:06_env_depl_am_prom-rules, caption=Prometheus target configuration in YAML syntax]
groups:
    - name: performance_usage_percent
      rules:
        - record: instance:worker_nodes:cpu_usage:percent
          expr: SUM(RATE(container_cpu_usage_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s])) BY (image)
        - record: instance:gpu:usage:percent
          expr: SUM(DCGM_FI_DEV_GPU_UTIL{device=~"(?:nvidia4|nvidia5)"}) / 2
\end{lstlisting}


% ===========================================
% ===========================================
\section{GitLab CI/CD Pipeline for Automated Deployment of Machine Learning Applications}
%
The CI pipeline is responsible to test, and submit the application to the Apache Spark cluster.
%
Important to notice, that Python is an interpreted programming language. It is possible to package the Python app as ZIP document or as an egg file. However, to keep the implementation simple, only raw Python files are submitted to the Apache Spark cluster. Therefore, there is no build stage in this CI pipeline architecture.
%
To be able to deploy a \textit{spark-submit} container on the host machine, a GitLab runner (introduced in SECTION XY) instance has to run on the same machine.


\subsection{GitLab Runner}

As being explained in SECTION THEO, GitLab CI/CD jobs are being executed by a GitLab runner.
To deploy a spark-submit container in the swarm network of the Apache Spark cluster, the GitLab needs to run on the same host to get access to the Docker engine.


%
As mentioned before, to deploy a spark-submit node on the host machine, a GitLab Runner instance is needed.
% Docker is needed
To be able to create Docker container in the same swarm network, the GitLab runner needs access to the Docker engine of the host machine.


% In this impl
In this implementation, the GitLab Runner is deployed as a Docker container.
% Docker as parent
The Docker container is build from the \texttt{gitlab/gitlab-runner:latest} image. This image provides an installation of the GitLab runner service and command-line-interface.
% Volumes
To enable Docker support in the context of the GitLab Runner Docker container, the Docker socket has to be mounted to the container. This \textit{socket-binding} approach is explained in detail on the GitLab online documentation\footnote{Building Docker images with GitLab CI/CD  - \url{https://docs.gitlab.com/ee/ci/docker/using_docker_build.html\#use-docker-socket-binding} (Accessed: 2021-01-31)}.


% Create a runner
The \texttt{gitlab-runner} command is set as image entrypoint. It initialises a new GitLab runner on the container.
% ENable docker on gitlab runner
To enable Docker access to the GitLab runner, various settings have to be set. First the docker excutor has to be activated. This enables the runner to perform jobs with user defined Docker images, which is required to use the \textit{spark-submit} image. Second, the runner needs access to the Docker socket as well. Therefore, the mounted Docker socket has to be set as volume for the runner. Lastly, the \texttt{docker:latest} image is used for the container where jobs are performed on. This gives jobs the ability to use the Docker CLI. This ability is needed to run the \texttt{docker run} command from a job.


% /builds volume
The \texttt{/builds} directory of a runner is the location where the source code of a repository is located. By default, this directory is only available to the job container deployed by the runner. To share this container with container deployed by a job, it has to be shared with the host system. Docker container cannot shaire their filesystem with other containers, because Docker run in isolation. This is described in detail in SECTION BLA NEXT.
% the listing
\Lst{lst:06_ci_runner_container_cmd} provides the command to create a GitLab runner in a Docker container.
% token and url
In addition to the previously mentioned configuration settings, a GitLab Runner requires a URL to a GitLab server and a registration token.
%
The registraktion token identifies a specific repository or a GitLab group. Then, the runner can only used from the 
%
This GitLab runner is reposnsible to perform submit Apache Spark application to the cluster. Therefore the spark-submit tag was set, so it only performs CI jobs tagged with this.

\begin{lstlisting}[label=lst:06_ci_runner_container_cmd, caption=CLI command to start a GitLab runner in a Docker container, language=bash]
$ docker run -d \
    --name gitlab-runner \
    --restart always \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /builds:/builds \
    gitlab/gitlab-runner:latest \
    gitlab-runner register \
    -n \
    --name "Spark-Cluster Runner" \
    --executor docker \
    --docker-image docker:latest \
    --docker-volumes /var/run/docker.sock:/var/run/docker.sock \
    --docker-volumes /builds:/builds \
    --docker-privileged=true
    --url https://gitlab.com/ \
    --registration-token mpCBWzZzhaaJrdqjXYZq \
    --tag-list spark-submit
\end{lstlisting}


\subsection{Pipeline Architecture}
%
As explained in SECTION AB, a GitLab CI/CD pipeline is configured by a \texttt{.gitlab-ci.yml} file in the root directory of the source code repository.
%
LST XY shows the .gitlab-ci.yml configuration file. The pipeline consists of a \textit{test} and a \textit{train} stage.


\subsubsection{Test Stage}
% test stage
The first stage is the test stage which consists of a single job called \textit{unit-tests} to perform unit tests.
% The image
It uses the \texttt{python:3.8-slim} Docker image, which has already Python 3.8 installed. To perform the unit tests, first a Python virtual environment with all dependencies is created. Lastly, all tests in the \texttt{tests/} directory are performed using the pytest\footnote{pytest - \url{https://docs.pytest.org/en/latest/} (Accessed: 2021-02-01)} library.


\paragraph{}
It is important to mention, that the train stage depends on the application. This implementation focuses on simplicity and provides a general concept of a train stage which can be used and extended for other projects.


\subsubsection{Train Stage}
% train stage
After the test stage has been performed successfully, the train stage is responsible to perform the Apache Spark application on the Apache Spark cluster.
% Jobs
The train stage consists of a single job called \textit{train-model}.
% submit
This jobs executes the \texttt{submit.sh} build script, which is located in the repository root directory. The script executes the \texttt{docker run} command to deploy a \textit{spark-submit} Docker container in the Apache Spark cluster. It takes the path to the application Python file and all needed parameters as input and forwards them to the \texttt{docker run} command. To be able to execute the applications Python file, it has to be mounted to the host filesystem, which is explained in this section.
% Config
To configure the \textit{spark-submit} container, all configuration parameters listed in TABLE CONFIG PARAM OF SPARK-SUBMIT can be set as environment variables in the \texttt{variables} section of the job.
% Tag
The job has to be tagged with \textit{spark-submit} to get executed by the runner on the host machine.


\paragraph{Creating a mount-point:}
% Isolation
As mentioned before in SECTION OBEN, Docker container run in isolation and do not share the same filesystem with each other.
% The runner
Each runner registered on the GitLab Runner Docker container runs its jobs on separate Docker container on the host system. All commands are being performed in the context of its container.
%
When the \textit{train-job} deploys a \textit{spark-submit} Docker container with the \texttt{docker run} command, it tries to mount the \texttt{/builds} directory from its own filesystem to the \textit{spark-submit} container.
%
To share the source code between the job container and the \textit{spark-submit} container, the \texttt{/builds} directory is mounted to the host system, as being explained in SECTION OBEN.
%
To overcome this problem, the \textit{train-model} job creates a new folder inside the \texttt{/builds} directory as mount-point and moves the application source code files to this location.
%
This mount-point is available to the \textit{spark-submit} container when given as a volume.
%
The approach of a mount-point is mentioned in a discussion on the GitLab website\footnote{Docker volumes not mounted when using docker:dind - \url{https://gitlab.com/gitlab-org/gitlab-foss/-/issues/41227} (Accessed: 2021-02-01)}.

- Screenshot von webui output
