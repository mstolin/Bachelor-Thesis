\chapter{Implementation}
\label{chap:06_implementation}

This chapter explains the implementation details of the conceptual design introduced in \Chap{chap:05_design}.


% ===========================================
% ===========================================
\section{Implementation Environment}
\label{sec:06_impl-env}
% The DGX
To implement this thesis concept, an NVIDIA DGX1\footnote{The Universal System for AI Infrastructure - \url{https://www.nvidia.com/en-us/data-center/dgx-1/} (Accessed: 2021-01-30)} machine is available, which is a shared live-system. Therefore, multiple applications are performing simultaneously on this machine and share the same resources.


\subsection{Technical Details}
% Hardware Resources
The hardware specification of the machine is the following:
\begin{itemize}
\item Memory: 512GB
\item Disk space: 4x 1.92TB (7.68TB total)
\item \hyperlink{abbr:gpu}{GPUs}: 8x NVIDIA Tesla v100, 32GB of memory each, which sums up to 256GB total
\item \hyperlink{abbr:cpu}{CPU}: Dual 20-Core Intel Xeon E5-2698 v4 2.2 GHz
\end{itemize}


% Available software
The following software, which is required for the implementation, is installed on the machine:
\begin{itemize}
\item Ubuntu 18.04.5 LTS
\item Docker version 18.09.4
\end{itemize}


\subsection{NVIDIA Docker Runtime}
% nvidia runtime
The NVIDIA Container Toolkit has to be installed on the host machine to enable \hyperlink{abbr:gpu}{GPU} support for Docker containers. This toolkit provides a runtime library that automatically enables Docker containers to leverage NVIDIA \hyperlink{abbr:gpu}{GPUs}.
% The problem
It is possible to define the runtime of a Docker container with the docker \texttt{run} command using the \texttt{runtime} option. However, this is not supported by Docker services because the Docker \texttt{service} command does not support an option to define the runtime\footnote{docker service create - \url{https://docs.docker.com/engine/reference/commandline/service_create/} (Accessed: 2021-02-21)}. The only remaining option to enable the NVIDIA runtime for Docker services is to set it as the default Docker runtime\footnote{Support for NVIDIA GPUs under Docker Compose - \url{https://github.com/docker/compose/issues/6691} (Accessed: 2021-02-21)}.
% How
How the NVIDIA Container toolkit is installed and the runtime is set as default runtime is thoroughly explained in the NVIDIA Container Toolkit documentation\footnote{NVIDIA Cloud Native technologies documentation - \url{https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html} (Accessed: 2021-01-28)}.

\paragraph{}
% Already installed
The NVIDIA Container Toolkit is installed on the host machine, but the NVIDIA runtime is not set as default runtime.
% The problem
Changing the default runtime requires a restart of the Docker service. Restarting the Docker service is impossible because it requires quitting all running Docker containers on the machine. Therefore, components that require access to \hyperlink{abbr:gpu}{GPU} resources cannot be deployed as Docker services.
% The solution
The solution to this problem is to deploy these components as Docker container instead of Docker services.
% the env
Given this problem, the DCGM-Exporter monitoring agent must be deployed as a Docker container with the NVIDIA runtime enabled.
% Finish
\Lst{lst:06_impl-env_prob_cmd} shows an example of the docker run command to deploy a Docker container using the NVIDIA runtime.
\begin{lstlisting}[label=lst:06_impl-env_prob_cmd, caption=Docker run command to deploy a container using the NVIDIA runtime, language=sh, numbers=none]
$ docker run -d --rm --runtime=nvidia --name dcgm-exporter nvidia/dcgm-exporter:latest
\end{lstlisting}

%\paragraph{No GPU worker for Auto-Scaler}
%The Auto-Scaler will still be implemented using the Docker service approach. The reason is that for the evaluation, only CPU-only worker nodes will be scaled using the Auto-Scaler. Because as mentioned above, only 2 GPUs are available for the implementation and the evaluation.
%However, the Auto-Scaler is still implemented that after enabling NVIDIA as the default runtime it should work.


% ===========================================
% ===========================================
\section{Computing Environment}
\label{sec:06_env}
% SHort intro
The computing environment is deployed as a Docker swarm (described in \Sec{subsec:04_docker_swarm}). It consists of several components that are all deployed as Docker services. The conceptual design is explained in \Sec{sec:05_env}.
% Problem
As mentioned previously, components that require \hyperlink{abbr:gpu}{GPUs} are not created as Docker services. These components are deployed as Docker containers in the same swarm network.
% The whole thing
Overall, the computing environment consists of the following components:
\begin{itemize}
% AM
\item Autonomic Manager
\begin{itemize}
\item Auto-Scaler
\item Prometheus
\item cAdvisor
\item DCGM-Exporter
\end{itemize}

% Spark cluster
\item Apache Spark Cluster
\begin{itemize}
\item Apache Spark master node
\item Apache Spark worker nodes
\item \textit{spark-submit} nodes
\end{itemize}
\end{itemize}


\subsection{Deployment of the Computing Environment}
\label{subsec:06_env_depl} 
% Uses compose
To simplify the deployment of a stack of services, Docker provides to define all services in a \textit{docker-compose}\footnote{Overview of docker-compose - \url{https://docs.docker.com/compose/} (Accesses: 2021-02-10)} file.
% What is compose
The computing environment \textit{docker-compose} file is defined in \Lst{lst:appendix_env_compose}.
%
\Tab{table:06_env_depl_docker-stack} shows all Docker Services defined in the \texttt{docker-compose.yml} with the used Docker image.
%
It defines all services, except the DCGM-Exporter, which requires the NVIDIA Docker runtime.
%
The whole stack can be deployed using the \texttt{docker stack} command. Then, the DCGM-Exporter container has to be deployed and added to the same network.
%
\Lst{lst:06_env_depl_docker-stack} shows the process of how to deploy the stack and the DCGM-Exporter container. First, all services are deployed using the \texttt{docker stack} command. Afterward, the DCGM-Exporter is deployed in a Docker container using the NVIDIA runtime and attached to the same network.
\begin{lstlisting}[label=lst:06_env_depl_docker-stack, caption=Commands to deploy the computing environment, language=sh, numbers=none]
$ docker stack deploy -c docker-compose.yml computing

$ docker run -d --rm -p "9400:9400" \
    --runtime=nvidia \
    --name dcgm-exporter \
    --network computing_net \
    nvidia/dcgm-exporter:2.0.13-2.1.1-ubuntu18.04
\end{lstlisting}
%Table
\begin{table}[]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Service Name  & Docker Image                                  \\ \midrule
auto-scaler   & \texttt{auto-scaler:latest}                            \\
prometheus    & \texttt{prom/prometheus:v2.24.1}                       \\
cadvisor      & \texttt{google/cadvisor:v0.33.3}                       \\
dcgm-exporter & \texttt{nvidia/dcgm-exporter:2.0.13-2.1.1-ubuntu18.04} \\
spark-master  & \texttt{spark-master:3.0.1-hadoop2.7}                  \\
spark-worker  & \texttt{spark-worker:3.0.1-hadoop2.7}                  \\ \bottomrule
\end{tabular}
\caption{The name and Docker image of each Docker service in the computing environment}
\label{table:06_env_depl_docker-stack}
\end{table}


\subsection{Autonomic Manager}
The autonomic manager is responsible for monitoring the computing environment and scaling the number of Apache Spark worker replicas.
It is composed of a monitoring system and the \textit{Auto-Scaler}.
Furthermore, the monitoring system consists of Prometheus, cAdvisor, and DCGM-Exporter components.
Together, all components build an autonomic manager according to the \hyperlink{abbr:mape}{MAPE} architecture.
The monitoring system monitors the components in the computing environment while the \textit{Auto-Scaler} analyses the performance metrics and adapts the number of Apache Spark workers.


\subsubsection{Prometheus Target Configuration}

% ==============
% Hands-On Infrastructure Monitoring with Prometheus -> Comparing CPU usage -> PromQL approach
% ==============



% Pull based
As being introduced in \Sec{sec:04_prom}, Prometheus is a pull-based monitoring tool.
% Target list
It requires a list of targets to pull performance metrics from. How Prometheus is configured is described in \Sec{sec:04_prom_config}.


% Describe config
\Lst{lst:06_computing_am_prom-config} specifies the scrape configuration of the Prometheus system.
% scrape interval
The \texttt{scrape\_interval} parameter is set to \textit{5s} which means, that every 5 seconds, Prometheus scrapes performance metrics from all defined targets.
% Label
Each target is configured using a \hyperlink{abbr:url}{URL} and a label, which is used to identify the metric source.
% All targets
In this configuration, the following targets are defined:
\begin{itemize}
\item \textit{cadvisor}: All container-related performance metrics are received from the cAdvisor target.

\item \textit{dcgm-exporter}: The DCGM-Exporter provides \hyperlink{abbr:gpu}{GPU} related performance metrics.

\item spark-master: The spark-master target provides metrics about the Apache Spark master service. This target is essential for the \textit{Auto-Scaler} to get information about running worker nodes and the number of running applications.
\end{itemize}
\newpage
% Target config
\begin{lstlisting}[label=lst:06_computing_am_prom-config, caption=Prometheus target configuration in YAML syntax]
global:
  scrape_interval: 5s
 
rule_files:
  - "/etc/prometheus/recording_rules.yml"
 
scrape_configs:
    - job_name: cadvisor
      static_configs:
          - targets: ["cadvisor:8080"]
            labels:
                group: "cadvisor"
 
    - job_name: dcgm-exporter
      static_configs:
          - targets: ["dcgm-exporter:9400"]
            labels:
                group: "dcgm-exporter"
    
    - job_name: spark-master
      metrics_path: /metrics/master/prometheus/
      static_configs:
          - targets: ["spark-master:4040"]
            labels:
              group: "spark"
\end{lstlisting}


\subsubsection{Prometheus Recording Rules Configuration}
%
As introduced in \Sec{sec:04_prom_arch}, Prometheus provides the ability to define recording rules. Recording rules can be queried at a much faster speed than PromQL queries because the results of recording rules are saved in the local storage of Prometheus.
%
Therefore, all suitable metrics to monitor the computing environment's performance introduced in \Sec{sec:05_metrics} are defined as recording rules.
%
\Lst{lst:06_env_depl_am_prom-rules} shows the configuration of both performance metrics as recording rules.
%
\begin{lstlisting}[label=lst:06_env_depl_am_prom-rules, caption=Prometheus target configuration in YAML syntax]
groups:
    - name: performance_usage_percent
      rules:
        - record: instance:worker_nodes:cpu_usage:percent
          expr: SUM(RATE(container_cpu_usage_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s])) BY (image)
        - record: instance:gpu:usage:percent
          expr: SUM(DCGM_FI_DEV_GPU_UTIL{device=~"(?:nvidia4|nvidia5)"}) / 2
\end{lstlisting}
% Explain queries
The \hyperlink{abbr:cpu}{CPU} query uses the \texttt{RATE} function to calculate the \hyperlink{abbr:cpu}{CPU} usage per-seconds rate over the last 30 seconds of each available worker node. Then the results are summed up using the \texttt{SUM} function.
% GPU
To get the utilization for two different \hyperlink{abbr:gpu}{GPU} devices, PromQL provides selectors for labels. In the configuration, the \texttt{=\~} selector is used for the device label. This selector selects all values that match the given regular-expression. The given regular expression selects the \hyperlink{abbr:gpu}{GPUs} with the name \textit{nvidia4} and \textit{nvidia5}. Then both utilization values are normalized using the number of \hyperlink{abbr:gpu}{GPUs}.


% ===========================================
% ===========================================
\section{Auto-Scaler}
% SHort intro 
The \textit{Auto-Scaler} is one of the two main modules of the autonomic manager. It is responsible for analyzing performance metrics, planning scaling actions according to the performance metrics, and execute scaling actions to adapt the number of Apache Spark workers in the computing environment.
% Mention conceptual design
It is implemented as a custom module in Python 3.8 and deployed as a Docker image using a custom Dockerfile to deploy the \textit{Auto-Scaler} running in a Docker container.


% Concept
The \textit{Auto-Scaler} integrates the Analyse, Plan, and Execute phase of the \hyperlink{abbr:mape}{MAPE} architecture. It furthermore scales the Apache Worker service horizontally by automatically altering the number of replicas.
% A Loop
The \textit{Auto-Scaler} runs as a background service that periodically performs each phase in sequence.
% Configuration
It can be configured with a special \hyperlink{abbr:yaml}{YAML} file, whereas the path to the configuration file has to be set as an argument.
% Start the sclaer
The \textit{Auto-Scaler} can be started with the command demonstrated in \Lst{lst:06_auto-scaler_start}.
\begin{lstlisting}[label=lst:06_auto-scaler_start, caption=\textit{Auto-Scaler} start command, language=sh, numbers=none]
$ python3 run.py --config=config.yml
\end{lstlisting}


\subsection{Configuration}
% Design
\paragraph{}
The configuration parameters of the \textit{Auto-Scaler} were introduced in \Sec{subsubsec:05_am_auto-scaler_config}.
% Its yaml
The configuration is defined in the \hyperlink{abbr:yaml}{YAML} file format and is structured in three sections: General, metrics, and worker.
% The sections
\begin{itemize}
% General
\item General:
The general section defines the length of seconds after an interval is performed, the length in seconds of the cooldown period, the recurrence factor, and the Prometheus \hyperlink{abbr:url}{URL}.

% Metrics
\item Metrics:
Metrics is a list of performance metric configuration parameters. A performance metric requires a query in the PromQL syntax. Additionally, a target utilization and the minimum and maximum utilization of the performance metric is required.

% Worker
\item Worker:
To scale the worker service replicas, the \textit{Auto-Scaler} needs to know the service name. The minimum and the maximum number of concurrent worker nodes need to be defined to prevent an overhead of concurrently running worker nodes.
\end{itemize}
% The yaml file
\Lst{lst:06_auto-scaler_config_service_example} provides an example of a configuration for scaling a Docker service.
%
To scale a service using the Docker Software Development Kit(\hyperlink{abbr:sdk}{SDK's}) service \hyperlink{abbr:api}{API}, only the service name is required.
%
Docker related configuration settings (e.g., environment variables) are defined in the \texttt{docker-compose.yml} explained in \Sec{subsec:06_env_depl}.
% Example
\begin{lstlisting}[label=lst:06_auto-scaler_config_service_example, caption=\textit{Auto-Scaler} configuration for scaling Docker Services]
general:
  interval_seconds: 5
  cooldown_period_seconds: 180
  recurrence_factor: 3
  prometheus_url: "http://localhost:9090"
 
metrics:
  cpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.5
    thresholds:
      min: 0.2
      max: 0.6
 
worker:
  service_name: "computing_spark-worker"
  thresholds:
    min: 1
    max: 30
\end{lstlisting}


\subsection{Scaling Apache Worker Nodes}
% Periodically performing
The \textit{Auto-Scaler} performs periodically. Each period fetches performance metrics, analyses the metrics, plans scaling-actions, and executes them if necessary.
% AP jobs
It uses the \textit{APScheduler}\footnote{\url{https://pypi.org/project/APScheduler/} (Accessed: 2021-01-26)} Python library that provides an \hyperlink{abbr:api}{API} to schedule jobs periodically.


\subsubsection{Estimation of Necessary Scaling Actions}
To estimate if a scaling action is necessary, the \textit{Auto-Scaler} uses the Scaling Heat algorithm (introduced in \Sec{alg:04_scal-heat_concept_algo}).
% input
The algorithm uses the utilization given by a performance metric, the lower- and upper-threshold, and the calculated heat of the last iteration as input parameters.
% utilization
The utilization is received from the Prometheus \hyperlink{abbr:http}{HTTP} \hyperlink{abbr:api}{API}. Furthermore, the \textit{Auto-Scaler} fetches the all given performance metrics in each iteration. It calculates the heat value for each single performance metric.
% the limit
If a performance metrics' heat value is equal to the recurrence factor $n$ ($+n$ or $-n$), a scaling-action is executed.
% cooldown
This phase is not performed if a cooldown period of a previous iteration is active.


\subsubsection{Calculating the Number of Needed Worker Nodes}
To calculate the number of needed Apache Spark workers, the \textit{Auto-Scaler} uses the \hyperlink{abbr:khpa}{KHPA} algorithm (introduced in \Sec{sec:04_background_khpa}).
As input parameters, the algorithm takes the current number of active worker nodes, the utilization given by the performance metric, and the target utilization


% Explain implementation
\Lst{lst:06_auto-scaler_plan_khpa} shows the implementation of the \hyperlink{abbr:khpa}{KHPA} algorithm in Python.
To receive the number of active worker nodes, the Apache Spark master node must be defined as a Prometheus target. This enables to fetch the \texttt{metrics\_master\_aliveWorkers\_Value} metric from the Prometheus \hyperlink{abbr:http}{HTTP} \hyperlink{abbr:api}{API} which returns the number of alive Apache Spark worker nodes in the cluster.

The utilization used by the \hyperlink{abbr:khpa}{KHPA} algorithm, has already been received in the previous phase and is defined in the configuration file. Lastly, the target utilization, defined in the \textit{Auto-Scaler} configuration, is defined as an argument for the  \hyperlink{abbr:khpa}{KHPA} implementation. 
% The python impl
\begin{lstlisting}[label=lst:06_auto-scaler_plan_khpa, caption=KHPA implementation using Python 3.8, language=Python]
def calculate_number_of_needed_worker(active_worker: int,
    utilization: float,
    target_utilization: float):
  return math.ceil(
    active_worker * (utilization / target_utilization))
\end{lstlisting}


\subsubsection{Performing a Scaling Action}
After the number of needed Apache Spark worker nodes is calculated, the \textit{Auto-Scaler} scales the worker service replicas to reach the desired performance goal.
% Docker lib
To scale a Docker service using Python, Docker provides a Python library for the Docker Engine\footnote{Docker SDK for Python 4.4.1 Documentation - \url{https://docker-py.readthedocs.io/en/4.4.1/} (Accessed: 2021-01-05)}. 
% How
The \hyperlink{abbr:sdk}{SDK} provides a services \hyperlink{abbr:api}{API} that can be used to scale given Docker Service replicas.
% Cooldown
After a scaling action has been executed successfully, a cooldown period is activated. The duration of the cooldown period is set in the configuration. During cooldown, no scaling actions are executed in subsequent iterations.


\subsubsection{Removing Apache Spark worker}
% RDD intro
As introduced in \Sec{subsec:04_spark_pr-model}, Apache Spark is built on top of a data structure called \hyperlink{abbr:rdd}{RDD}. An \hyperlink{abbr:rdd}{RDD} consists of different partitions that are distributed across multiple executors in the cluster.
% No downsscaling
If an executor stores essential data, the executor's worker node cannot be removed during runtime. If the worker is removed during runtime, it will cause the loss of necessary \hyperlink{abbr:rdd}{RDD} partitions.
% Check if worker is busy
Therefore, worker nodes will not be removed if applications are actively performing on the Apache Spark cluster.
% The metric
Prometheus provides a metric called \texttt{metrics\_master\_apps\_Value}, which returns the number of running applications on the cluster. To fetch this metric, the Apache Spark master node has to be configured as a Prometheus target.
% auto-scaler
The \textit{Auto-Scaler} only reduces the number of worker replicas if this metric returns 0, which represents that no applications are executed.


\subsection{Docker Image}
% Why dockerfile
A Docker image is needed to deploy the \textit{Auto-Scaler} as a Docker service in the computing environment. Therefore a custom Dockerfile is created to build an \textit{Auto-Scaler} Docker image.
% Explain Dockerfile
\Lst{lst:06_auto-scaler_dockerfile} shows the \textit{Auto-Scaler} Dockerfile implementation. The \texttt{python:3} Docker image is set as the parent image. 
Next, the \textit{Auto-Scaler} source code is copied to the image. Then, a Python virtual environment is created with all dependencies installed.
As entrypoint, the Docker image starts the \textit{Auto-Scaler} process.
\begin{lstlisting}[label=lst:06_auto-scaler_dockerfile, caption=\textit{Auto-Scaler} Dockerfile]
FROM python:3
 
WORKDIR /usr/src/auto_scaler
 
# Copy the python module
COPY setup.py .
COPY src src/
 
# Update and install packages
RUN pip3 install -e .
 
ENTRYPOINT [ "python3", "src/run.py" ]
\end{lstlisting}


% Building the Docker image
To build the \textit{Auto-Scaler} Docker image, a build script was implemented. \Lst{lst:06_auto-scaler_build} shows the build script implementation.
The script takes the version of the \textit{Auto-Scaler} module as an input attribute. It uses the version to tag the Docker image.
It builds two versions of the \textit{Auto-Scaler}, one tagged with the current \textit{Auto-Scaler} version (e.g., \textit{auto-scaler:1.0}) and one tagged as \textit{latest} (e.g., \textit{auto-scaler:latest}). 
% How to use the script
The script can be used from the command line as demonstrated in \Lst{lst:06_auto-scaler_build-use}.
\begin{lstlisting}[label=lst:06_auto-scaler_build-use, caption=\textit{Auto-Scaler} build-script usage, language=sh, numbers=none]
$ sh build-image.sh "1.0"
\end{lstlisting}
\newpage
\begin{lstlisting}[label=lst:06_auto-scaler_build, caption=\textit{Auto-Scaler} build script, language=sh]
#!/bin/bash
 
if [ $# -ge 1 ]
  then
    VERSION=$1
 
    PWD=$(pwd)
 
    CI_REGISTRY=${CI_REGISTRY-local}
    IMAGE_TAG_PATH="${CI_REGISTRY}/cci/distributed-computing-framework"
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:$VERSION \
      $PWD
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:latest \
      $PWD
  else
    echo "No arguments supplied\n"
      echo "Use the script as follows:"
      echo "build-image.sh <VERSION>"
      exit 1
fi
\end{lstlisting}



% ===========================================
% ===========================================
\section{Apache Spark Cluster with GPU Acceleration}
\label{sec:07_spark}
The conceptual design of the Apache Spark cluster is introduced in \Sec{sec:05_spark}. It consists of a single master node, a dynamic number of worker nodes, and \textit{spark-submit} nodes. A \textit{spark-submit} node is deployed whenever an application is submitted to the cluster.
% standalone mode
All master and worker nodes are deployed in standalone mode (described in \Sec{subsec:04_spark_standalone}).


\subsection{Apache Spark Base Image}
All services in the Apache Spark cluster share the same dependencies. To simplify the creation of the Docker images, a base image is created. This base image serves as the parent image for all other images.
%
\Lst{lst:appendix_spark_base_dockerfile} provides the implementation of the \textit{base-image} Dockerfile.
%
To install a specific Apache Spark version on the Docker image, the \texttt{SPARK\_VERSION}, and \texttt{HADOOP\_VERSION} build arguments are provided. The build process of the images is explained in \Sec{subsec:07_spark_building-images}.
%
The base image uses the \texttt{nvidia/cuda:11.0-devel-ubuntu16.04} Docker image as the parent image, which is provided by NVIDIA and has already installed the NVIDIA \hyperlink{abbr:gpu}{GPU} driver and \hyperlink{abbr:cuda}{CUDA} toolkit dependencies.
%
Additionally, the \textit{spark-base} Dockerfile installs all required dependencies to run a standalone Apache Spark node with \hyperlink{abbr:gpu}{GPU} acceleration introduced in \Sec{subsec:04_rapids_req}. This includes Apache Spark, \hyperlink{abbr:jre}{JRE} 1.8, Python3, \hyperlink{abbr:gpu}{GPU} discovery script, RAPIDS Java binary, cuDF Java binary, XGBoost4j binary, and XGBoost4j-spark binary.


\subsection{Apache Spark Master Image}
% Intro Dockerfile
The Apache Spark master node Docker image is created from a custom Dockerfile (see \Lst{lst:appendix_spark_master_dockerfile}).
% Base image
This image is built on top of the \textit{spark-base} image and therefore already has all dependencies installed.
% Ports
It configures the Apache Spark master port to 7077 and the port for the web user interface to 4040.
% entrypoint
Additionally, the \texttt{start-master.sh} script is set as the entrypoint. Therefore, when a service is built from this image, it will automatically start an Apache Spark master node in standalone mode.


\subsection{Apache Spark Worker Image}
% Intro
The Apache Spark worker image is created from a custom Dockerfile as well.
%
This Dockerfile uses the \textit{spark-base} image as the parent.
% Homo
As mentioned in \Sec{subsec:02_foundations_scalability_horizontal-scaling}, the horizontal scaling approach is more effective with homogeneous nodes. Each Apache Spark worker node is being created from the same Docker image given the same resources.
%
Therefore, no additional dependencies have to be installed.
%
To configure the worker nodes' resources, the \texttt{spark-env.sh} is copied to the Apache Spark \texttt{conf/} folder. \Lst{lst:06_env_depl_worker-env} shows the configuration file. It sets the number of \hyperlink{abbr:gpu}{GPUs} for each executor and the path to the \hyperlink{abbr:gpu}{GPU} discovery script on the node host.
%
The entrypoint is set to \texttt{start-slave.sh} to start an Apache Spark worker in standalone mode. To connect to the Apache Spark master node, the \hyperlink{abbr:uri}{URI} has to be set as an environment variable.
\begin{lstlisting}[label=lst:06_env_depl_worker-env, caption=Environment configuration for all worker nodes, language=sh, numbers=none]
SPARK_WORKER_OPTS="-Dspark.worker.resource.gpu.amount=1 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
\end{lstlisting}


\subsection{Apache Spark Submit Image}
% Intro
The \textit{spark-submit} Docker image is responsible for executing the spark-submit executable (introduced in \Sec{subsubsec:04_spark_standalone_submit}) to run an Apache Spark application on the cluster.
% Parent
The spark-submit Dockerfile uses the \textit{spark-base} image as parent.
% submit
It copies a custom submit script to the image filesystem which is used as the image entrypoint. When a \textit{spark-submit} container is started, it executes the custom submit script with all provided arguments.


\subsubsection{Custom Submit Script}
% spark-submit
The custom submit script is provided at \Lst{lst:appendix_spark-submit_script}. It provides a simplified interface for the spark-submit executable. Furthermore, it includes all necessary configuration parameters to perform an application with \hyperlink{abbr:gpu}{GPU}-acceleration enabled. Additionally, if the \texttt{CPU\_ONLY} environment variable is set to \textit{true}, the script disables \hyperlink{abbr:gpu}{GPU}-acceleration for the given application.
% task-parallelism
This feature is useful when algorithms optimized for task-parallelism are being executed.
% Example
\Lst{lst:06_env_depl_submit} demonstrates a usage example of the custom submit script.
% Exaplin
As arguments, the script takes the path of the application executable file and all needed application arguments as input. All arguments are forwarded to the spark-submit executable, which executes the applications using the configuration provided by all environment variables. In the example, the \texttt{train.py} application is executed with a \texttt{max-depth} argument. 
% The script
\begin{lstlisting}[label=lst:06_env_depl_submit, caption=Usage of the submit script, language=bash, numbers=none]
$ sh submit.sh train.py --max-depth=50
\end{lstlisting}


\subsubsection{Environment Variables}
% The env arguments
The configuration of the underlying spark-submit executable can be configured by providing the environment variables listed in \Tab{table:06_env_depl_submit-envs}. The table shows the environment variable name and the default value if given.
% master
The only environment variable which is required is \texttt{SPARK\_MASTER\_URI}. This variable needs the \hyperlink{abbr:uri}{URI} of the Apache Spark master node.
% CPU_ONLY
If the \texttt{CPU\_ONLY} variable is set to \textit{true}, the custom submit script executes the spark-submit executable without enabling \hyperlink{abbr:gpiu}{GPU}-acceleration using the RAPIDS plugin.
% spark configs
\texttt{DRIVER\_MEMORY} sets the available memory for the driver process, and \texttt{EXECUTOR\_MEMORY} sets the memory for each executor process.
% RAPIDS vars
The RAPIDS environment variables are described in detail in the online documentation\footnote{spark-rapids Configuration - \url{https://nvidia.github.io/spark-rapids/docs/configs.html} (Accessed: 2021-02-09)}.
% ENV TABLE
\begin{table}[]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Name                                                   & Default Value     \\ \midrule
\texttt{SPARK\_MASTER\_URI}           & \textit{required} \\
\texttt{CPU\_ONLY}           & false \\
\texttt{DRIVER\_MEMORY}               & 4g                \\
\texttt{EXECUTOR\_MEMORY}             & 8g                \\
\texttt{RAPIDS\_GPU\_ALLOC\_FRACTION} & 1                 \\
\texttt{RAPIDS\_INCOMPATIBLE\_OPS}    & false             \\
\texttt{RAPIDS\_DEBUG}                & NONE              \\
\texttt{RAPIDS\_GPU\_POOL}            & ARENA             \\ \bottomrule
\end{tabular}
\caption{\textit{spark-submit} image environment variables}
\label{table:06_env_depl_submit-envs}
\end{table}


\subsubsection{Executing an Application with the spark-submit Image}
% 
\Lst{lst:06_env_depl_submit_example} provides an example of how to perform an Apache Spark application using the \textit{spark-submit} Docker image.
% envs
The \texttt{SPARK\_MASTER\_URI} and \texttt{EXECUTOR\_MEMORY} environment variables are set using the \texttt{-e} flag.
%
It is important to set the \texttt{--rm} flag, which automatically removes the Docker container after submitting the script.
% listing
\begin{lstlisting}[label=lst:06_env_depl_submit_example, caption=Example of the spark-submit image, language=bash, numbers=none]
$ docker run \
  --rm \
  --network computing_net \
  -e SPARK_MASTER_URI=spark://spark-master:7077 \
  -e EXECUTOR_MEMORY=16g \
  spark-submit:latest \
  train.py \
  --max-depth=50
\end{lstlisting}


\subsection{Building Apache Spark Docker Images}
\label{subsec:07_spark_building-images}
% Intro
A script is used to automate the build process of all images.
% source
The source code of the build script is provided at \Lst{lst:appendix_spark-build-script}.
% params
To specify which Apache Spark version is installed on all images, the script takes the Apache Spark and Hadoop version as input arguments which are required by the \textit{spark-base} image.
% Taging
Additionally, the build script tags each image with the provided Apache Spark and Hadoop version.
% Example
An example of the usage is provided in \Lst{lst:06_env_depl_build_example}.
% Explain
To build the images, the example uses Apache Spark version 3.0.1 and Hadoop 2.7.
% example
\begin{lstlisting}[label=lst:06_env_depl_build_example, caption=Example of the spark-submit image, language=bash, numbers=none]
$ sh build-images.sh 3.0.1 2.7
\end{lstlisting}


% ===========================================
% ===========================================
\section{GitLab CI/CD Pipeline for Automated Deployment of Machine Learning Applications}
\label{sec:06_pipeline}
%
The \hyperlink{abbr:ci}{CI} pipeline is responsible to automatically validate, and submit an application to the Apache Spark cluster whenever a change is committed to the source code of the application.
% Build
It is important to note, that Python is an interpreted programming language which is not compiled into a binary file. However, Apache Spark supports \texttt{.zip}, \texttt{.egg}, and raw \texttt{.py} files when submitting a Python application. To keep the implementation simple, only raw \texttt{.py} files are submitted to the Apache Spark cluster. Therefore, there is no build stage in this \hyperlink{abbr:ci}{CI} pipeline architecture.
%
The \hyperlink{abbr:ci}{CI} pipeline is implemented using GitLab CI/CD (introduced in \Sec{sec:04_background_gitlab}).
%
To perform \hyperlink{abbr:ci}{CI}/\hyperlink{abbr:cd}{CD} jobs on the host machine, a GitLab runner (introduced in \Sec{sec:04_background_gitlab_job}) instance has to run on the same machine.


\subsection{GitLab Runner}
\label{subsec:06_pipeline_runner}
To deploy a \textit{spark-submit} container to the Apache Spark cluster's swarm network through a \hyperlink{abbr:ci}{CI}/\hyperlink{abbr:cd}{CD} job, the GitLab Runner needs to run on the same host with access to the overlaying Docker engine.


% In this impl
In this implementation, a GitLab Runner is deployed inside a Docker container.
% Docker as parent
The Docker container is built from the \texttt{gitlab/gitlab-runner:latest} image.
% Volumes
To enable Docker support in the context of the GitLab Runner Docker container, the Docker socket has to be mounted to the container. The \textit{socket-binding} approach is explained in detail on the GitLab online documentation\footnote{Building Docker images with GitLab CI/CD  - \url{https://docs.gitlab.com/ee/ci/docker/using_docker_build.html\#use-docker-socket-binding} (Accessed: 2021-01-31)}.


\subsubsection{Create a GitLab Runner Instance}
% Intro
\Lst{lst:06_ci_runner_container_cmd} shows the command to create a GitLab runner in a Docker container.
% socket binding
The command creates a Docker container using the \texttt{gitlab/gitlab-runner:latest} image. This image provides an installation of the GitLab runner service and \hyperlink{abbr:cli}{CLI}. Additionally, the Docker Engine and the \texttt{/build} directory are mounted to the container. The reason to mount the \texttt{/build} directory to the host filesystem is explained in \Sec{subsec:06_pipeline_share}.
% Create image
Next, the \texttt{gitlab-runner register} command is performed in the context of the Docker container.
% Settings
To enable Docker access for the GitLab Runner, various settings have to be set. First, the docker executor has to be activated. This enables the runner to perform jobs with access to user defined Docker images, which is required to deploy a \textit{spark-submit} Docker container from a \hyperlink{abbr:ci}{CI}/\hyperlink{abbr:cd}{CD} job. Second, the \texttt{docker:latest} image is set as Docker image for \hyperlink{abbr:ci}{CI}/\hyperlink{abbr:cd}{CD} job containers which enables to perform Docker \hyperlink{abbr:cli}{CLI} commands from \hyperlink{abbr:ci}{CI}/\hyperlink{abbr:cd}{CD} job containers.
%
The support of the Docker \hyperlink{abbr:cli}{CLI} within \hyperlink{abbr:ci}{CI}/\hyperlink{abbr:cd}{CD} jobs is required to deploy a \textit{spark-submit} container.
\newpage
% the example
\begin{lstlisting}[label=lst:06_ci_runner_container_cmd, caption=CLI command to start a GitLab runner in a Docker container, language=bash, numbers=none]
$ docker run -d \
    --name gitlab-runner \
    --restart always \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /builds:/builds \
    gitlab/gitlab-runner:latest \
    gitlab-runner register \
    -n \
    --name "Spark-Cluster Runner" \
    --executor docker \
    --docker-image docker:latest \
    --docker-volumes /var/run/docker.sock:/var/run/docker.sock \
    --docker-volumes /builds:/builds \
    --docker-privileged=true
    --url https://gitlab.com/ \
    --registration-token mpCBWzZzhaaJrdqjXYZq \
    --tag-list spark-submit
\end{lstlisting}


\subsection{Pipeline Architecture}
%
As explained in \Sec{subsec:04_background_gitlab_pipeline}, a GitLab CI/CD pipeline is configured by a \texttt{.gitlab-ci.yml} file in the source code repository's root directory.
%
\Lst{lst:appendix_ci-pipeline_job} shows the \texttt{.gitlab-ci.yml}  configuration file of this implementation. The pipeline consists of a \textit{test} and a \textit{train} stage, as explained in the conceptual design in \Sec{sec:05_pipeline}.


\subsubsection{Test Stage}
% test stage
The first stage is the test stage, which consists of a single job called \textit{unit-tests}.
% The image
It uses the \texttt{python:3.8-slim} Docker image, which already has Python 3.8 installed. To perform the tests, first a Python virtual environment with all dependencies is created. Lastly, all tests in the \texttt{tests/} directory are performed using the pytest\footnote{pytest - \url{https://docs.pytest.org/en/latest/} (Accessed: 2021-02-01)} library.


\paragraph{}
It is important to mention that the test stage depends on the application. This implementation focuses on simplicity and provides a general concept of a test stage that can be used and extended for other projects.


\subsubsection{Train Stage}
% train stage
After the test stage has been performed successfully, the train stage is responsible for submitting the Apache Spark application to the Apache Spark cluster.
% Jobs
The train stage consists of a single job called \textit{train-model}.
% submit
This job executes the \texttt{submit.sh} build script (see \Lst{lst:06_ci_pipeline_submit}), which is located in the repository root directory. The script executes the \texttt{docker run} command to deploy a \textit{spark-submit} Docker container in the Apache Spark cluster. It takes the path to the application Python file and all needed parameters as input and forwards them to the \texttt{docker run} command.
% Config
To configure the \textit{spark-submit} container, all configuration parameters listed in \Tab{table:06_env_depl_submit-envs} can be set as environment \texttt{variables} in the \hyperlink{abbr:ci}{CI}/\hyperlink{abbr:cd}{CD} job variables section.
% Tag
To perform this job with the in \Sec{subsec:06_pipeline_runner} created runner, the \textit{train-model} job has to be tagged with the \textit{spark-submit} tag.
%
\begin{lstlisting}[label=lst:06_ci_pipeline_submit, caption=Submit script to execute \texttt{docker run} with all needed configuration parameters, language=bash]
#!/bin/bash
 
DRIVER_MEMORY=${DRIVER_MEMORY-4g}
EXECUTOR_MEMORY=${EXECUTOR_MEMORY-8g}
RAPIDS_GPU_ALLOC_FRACTION=${RAPIDS_GPU_ALLOC_FRACTION-1}
RAPIDS_INCOMPATIBLE_OPS=${RAPIDS_INCOMPATIBLE_OPS-"false"}
RAPIDS_DEBUG=${RAPIDS_DEBUG-"NONE"}
RAPIDS_GPU_POOL=${RAPIDS_GPU_POOL-"ARENA"}
 
docker run \
  --rm \
  --network $NETWORK \
  -v "${MOUNT_POINT}:/mnt" \
  -e SPARK_MASTER_URI=$SPARK_MASTER_URI \
  -e CPU_ONLY=$CPU_ONLY \
  -e DRIVER_MEMORY=$DRIVER_MEMORY \
  -e EXECUTOR_MEMORY=$EXECUTOR_MEMORY \
  -e RAPIDS_GPU_ALLOC_FRACTION=$RAPIDS_GPU_ALLOC_FRACTION \
  -e RAPIDS_INCOMPATIBLE_OPS=$RAPIDS_INCOMPATIBLE_OPS \
  -e RAPIDS_DEBUG=$RAPIDS_DEBUG \
  -e RAPIDS_GPU_POOL=$RAPIDS_GPU_POOL \
  local/cci/distributed-computing-framework/spark-images/spark-submit:3.0.1-hadoop2.7 \
  $@
\end{lstlisting}


\paragraph{}
To mount the application source code to the \textit{spark-submit} container, it has to be available to the host file system, which is explained in \Sec{subsec:06_pipeline_share}.
% location
The application source code is located at \texttt{/builds/<GITLAB\_USERNAME>/<PROJECT\_NAME>} on the \textit{train-model} job Docker container, where \textit{GITLAB\_USERNAME} is the name of the project's developer and \textit{PROJECT\_NAME} the name of the project on GitLab. This directory has to be shared with the \textit{spark-submit} container.
%
In the submit script at \Lst{lst:06_ci_pipeline_submit}, this directory is saved as an environment variable called \texttt{MOUNT\_POINT} and mounted to the \texttt{/mnt} directory on the \textit{spark-submit} container. Then, inside the \textit{spark-submit} container, the application source code is located at \texttt{/mnt}.
%
This concept is discussed on the GitLab website\footnote{Docker volumes not mounted when using docker:dind - \url{https://gitlab.com/gitlab-org/gitlab-foss/-/issues/41227} (Accessed: 2021-02-01)}.


\subsection{Sharing the Applications Source Code between Docker Containers}
\label{subsec:06_pipeline_share}
% Whats the problem
Docker containers run in isolation and do no share their local filesystem with other containers.
% 
When the \textit{train-model} job performs the \texttt{docker run} command to deploy a \textit{spark-submit} container, it executes the command in the context of its container but deploys the container using the host Docker engine. Therefore, the \texttt{/builds} directory where the source code is located cannot be shared as a volume with the \textit{spark-submit} container.
% 
To overcome this problem, the \texttt{/builds} directory has to be mounted to the host filesystem. The \textit{train-model} job can then mount the \texttt{/builds} directory from the host filesystem to the \textit{spark-submit} container.
%
To make the \texttt{/builds} directory available on the host filesystem, the \textit{train-model} job mounts the directory to the GitLab runner container filesystem when the GitLab runner starts the job in a separate Docker container. The GitLab runner container mounts the \texttt{/builds}  directory to the host filesystem, as explained in \Sec{subsec:06_pipeline_runner}.
