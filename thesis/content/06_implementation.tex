\chapter{Implementation}
\label{chap:06_implementation}

This chapter explains the implementation process of the conceptual detail introduced in \Chap{chap:05_design}.


% ===========================================
% ===========================================
\section{Implementation Environment}
% DGX
To implement the introduced concept, a NVIDIA DGX machine is available.
% Live system
This system is a shared live system. Many applications from different departments are performing on the machine. All applications are sharing the same resources prided by the machine.
% GPUs
Overall two of eight GPUs are available for the implementation of this thesis.


\subsection{Technical Details}
% Hardware Resources
The hardware specification of the machine is the following:
\begin{itemize}
\item 11GB RAM
\item 300 TB DIsk space 
\item 8x NVIDIA Tesla GPU a 32GB
\item 500x Intel CPU COre 29876
\end{itemize}


% Available software
The following software is installed on the machine:
\begin{itemize}
\item Ubuntu 30.5
\item Docker 19.5
\item NVIDIA Docker runtime 1
\end{itemize}


\subsection{NVIDIA Runtime Problem Statement}
% nvidia runtime
To enable GPU support for Docker containers, the NVIDIA Container Toolkit\footnote{NVIDIA Cloud Native technologies documentation - \url{https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html} (Accessed: 2021-01-28)} has to be installed on the host machine. This toolkit provides a runtime library which automatically enables Docker containers to leverage NVIDIA GPUs.
% The problem
It is possible to define the runtime of a Docker container with the docker \texttt{run} command. However, this is not supported for Docker services. To deploy Docker services with the NVIDIA runtime enabled, the NVIDIA runtime has to be set as the default runtime for Docker.
% Already installed
On the host machine, the NVIDIA Container Toolkit is installed, however the NVIDIA runtime is not set as default runtime.
% The problem
Changing the default runtime requires a restart of the Docker service. Restarting the Docker service is not possible because it requires to quit all running Docker containers on the machine. Therefore, components which require access to GPU resources (Apache Spark worker nodes and the dcgm-exporter) cannot be deployed as Docker services.
% The solution
The solution to this problem is, to deploy these components as Docker container instead of Docker services.
% Finish
Given this problem statement, the \textit{Auto-Scaler} has to be implemented to use the Docker container API instead of the Docker service API to create Apache Spark worker nodes in the environment. Additionally, the dcgm-exporter Docker container has to be created manually and cannot be created with the Docker stack command.


%\subsection{Development Steps}
%Vll auch irgendwie ala Scrum etc
%Wasserfall modell ....

%\begin{enumerate}
% 1
%\item Implementing the Auto-Scaler Python module
% 2
%\item Dockerizing the Auto-Scaler
% 3
%\item Creating the computing environment in a Docker swarm
%\begin{itemize}
%\item Create an Apache Spark cluster with GPU acceleration
%\item Create an autonomic manager with a monitoring system and an Auto-Scaler
%\end{itemize}
% 4
%\item Implement a GitLab CI pipeline to deploy Apache Spark applications to the cluster
%\end{enumerate}


% ===========================================
% ===========================================
\section{Auto-Scaler}
% SHort intro 
The Auto-Scaler is a main module of the autonomic manager. It is responsible to analyse performance metrics, plan scaling actions in accordance to the performance metrics and execute scaling actions to adapt the number of Apache Spark worker in the computing environment.
% Mention conceptual design
It is implemented as a custom module in Python. There is no existing solution that satisfies the requirements of the conceptual Auto-Scaler design.
% DOcker
Furthermore to deploy the Auto-Scaler as a Docker container, a Dockerfile is created to build a Docker image.


% Concept
The Auto-Scaler integrates the Analyse, Plan, and Execute phase of the MAPE architecture. Furthermore it scales the Apache Worker horizontally by adding and removing Apache Spark worker Docker containers.
% A Loop
The Auto-Scaler runs as a background services that periodically performs each phase in order.
% Configuration
A file which defines runtime configuration can be set.
% Start the sclaer
The Auto-Scaler can be started with the following command:
\begin{lstlisting}[label=lst:06_auto-scaler_start, caption=Auto-Scaler start command, language=sh]
$ python3 run.py --config=config.yml
\end{lstlisting}


\subsection{Technical Background}
% Technical background
The Auto-Scaler is implemented in Python 3.8. It consists of different classes, each having different responsibilities.
% A list of the Python libs
The following Python libraries have been used for the implementation:
% The list
\begin{itemize}
\item aiohttp\footnote{\url{https://pypi.org/project/aiohttp/} (Accessed: 2021-01-26)}
\item APScheduler\footnote{\url{https://pypi.org/project/APScheduler/} (Accessed: 2021-01-26)}
\item docker\footnote{\url{https://pypi.org/project/docker/} (Accessed: 2021-01-26)}
\item PyYAML\footnote{\url{https://pypi.org/project/PyYAML/} (Accessed: 2021-01-26)}
\end{itemize}


\subsection{Configuration}
% Design
\paragraph{}The configuration parameter for the Auto-Scaler have been introduced in \Sec{subsubsec:05_am_auto-scaler_config}.
% Its yaml
All parameter are defined in the YAML file format
% The yaml file
\Lst{lst:06_auto-scaler_config_example} provides an example of a configuration.
% Overall structure
Overall, a configuration file is structured in three sections: General, metrics, and worker.
% The sections
\begin{itemize}
% General
\item General:
The general section defines details about the scaling and heat algorithm and the Prometheus URL.

% Metrics
\item Metrics:
Metrics is a list of performance metrics configuration parameters. A performance metric requires a query in the PromQL syntax. Additionally a target utilization is needed and the minimum and maximum utilization of the performance metric.

% Worker
\item Worker:
To scale the replicas of the Apache Spark worker service, the name of the Docker service needs to be set. In addition, the minimum and maximum number of concurrent worker nodes needs to be defined to prevent an overhead of running worker nodes.
\end{itemize}

% The table
\Tab{table:06_auto-scaler_config_parameter} lists all available configuration parameters. It describes the value type and the default value of each parameter. Some parameters are required to be defined by the administrator and have no default value.
% Parameter table
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Name                      & Type    & Default           \\ \midrule
\multicolumn{3}{l}{\textbf{general}}                    \\ \midrule
interval\_seconds         & Integer & 1                 \\
cooldown\_period\_seconds & Integer & 180               \\
recurrence\_factor        & Integer & 1                 \\
prometheus\_url           & String  & \textit{Required} \\
\multicolumn{3}{l}{\textbf{metrics}}                    \\ \midrule
query                     & String  & \textit{Required} \\
target\_utilization       & Float   & 0.5               \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\
\multicolumn{3}{l}{\textbf{worker}}                     \\ \midrule
service\_name             & String  & \textit{Required} \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\ \bottomrule
\end{tabular}
\caption{Auto-Scaler configuration parameter}
\label{table:06_auto-scaler_config_parameter}
\end{table}

\todo{Update weil kein DOcker swarm mehr}
% Example
\begin{lstlisting}[label=lst:06_auto-scaler_config_example, caption=Auto-Scaler configuration YAML file]
general:
  interval_seconds: 5
  cooldown_period_seconds: 180
  recurrence_factor: 3
  prometheus_url: "http://localhost:9090"
 
metrics:
  cpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.5
    thresholds:
      min: 0.2
      max: 0.6
  
  gpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.3
    thresholds:
      min: 0.2
      max: 0.6
 
worker:
  service_name: "computing_spark-worker"
  thresholds:
    min: 1
    max: 30
\end{lstlisting}


\subsection{Scaling Apache Worker Nodes}
% Periodically performing
The Auto-Scaler performs periodically. Each period it fetches performance metrics, analyses the metrics, plans scaling actions, and executes them if necessary.
% Tasks
To perform tasks periodically, it uses the APScheduler library. This allows to perform a tasks each $n$ seconds.
% Explain tasks
In each task, the Analyse, Plan, and Execute phase is performed in order.


\subsubsection{Estimation of Necessary Scaling Actions}
To estimate of a scaling actions is necessary, the Auto-Scaler uses the Scaling Heat algorithm (introduced in SECTION XY).
The algorithm takes the utilization of a performance metric, the lower- and upper-threshold, and the calculated heat of the last iteration as input parameters.
The utilization is received from the Prometheus HTTP API. Furthermore, the Auto-Scaler fetches the utilization of all given performance metrics in each iteration. It calculates the heat value for each single performance metrics. If the upper-threshod value gets violated, the heat value is increased by 1. Otherwise, if the lower-threshold is violated, the heat is reduced by one.
Until the heat value reaches positive or negative of the recurrence-factor a scaling action will be executed.
This phase will not be performed if a cooldown period has been activated in the previous iteration.


\subsubsection{Calculating the Number of Needed Worker Nodes}
To calculate the number of needed Apache Spark worker, the Auto-Scaler uses the KHPA algorithm.
As input parameters, the algorithm takes the current number of active worker nodes, the utilization of the performance metric, and target utilization.

\Lst{lst:06_auto-scaler_plan_khpa} shows the implementation of the KHPA algorithm in Python.
To receive the number of active worker nodes, the Apache Spark master node has to be defined as a Prometheus target (described in SECTION). This enables to fetch the \texttt{metrics\_master\_aliveWorkers\_Value} metric from the Prometheus HTTP API. The current utilization of a performance metric has been received previously in the Analyse phase. Lastly, the target utilization for the performance metric is defined in the Auto-Scaler configuration. 
% The python impl
\begin{lstlisting}[label=lst:06_auto-scaler_plan_khpa, caption=KHPA implementation using Python 3.8, language=Python]
def calculate_number_of_needed_worker(active_worker: int,
    utilization: float,
    target_utilization: float):
  return math.ceil(
    active_worker * (utilization / target_utilization))
\end{lstlisting}


\subsubsection{Performing a Scaling Action}
After the number of needed Apache Spark worker nodes are calculated, the Auto-Scaler is responsible to add or remove the necessary worker Docker container to reach the desired performance goal.
To add or remove Docker container, Docker provides a Python library for the Docker Engine\footnote{Docker SDK for Python 4.4.1 Documentation - \url{https://docker-py.readthedocs.io/en/4.4.1/} (Accessed: 2021-01-05)}. This library is used to send instructions to the Docker engine running on the host machine.
% Check if worker is busy
If worker need to be removed, it is necessary to check if the worker are running any applications. Removing a worker while an application is performing will cause the cancellation of the application.
% How to check if applications are running
To check if applications are actively running on the Apache Spark cluster, the Auto-Scaler fetches the \texttt{metrics\_master\_apps\_Value} metric from the Prometheus HTTP API. To receive this metric, the Apache Spark master node has to be set as a target in Prometheus.
% Cooldown
After a scaling action has been executed successfully by the Docker engine, a cooldown period is activated. The duration of the cooldown period can be set in the configuration. During this cooldown no scaling actions are executed.


\subsection{Docker Image}
% Why dockerfile
A Docker image is needed to deploy the Auto-Scaler as a Docker service in the computing environment. Therefore a custom Dockerfile is created to build an Auto-Scaler Docker image.
% Explain Dockerfile
\Lst{lst:06_auto-scaler_dockerfile} shows the Auto-Scaler Dockerfile implementation. The python:3 Docker image is set as the parent image. 
Next, the Auto-Scaler source code is copied to image. Afterwards, a Python virtual environment is created with all dependencies installed.
As entrypoint, the Docker image starts the Auto-Scaler process.
\begin{lstlisting}[label=lst:06_auto-scaler_dockerfile, caption=Auto-Scaler Dockerfile]
FROM python:3
 
WORKDIR /usr/src/auto_scaler
 
# Copy the python module
COPY setup.py .
COPY src src/
 
# Update and install packages
RUN pip3 install -e .
 
ENTRYPOINT [ "python3", "src/run.py" ]
\end{lstlisting}


% Building the Docker image
To build the \textit{Auto-Scaler} Docker image, a build script was implemented. \Lst{lst:06_auto-scaler_build} shows the build script implementation.
The script takes the version of the \textit{Auto-Scaler} module as input attribute and uses the version to tag the Docker image.
It builds two versions of the \textit{Auto-Scaler}, one tagged with the current \textit{Auto-Scaler} version (e.g. \textit{auto-scaler:1.0}) and one tagged as \textit{latest} (e.g. \textit{auto-scaler:latest}).
\begin{lstlisting}[label=lst:06_auto-scaler_build, caption=Auto-Scaler build script, language=sh]
#!/bin/bash
 
if [ $# -ge 1 ]
  then
    VERSION=$1
 
    PWD=$(pwd)
 
    CI_REGISTRY=${CI_REGISTRY-local}
    IMAGE_TAG_PATH="${CI_REGISTRY}/cci/distributed-computing-framework"
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:$VERSION \
      $PWD
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:latest \
      $PWD
  else
    echo "No arguments supplied\n"
      echo "Use the script as follows:"
      echo "build-image.sh <VERSION>"
      exit 1
fi
\end{lstlisting}
% How to use the script
The script can be used from the command line as \texttt{sh build-image.sh "1.0"}.

% ===========================================
% ===========================================
\section{Computing Environment}
% SHort intro
The computing environment is deployed as a Docker swarm (described in SECTION AB). It consists of several components which are all deployed as Docker services. The conceptual design is explained in SECTION XY.
% Problem
As mentioned in SECTION XY, components which require GPUs are not created as Docker services. These components are deployed as Docker containers in the same swarm network.
% The whole thing
Overall, the computing environment consists of the following components:
\begin{itemize}
% AM
\item Autonomic Manager
\begin{itemize}
\item Auto-Scaler
\item Prometheus
\item cAdvisor
\item dcgm-exporter
\end{itemize}

% Spark cluster
\item Apache Spark Cluster
\end{itemize}


\subsection{Deployment of the Computing Environment}
%
To simplify the deployment of a stack of services, Docker proved to define all services in a docker-compose file.
% 
The computing environment docker-compose file is defined in \Lst{lst:appendix_env_compose}.
%
It defines all services, except the Apache Spark worker and the dcgm-export because these services require the NVIDIA Docker runtime.
%
To deploy all services defined in the docker-compose file, Docker provides the docker stack command.
%
After the Docker stack has been deployed, the dcgm-exporter container has to be deployed and added to the same network.
%
\Lst{lst:06_env_depl_docker-stack} provides the process how to deploy the stack and the dcgm-exporter container.
\begin{lstlisting}[label=lst:06_env_depl_docker-stack, caption=Auto-Scaler start command, language=sh]
$ docker stack deploy -c docker-compose.yml computing
...
$ docker run -d --rm -p "9400:9400" \
    --runtime=nvidia \
    --name dcgm-exporter \
    --network computing_net \
    nvidia/dcgm-exporter:2.0.13-2.1.1-ubuntu18.04
\end{lstlisting}


\subsection{Apache Spark Cluster with GPU Acceleration}
The Apache Spark cluster consists of a single master node, a dynamic number of worker nodes, and spark-submit nodes.
A spark-submit is deployed, whenever an application is submitted to cluster.
The master worker nodes are deployed in standalone mode.


\subsubsection{Apache Spark Base Image}
All services in the Apache Spark cluster have the same dependencies but perform different services. To simplifiy the creation of the Docker images, a base image is created. This base image serves as the parent image for all other images.
Additionally, this contributes to the homgenity of running services in cluster.


%
To install a specific Apache Spark version on the Docker image, two arguments are provides. The arguments can be set in the Docker build command with the \texttt{--build-arg} flag. LISTING BLA provides 
%
The base image is build from the \texttt{nvidia/cuda:11.0-devel-ubuntu16.04} Docker image. This image is provided by NVIDIA and has alerady install all neded dependecnies to leverage GPUs to Docker images.
%
The Dockerfile installs all required dependecies to run a standalone Apache Spark node with GPU acceleration. This includes the Apache Spark, JRE 1.8, Python3, GPU discovery script, RAPIDS Java binary, cuDF Java binary, XGBoost4j binary, and XGBoost4j-spark binary.


\subsubsection{Apache Spark Master Image}
% Intro Dockerfile
The Apache Spark master node Docker image is created from a custom Dockerfile (see \Lst{lst:appendix_spark_master_dockerfile}).
% Base image
This image is build on top the spark-base image and therefore has already all dependencies installed.
% Ports
It configures the Apache Spark master port to 7077 and the port for the web user interface to 4040.
% entrypoint
Additionally, \texttt{the start-master.sh} script is set as entrypoint. Therefore, when a service is build from this image, it will automatically start an Apache Spark master node in standalone mode.


\subsubsection{Apache Spark Worker Image}
% Intro
The Apache Spark worker image is created from a custom Dockerfile as well.
%
This Dockerfile uses the \textit{spark-base} image as parent.
%
Therefore, no additional dependencies have to be installed.
%
To configure the worker nodes resources, the spark-env.sh is copied to the Apache Spar conf/ folder. \Lst{lst:06_env_depl_worker-env} shows the configuration file. It sets the number of GPUs for each executor and the path to the GPU discovery script on the node host.
%
The entrypoint is set to \texttt{start-slave.sh} to start an Apache Spark worker in standalone mode. To connect to Apache Spark master node, the URI has to be set as an environment variable.
\begin{lstlisting}[label=lst:06_env_depl_worker-env, caption=Environment configuration for all worker nodes, language=sh]
SPARK_WORKER_OPTS="-Dspark.worker.resource.gpu.amount=1 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
\end{lstlisting}


\subsubsection{Apache Spark Submit Image}
% Intro
An Apache Spark submit container is deployed by the CI pipeline whenever an application is submitted to the cluster. This container is created from a \textit{spark-submit} Docker image.
% Parent
The spark-submit Dockerfile uses the spark-base image as parent.
% submit
It copies a custom submit script to the image which is used as the image entrypoint.
% spark-submit
The submit script is introduced in \Lst{lst:06_env_depl_submit}. It provides a simplified interface for the spark-submit executable.
% CI pipeline
The usage is described in detail in SECTION CI PIPELINE.
% The script
\begin{lstlisting}[label=lst:06_env_depl_submit, caption=Custom submit script, language=sh]
spark-submit bla bla
\end{lstlisting}


\subsubsection{Apache Spark Base Image Installation Details}
\paragraph{}The base image Dockerfile is available at \Lst{lst:appendix_spark_base_dockerfile}.
% Ubuntu base docker image
As parent, the base image uses the \texttt{nvidia/cuda:11.0-devel-ubuntu16.04} Docker image. The parent image runs Ubuntu\footnote{Enterprise Open Source and Linux - \url{https://www.ubuntu.com/} (Accessed: 2021-01-03)} in version 16.04. Additionally the CUDA Toolkit and the NVIDIA GPU driver are already installed.
% Args for spark and hadoop version
Docker provides the ability to set build arguments. To be able to install a specific Apache Spark version, two arguments,  can be set when building the Docker image.
% Apache Spark
Apache Spark will be installed at \texttt{/opts/Spark}.
% Work dir
This directory will be set as the working directory for the Docker image as well.
% Ubuntu packages
Furthermore, required Ubuntu packages will be installed. This includes the Java Runtime Environment Version 8, which is a requirement for Apache Spark.
% RAPIDS
To enable GPU acceleration on all Apache Spark nodes, the base image will install the compiled Java files for the RAPIDS plugin at \texttt{/opt/sparkRapidsPlugin} (for RAPIDS installation requirements, see \SubSec{subsec:04_rapids_req}). The .jar files can be downloaded in the maven repository.
% Discovery script
To enable Apache Spark to discover available GPUs, a GPU discovery script is needed (see \SubSec{subsubsec:04_spark_standalone_res-alloc} for details about resource allocation). This discovery script will be placed at \texttt{/opt/sparkRapidsPlugin} as well. The discovery script is introduced at \Lst{lst:appendix_spark_gpu-discovery}.

%% HETEROGEN


% Building
%\paragraph{}Listing XY demonstrates how the base image can be build 


% Env var table
%The table \ref{table:1} is an example of referenced \LaTeX elements.

%\begin{table}[h!]
%\centering
%\begin{tabular}{||c c||} 
 %\hline
 %Environment variable & Value \\ [0.5ex] 
 %\hline\hline
 %$SPARK\_RAPIDS\_DIR$ & /opt/sparkRapidsPlugin \\ 
 %$SPARK\_CUDF\_JAR$ & opt/sparkRapidsPlugin/cudf-0.15-cuda11.jar \\
 %$SPARK\_RAPIDS\_PLUGIN\_JAR$ & opt/sparkRapidsPlugin/rapids-4-spark_2.12-0.2.0.jar \\
 %$JAVA\_HOME$ & /usr/lib/jvm/java-8-openjdk-amd64 \\
 %$PYSPARK\_PYTHON$ & python3 \\ [1ex] 
 %\hline
%\end{tabular}
%\caption{Table to test captions and labels}
%\label{table:1}
%\end{table}


\subsubsection{Standalone Master and Worker Image}
% Base image
The master and worker image are build on top of the  Apache Spark base image.
% no additional installation
Therefore, no additional installation steps are required.
% standalone
The master and worker nodes will be launched in standalone mode (see \SubSec{subsec:04_spark_standalone} for standalone mode details).


\paragraph{Master image:}
Implementation of the master node Dockerfile is available at \Lst{lst:appendix_spark_master_dockerfile}.
% Ports
The master node image needs two ports configured: The Apache Spark service port and the port for the web user interface.
% web ui port
The Apache Spark service port ist set to 7077 and the web user interface port to 4040. 
% Start in standalone
To start the master node in standalone mode, the start-master.sh launchable will be set as image entrypoint which requires no additional arguments.


\paragraph{Worker image:}
\Lst{lst:appendix_spark_worker_dockerfile} describes the implementation of the worker node Dockerfile.
% web ui port
The port for the worker web interface will be exposed at 4041.
% Port
To start the worker in standalone mode, the start-slave.sh executable will be set as entrypoint for the image.
% Master uri
The launch script requires the master node URI as a parameter. To keep the configuration simple, the environment variable HIER DIE VAR will be set in the compose file.
% Worker configuration
\Lst{lst:06_computing_spark_worker-env} describes the configuration environment for the worker. As mentioned previously (in \Sec{sec:05_restrictions}), for this project two GPUs are available on the DGX workstation. Furthermore, the worker needs to know where to find the GPU resource discovery script.

% Building script
\begin{lstlisting}[label=lst:06_computing_spark_worker-env, caption=Environment configuration for all worker nodes, language=bash]
SPARK_WORKER_OPTS="-Dspark.worker.resource.gpu.amount=2 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
\end{lstlisting}


\subsubsection{Submit Image}
% Pyspark
As mentioned in \Sec{sec:05_restrictions}, a requirements is, that Apache Spark application will be implemented in Python. Accordingly, the pyspark Python module needs to installed on all submit nodes.
% Application workspace
Apache Spark application will be placed at \textit{/opt/spark-apps}. SECTION CI describes how an Apache Spark application will be copied to a submit node.
% Entrypoint
As entrypoint, the image will perform a custom submit script (available at LISTING AB). This script performs the spark-submit executable (usage described in detail in \SubSec{subsubsec:04_spark_standalone_submit}).


\subsection{Autonomic Manager}
As mentioned in \SubSec{subsec:05_arch_am}, the autonomic manager will consist of a monitoring system and the Auto-Scaler to create a complete control loop.

\paragraph{}The monitoring system conceptual design was introduced in SECTION XY.
It consists of a cAdvisor (SECTION XY) service and a Prometheus (SECTION XY) service.
All modules will run as individual Docker services in the overall swarm.
% Docker images
The following DOcker images will be used for the monitoring system:
% List all images
\begin{itemize}
\item \textbf{cAdvisor:} google/cadvisor
\item \textbf{Prometheus:} prom/prometheus
\end{itemize}

\subsubsection{Prometheus Target Configuration}

% ==============
% Hands-On Infrastructure Monitoring with Prometheus -> Comparing CPU usage -> PromQL approach
% ==============



% Pull based
As mentioned in \Sec{sec:04_prom}, Prometheus is a pull-based monitoring tool.
% Target list
It requires a list of targets to pull performance metrics from.


% Describe config
\Lst{lst:06_computing_am_prom-config} specifies the scrape configuration of the Prometheus system.
% cAdvisor target
The cAdvisor service is specified as a target. Prometheus will scrape every 5 seconds performance metrics from cAdvisor. All performance metrics will be labeled with the cAdvisor lable. The cAdvisor service is available at  cadvisor:8080.


% Building script
\begin{lstlisting}[label=lst:06_computing_am_prom-config, caption=Prometheus target configuration in YAML syntax]
scrape_configs:
    - job_name: prometheus
      scrape_interval: 5s
      static_configs:
          - targets: ["localhost:9090"]
 
    - job_name: cadvisor
      scrape_interval: 5s
      static_configs:
          - targets: ["cadvisor:8080"]
            labels:
                group: "cadvisor"
\end{lstlisting}


% ===========================================
% ===========================================
\section{Automatic Deployment of Apache Spark Applications}
Vielleicht eher das Kapitel so nennen
- gitlab-ci.yml erklären
- Screenshot von webui output
