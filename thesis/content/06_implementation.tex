\chapter{Implementation}
\label{chap:06_implementation}

This chapter explains the implementation process of the conceptual detail introduced in \Chap{chap:05_design}.


% ===========================================
% ===========================================
\section{Implementation Environment}
% The DGX
For the implementation of the conceptual design this thesis, a NVIDIA DGX\footnote{The Universal System for AI Infrastructure - \url{https://www.nvidia.com/en-us/data-center/dgx-a100/} (Accessed: 2021-01-30)} machine is available. 
% Live system
This machine is a shared live-system. Therefore, multiple application are performing simultaneously on this machine which share the same resources.


\subsection{Technical Details}
% Hardware Resources
The hardware specification of the machine is the following:
\begin{itemize}
\item 11GB RAM
\item 300 TB DIsk space 
\item 8x NVIDIA Tesla GPU a 32GB
\item 500x Intel CPU COre 29876
\end{itemize}


% Available software
The following software is installed on the machine:
\begin{itemize}
\item Ubuntu 30.5
\item Docker 19.5
\item NVIDIA Docker runtime 1
\end{itemize}


\subsection{NVIDIA Docker Runtime}
% nvidia runtime
To enable GPU support for Docker containers, the NVIDIA Container Toolkit has to be installed on the host machine. This toolkit provides a runtime library which automatically enables Docker containers to leverage NVIDIA GPUs.
% The problem
It is possible to define the runtime of a Docker container with the docker \texttt{run} command. However, this is not supported for Docker services. To deploy Docker services with the NVIDIA runtime enabled, the NVIDIA runtime has to be set as the default runtime for Docker.
% How
How the NVIDIA Container toolkit is installed and the runtime is set as default runtime is thoroughly explained in the NVIDIA Container Toolkit documentation\footnote{NVIDIA Cloud Native technologies documentation - \url{https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html} (Accessed: 2021-01-28)}.

\paragraph{Problem statement:}
% Already installed
The NVIDIA Container Toolkit is installed on the host machine, but the NVIDIA runtime is not set as default runtime.
% The problem
Changing the default runtime requires a restart of the Docker service. Restarting the Docker service is not possible because it requires to quit all running Docker containers on the machine. Therefore, components which require access to GPU resources cannot be deployed as Docker services.
% The solution
The solution to this problem is, to deploy these components as Docker container instead of Docker services.
% the env
Given this problem statement, the dcgm-exporter monitoring agent and each GPU accelerated Apache Spark worker have to be deployed as Docker container with the NVIDIA runtime enabled.
% Finish
\Lst{lst:06_env_prob_cmd} shows an example of the docker run command to deploy a Docker container using the NVIDIA runtime.
\begin{lstlisting}[label=lst:06_env_prob_cmd, caption=Docker run command to deploy a container using the NVIDIA runtime, language=sh, numbers=none]
$ docker run -d --rm --runtime=nvidia --name dcgm-exporter nvidia/dcgm-exporter:latest
\end{lstlisting}

\paragraph{Auto-Scaler}
The Auto-Scaler will still be implemented using the Docker service approach. The reson is that for the evaluation, only CPU-only worker nodes will be scaled using the Auto-Scaler. Because as mentioned above, only 2 GPUs are available for the implementation and the evaluation.
However, the Auto-Scaler is still implemented that after enabling NVIDIA as the default runtime it should work.


% ===========================================
% ===========================================
\section{Computing Environment}
% SHort intro
The computing environment is deployed as a Docker swarm (described in \Sec{subsec:04_docker_swarm}). It consists of several components which are all deployed as Docker services. The conceptual design is explained in \Sec{sec:05_env}.
% Problem
As mentioned previously, components which require GPUs are not created as Docker services. These components are deployed as Docker containers in the same swarm network.
% The whole thing
Overall, the computing environment consists of the following components:
\begin{itemize}
% AM
\item Autonomic Manager
\begin{itemize}
\item Auto-Scaler
\item Prometheus
\item cAdvisor
\item dcgm-exporter
\end{itemize}

% Spark cluster
\item Apache Spark Cluster
\end{itemize}


\subsection{Deployment of the Computing Environment}
%
To simplify the deployment of a stack of services, Docker provides to define all services in a \textit{docker-compose} file.
% 
The computing environment \textit{docker-compose} file is defined in \Lst{lst:appendix_env_compose}.
%
It defines all services, except the dcgm-exporter because it requires the NVIDIA Docker runtime.
%
The whole stack can be deployed using the \texttt{docker stack} command. Then, the dcgm-exporter container has to be deployed and added to the same network.
%
\Lst{lst:06_env_depl_docker-stack} shows the process how to deploy the stack and the dcgm-exporter container. First, all services all deployed using the docker stack command. Afterwards, the dcgm-exporter is deployed in a Docker container using the NVIDIA runtime and attached to the same network.
\begin{lstlisting}[label=lst:06_env_depl_docker-stack, caption=Auto-Scaler start command, language=sh, numbers=none]
$ docker stack deploy -c docker-compose.yml computing

$ docker run -d --rm -p "9400:9400" \
    --runtime=nvidia \
    --name dcgm-exporter \
    --network computing_net \
    nvidia/dcgm-exporter:2.0.13-2.1.1-ubuntu18.04
\end{lstlisting}


\subsection{Autonomic Manager}
The autonomic manager is responsible to monitor the computing environment and scale the number of Apache Spark worker replicas.
It is composed of a monitoring system and the \textit{Auto-Scaler}.
Furthermore, the monitoring system consists of Prometheus, cadvisor, and dcgm-exporter components.
Together, all components build an autonomic manager according to the MAPE architecture.
The monitoring system monitors the components in the computing environment while the \textit{Auto-Scaler} analyses the performance metrics and adapts the number of Apache Spark workers.


\subsubsection{Prometheus Target Configuration}

% ==============
% Hands-On Infrastructure Monitoring with Prometheus -> Comparing CPU usage -> PromQL approach
% ==============



% Pull based
As being introduced in \Sec{sec:04_prom}, Prometheus is a pull-based monitoring tool.
% Target list
It requires a list of targets to pull performance metrics from. How Prometheus is configured is described in \Sec{sec:04_prom_config}.


% Describe config
\Lst{lst:06_computing_am_prom-config} specifies the scrape configuration of the Prometheus system.
% scrape interval
The \texttt{scrape\_interval} parameter is set to \textit{5s} which means, that every 5 seconds, Prometheus scraped performance metrics from all defined targets.
% Label
Each target is configured using a URL and a label. The label is used to identify the metric source.
% All targets
In this configuration, the following targets are defined:
\begin{itemize}
\item cadvisor: All container related performance metrics are received from the cAdvisor target.

\item dcgm-exporter: The dcgm-export provides GPU related performance metrics.

\item spark-master: The spark-master target provides metrics about the Apache Spark master service. This target is important for the \textit{Auto-Scaler} to get information about the number of running worker nodes and the number of running applications.
\end{itemize}
% Target config
\begin{lstlisting}[label=lst:06_computing_am_prom-config, caption=Prometheus target configuration in YAML syntax]
global:
  scrape_interval: 5s
 
rule_files:
  - "/etc/prometheus/recording_rules.yml"
 
scrape_configs:
    - job_name: cadvisor
      static_configs:
          - targets: ["cadvisor:8080"]
            labels:
                group: "cadvisor"
 
    - job_name: dcgm-exporter
      static_configs:
          - targets: ["dcgm-exporter:9400"]
            labels:
                group: "dcgm-exporter"
    
    - job_name: spark-master
      metrics_path: /metrics/master/prometheus/
      static_configs:
          - targets: ["spark-master:4040"]
            labels:
              group: "spark"
\end{lstlisting}


\subsubsection{Prometheus Recording Rules Configuration}
%
As introduced in \Sec{sec:04_prom_arch}, Prometheus provides the ability to define recording rules. Recording rules can be queried at a much faster speed than PromQL queries because the results of recording rules are saved in the local storage of Prometheus.
%
Therefore, all suitable metrics to monitor the performance of the computing environment introduced in \Sec{sec:05_metrics} are defined as recording rules.
%
\Lst{lst:06_env_depl_am_prom-rules} shows the configuration of both performance metrics as recording rules.
%
\begin{lstlisting}[label=lst:06_env_depl_am_prom-rules, caption=Prometheus target configuration in YAML syntax]
groups:
    - name: performance_usage_percent
      rules:
        - record: instance:worker_nodes:cpu_usage:percent
          expr: SUM(RATE(container_cpu_usage_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s])) BY (image)
        - record: instance:gpu:usage:percent
          expr: SUM(DCGM_FI_DEV_GPU_UTIL{device=~"(?:nvidia4|nvidia5)"}) / 2
\end{lstlisting}
% Explain queries
The CPU query use the \texttt{RATE} function to calculate the per-second rate of the CPU usage over the last 30 seconds of each available worker node. Then the results are summed up using the \texttt{SUM} function.
% GPU
To get the utilization for two different GPU devices, PromQL provides selectors for labels. In the configuration the \texttt{=\~} selector is used for the device label. This selector selects all values that match the given regular-expression. The given regular-expression selects the GPUs with the name \textit{nvidia4}, and \textit{nvidia5}. Then both utilizations values are summed up and divided by the number of GPUs.


% ===========================================
% ===========================================
\section{Auto-Scaler}
% SHort intro 
The \textit{Auto-Scaler} is a main module of the autonomic manager. It is responsible to analyse performance metrics, plan scaling actions in accordance to the performance metrics and execute scaling actions to adapt the number of Apache Spark worker in the computing environment.
% Mention conceptual design
It is implemented as a custom module in Python 3.8 and deployed as a Docker image using a custom Dockerfile to deploy the \textit{Auto-Scaler} running in a Docker container.


% Concept
The \textit{Auto-Scaler} integrates the Analyse, Plan, and Execute phase of the MAPE architecture. Furthermore it scales the Apache Worker horizontally by adding and removing Apache Spark worker Docker containers.
% A Loop
The Auto-Scaler runs as a background service that periodically performs each phase in order.
% Configuration
A file which defines runtime configuration can be set.
% Start the sclaer
The Auto-Scaler can be started with the following command:
\begin{lstlisting}[label=lst:06_auto-scaler_start, caption=Auto-Scaler start command, language=sh, numbers=none]
$ python3 run.py --config=config.yml
\end{lstlisting}


\subsection{Technical Background}
% Technical background
The \textit{Auto-Scaler} is implemented in Python 3.8. It consists of different classes, each having different responsibilities.
% A list of the Python libs
The following Python libraries have been used for the implementation:
% The list
\begin{itemize}
\item aiohttp\footnote{\url{https://pypi.org/project/aiohttp/} (Accessed: 2021-01-26)}
\item APScheduler\footnote{\url{https://pypi.org/project/APScheduler/} (Accessed: 2021-01-26)}
\item docker\footnote{\url{https://pypi.org/project/docker/} (Accessed: 2021-01-26)}
\item PyYAML\footnote{\url{https://pypi.org/project/PyYAML/} (Accessed: 2021-01-26)}
\end{itemize}


\subsection{Configuration}
% Design
\paragraph{}The configuration parameter for the \textit{Auto-Scaler} have been introduced in \Sec{subsubsec:05_am_auto-scaler_config}.
% Its yaml
All parameter are defined in the YAML file format
% The yaml file
\Lst{lst:06_auto-scaler_config_example} provides an example of a configuration.
% Overall structure
Overall, a configuration file is structured in three sections: General, metrics, and worker.
% The sections
\begin{itemize}
% General
\item General:
The general section defines details about the scaling and heat algorithm and the Prometheus URL.

% Metrics
\item Metrics:
Metrics is a list of performance metrics configuration parameters. A performance metric requires a query in the PromQL syntax. Additionally a target utilization is needed and the minimum and maximum utilization of the performance metric.

% Worker
\item Worker:
To scale the replicas of the Apache Spark worker service, the name of the Docker service needs to be set. In addition, the minimum and maximum number of concurrent worker nodes needs to be defined to prevent an overhead of running worker nodes.
\end{itemize}

% The table
\Tab{table:06_auto-scaler_config_parameter} lists all available configuration parameters. It describes the value type and the default value of each parameter. Some parameters are required to be defined by the administrator and have no default value.
% Parameter table
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Name                      & Type    & Default           \\ \midrule
\multicolumn{3}{l}{\textbf{general}}                    \\ \midrule
interval\_seconds         & Integer & 1                 \\
cooldown\_period\_seconds & Integer & 180               \\
recurrence\_factor        & Integer & 1                 \\
prometheus\_url           & String  & \textit{Required} \\
\multicolumn{3}{l}{\textbf{metrics}}                    \\ \midrule
query                     & String  & \textit{Required} \\
target\_utilization       & Float   & 0.5               \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\
\multicolumn{3}{l}{\textbf{worker}}                     \\ \midrule
service\_name             & String  & \textit{Required} \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\ \bottomrule
\end{tabular}
\caption{Auto-Scaler configuration parameter}
\label{table:06_auto-scaler_config_parameter}
\end{table}

\todo{Update weil kein DOcker swarm mehr}
% Example
\begin{lstlisting}[label=lst:06_auto-scaler_config_example, caption=Auto-Scaler configuration YAML file]
general:
  interval_seconds: 5
  cooldown_period_seconds: 180
  recurrence_factor: 3
  prometheus_url: "http://localhost:9090"
 
metrics:
  cpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.5
    thresholds:
      min: 0.2
      max: 0.6
  
  gpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.3
    thresholds:
      min: 0.2
      max: 0.6
 
worker:
  service_name: "computing_spark-worker"
  thresholds:
    min: 1
    max: 30
\end{lstlisting}


\subsection{Scaling Apache Worker Nodes}
% Periodically performing
The \textit{Auto-Scaler} performs periodically. Each period fetches performance metrics, analyses the metrics, plans scaling actions, and executes them if necessary.
% AP jobs
To perform periodically, it uses the \textit{APScheduler}\footnote{\url{https://pypi.org/project/APScheduler/} (Accessed: 2021-01-26)} Python library. The \textit{APScheduler} provides to schedule jobs periodically.
% Explain jobs
In each job, the Analyse, Plan, and Execute phase is performed in order.


\subsubsection{Estimation of Necessary Scaling Actions}
To estimate of a scaling actions is necessary, the Auto-Scaler uses the Scaling Heat algorithm (introduced in \Sec{alg:04_scal-heat_concept_algo}).
% input
The algorithm takes the utilization of a performance metric, the lower- and upper-threshold, and the calculated heat of the last iteration as input parameters.
% utilization
The utilization is received from the Prometheus HTTP API. Furthermore, the \textit{Auto-Scaler} fetches the utilization of all given performance metrics in each iteration. It calculates the heat value for each single performance metric. If the upper-threshod value gets violated, the heat value is increased by 1. Otherwise, if the lower-threshold is violated, the heat is reduced by one.
% the limit
Until the heat value reaches positive or negative of the recurrence-factor a scaling action will be executed.
% cooldown
This phase will not be performed if a cooldown period has been activated in the previous iteration.


\subsubsection{Calculating the Number of Needed Worker Nodes}
To calculate the number of needed Apache Spark worker, the \textit{Auto-Scaler} uses the KHPA algorithm (introduced in \Sec{sec:04_background_khpa}).
As input parameters, the algorithm takes the current number of active worker nodes, the utilization of the performance metric, and target utilization.


% Explain implementation
\Lst{lst:06_auto-scaler_plan_khpa} shows the implementation of the KHPA algorithm in Python.
To receive the number of active worker nodes, the Apache Spark master node has to be defined as a Prometheus target (described in SECTION). This enables to fetch the \texttt{metrics\_master\_aliveWorkers\_Value} metric from the Prometheus HTTP API. This metrics returns the number of alive Apache Spark worker nodes in the cluster.

The utilization of the performance metric, which is used for the KHPA algorithm, has already been received in the previous phase. Lastly, the target utilization for the performance metric is defined in the \textit{Auto-Scaler} configuration. 
% The python impl
\begin{lstlisting}[label=lst:06_auto-scaler_plan_khpa, caption=KHPA implementation using Python 3.8, language=Python]
def calculate_number_of_needed_worker(active_worker: int,
    utilization: float,
    target_utilization: float):
  return math.ceil(
    active_worker * (utilization / target_utilization))
\end{lstlisting}


\subsubsection{Performing a Scaling Action}
After the number of needed Apache Spark worker nodes are calculated, the \textit{Auto-Scaler} is responsible to scale the replicas of the worker service to reach the desired performance goal.
% Docker lib
To scale a Docker service using Python, Docker provides a Python library for the Docker Engine\footnote{Docker SDK for Python 4.4.1 Documentation - \url{https://docker-py.readthedocs.io/en/4.4.1/} (Accessed: 2021-01-05)}. 
% Cooldown
After a scaling action has been executed successfully by the Docker engine, a cooldown period is activated. The duration of the cooldown period can be set in the configuration. During this cooldown no scaling actions are executed.


\subsubsection{Removing Apache Spark worker}
% RDD intro
As being introduced in SECTION RDD, Apache Spark is build on top of a data structure called RDD. A RDD can be compose of bla bla bla.
% Shuffle data
Executors are launched on workers to process data. Therefore, a worker node stores temporary data of an executor containing cache data and shuffle-data.
% No downsscaling
If a worker node store important shuffle data, even if the overall performance is underutilized, the worker cannot be removed.
% How
% Check if worker is busy
Therefore, worker will not be removed if applications are actively performed on the Apache Spark cluster.
% The metric
Prometheus provides a metrics called \texttt{metrics\_master\_apps\_Value} which returns the number of running applications on the cluster. To fetch this metrics, the Apache Spark master node has to be set as Prometheus target.
% auto-scaler
The Auto-Scaler only reduces the number of worker replicas, if this metric returns 0.


\subsection{Docker Image}
% Why dockerfile
A Docker image is needed to deploy the \textit{Auto-Scaler} as a Docker service in the computing environment. Therefore a custom Dockerfile is created to build an \textit{Auto-Scaler} Docker image.
% Explain Dockerfile
\Lst{lst:06_auto-scaler_dockerfile} shows the Auto-Scaler Dockerfile implementation. The python:3 Docker image is set as the parent image. 
Next, the \textit{Auto-Scaler} source code is copied to image. Afterwards, a Python virtual environment is created with all dependencies installed.
As entrypoint, the Docker image starts the \textit{Auto-Scaler} process.
\begin{lstlisting}[label=lst:06_auto-scaler_dockerfile, caption=Auto-Scaler Dockerfile]
FROM python:3
 
WORKDIR /usr/src/auto_scaler
 
# Copy the python module
COPY setup.py .
COPY src src/
 
# Update and install packages
RUN pip3 install -e .
 
ENTRYPOINT [ "python3", "src/run.py" ]
\end{lstlisting}


% Building the Docker image
To build the \textit{Auto-Scaler} Docker image, a build script was implemented. \Lst{lst:06_auto-scaler_build} shows the build script implementation.
The script takes the version of the \textit{Auto-Scaler} module as input attribute and uses the version to tag the Docker image.
It builds two versions of the \textit{Auto-Scaler}, one tagged with the current \textit{Auto-Scaler} version (e.g. \textit{auto-scaler:1.0}) and one tagged as \textit{latest} (e.g. \textit{auto-scaler:latest}).
\begin{lstlisting}[label=lst:06_auto-scaler_build, caption=Auto-Scaler build script, language=sh]
#!/bin/bash
 
if [ $# -ge 1 ]
  then
    VERSION=$1
 
    PWD=$(pwd)
 
    CI_REGISTRY=${CI_REGISTRY-local}
    IMAGE_TAG_PATH="${CI_REGISTRY}/cci/distributed-computing-framework"
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:$VERSION \
      $PWD
 
    docker build \
      -t $IMAGE_TAG_PATH/auto-scaler:latest \
      $PWD
  else
    echo "No arguments supplied\n"
      echo "Use the script as follows:"
      echo "build-image.sh <VERSION>"
      exit 1
fi
\end{lstlisting}
% How to use the script
The script can be used from the command line as \texttt{sh build-image.sh "1.0"}.


% ===========================================
% ===========================================
\section{Apache Spark Cluster with GPU Acceleration}
The conceptual design of the Apache Spark cluster is introduced in \Sec{sec:05_spark}. It consists of a single master node, a dynamic number of worker nodes, and \textit{spark-submit} nodes. A \textit{spark-submit} node is deployed, whenever an application is submitted to cluster.
% standalone mode
All master and worker nodes are deployed in standalone mode (described in \Sec{subsec:04_spark_standalone}).


\subsection{Apache Spark Base Image}
All services in the Apache Spark cluster have share the same dependencies. To simplify the creation of the Docker images, a base image is created. This base image serves as the parent image for all other images. Furthermore, this contributes to the homogeneity of running services in cluster.
%
\Lst{lst:appendix_spark_base_dockerfile} provides the implementation of the \textit{base-image} Dockerfile.
%
To install a specific Apache Spark version on the Docker image, two arguments are provided. The arguments can be set in the Docker build command with the \texttt{--build-arg} flag. This is hown in CHAPTER BUILDING TH IMAGES WEITER UNTEN.
%
The base image uses the \texttt{nvidia/cuda:11.0-devel-ubuntu16.04} Docker image as parent image. This image is provided by NVIDIA and has already installed the NVIDIA GPU driver and the CUDA toolkit dependencies.
%
Additionally, the \textit{spark-base} Dockerfile installs all required dependencies to run a standalone Apache Spark node with GPU acceleration introduced in \Sec{subsec:04_rapids_req}. This includes Apache Spark, JRE 1.8, Python3, GPU discovery script, RAPIDS Java binary, cuDF Java binary, XGBoost4j binary, and XGBoost4j-spark binary.


\subsection{Apache Spark Master Image}
% Intro Dockerfile
The Apache Spark master node Docker image is created from a custom Dockerfile (see \Lst{lst:appendix_spark_master_dockerfile}).
% Base image
This image is build on top the \textit{spark-base} image and therefore has already all dependencies installed.
% Ports
It configures the Apache Spark master port to 7077 and the port for the web user interface to 4040.
% entrypoint
Additionally, the \texttt{start-master.sh} script is set as entrypoint. Therefore, when a service is build from this image, it will automatically start an Apache Spark master node in standalone mode.


\subsection{Apache Spark Worker Image}
% Intro
The Apache Spark worker image is created from a custom Dockerfile as well.
%
This Dockerfile uses the \textit{spark-base} image as parent.
% Homo
As being mentioned in \Sec{subsec:02_foundations_scalability_horizontal-scaling}, the horizontal scaling approach is more effective with homogeneous nodes. Each Apache Spark worker node is being created from the same Docker image given the same resources.
%
Therefore, no additional dependencies have to be installed.
%
To configure the worker nodes resources, the \texttt{spark-env.sh} is copied to the Apache Spark \texttt{conf/} folder. \Lst{lst:06_env_depl_worker-env} shows the configuration file. It sets the number of GPUs for each executor and the path to the GPU discovery script on the node host.
%
The entrypoint is set to \texttt{start-slave.sh} to start an Apache Spark worker in standalone mode. To connect to Apache Spark master node, the URI has to be set as an environment variable.
\begin{lstlisting}[label=lst:06_env_depl_worker-env, caption=Environment configuration for all worker nodes, language=sh, numbers=none]
SPARK_WORKER_OPTS="-Dspark.worker.resource.gpu.amount=1 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
\end{lstlisting}


\subsection{Apache Spark Submit Image}
% Intro
The \textit{spark-submit} Docker image is reponsible to execute the spark-submit executable (introduced in \Sec{subsubsec:04_spark_standalone_submit}) to run an application on the Apache Spark cluster.
% Parent
The spark-submit Dockerfile uses the \textit{spark-base} image as parent.
% submit
It copies a custom submit script to the image filesystem which is used as the image entrypoint. When a \textit{spark-submit} container is started, it executes the custom submit script with all provided arguments.


\subsubsection{Custom Submit Script}
% spark-submit
The custom submit script is provided at \Lst{lst:appendix_spark-submit_script}. It provides a simplified interface for the spark-submit executable. Furthermore, it includes all necessary configuration parameters to perform an application with GPU acceleration enabled. Additionally, if the \texttt{CPU\_ONLY} environment variable is set to \textit{true}, the script will disable GPU acceleration for the given application.
% task-parallelism
This feature is useful when algorithms optimized for task-parallelism are being executed.
% Example
\Lst{lst:06_env_depl_submit} demonstrates an usage example of the custom submit script.
% Exaplin
As arguments, the script takes the path of the application executable file and all needed application arguments as input. All arguments are forwarded to the spark-submit executable which executes the applications using the configuration provided by all environment variables. In the example, the \texttt{train.py} application is executed with a \texttt{max-depth} argument. 
% The script
\begin{lstlisting}[label=lst:06_env_depl_submit, caption=Usage of the submit script, language=bash, numbers=none]
$ sh submit.sh train.py --max-depth=50
\end{lstlisting}


\subsubsection{Environment Variables}
% The env arguments
The configuration of the underlying spark-submit executable can be configured by providing the environment variables listed in \Tab{table:06_env_depl_submit-envs}. The table shows the environment variable name and the default value if given.
% master
The only environment variable which is required is \texttt{SPARK\_MASTER\_URI}. This variable needs the URI of the Apache Spark master node.
% CPU_ONLY
If \texttt{CPU\_ONLY} variable is set to \textit{true}, the custom submit script executes the spark-submit executable without enabling GPU acceleration using the RAPIDS plugin.
% spark configs
\texttt{DRIVER\_MEMORY} sets the available memory for the driver process and \texttt{EXECUTOR\_MEMORY} sets the memory for each executor process.
% RAPIDS vars
The RAPIDS environment variables are described in detail at the online documentation\footnote{spark-rapids Configuration - \url{https://nvidia.github.io/spark-rapids/docs/configs.html} (Accessed: 2021-02-09)}.
% ENV TABLE
\begin{table}[]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Name                                                   & Default Value     \\ \midrule
\texttt{SPARK\_MASTER\_URI}           & \textit{required} \\
\texttt{CPU\_ONLY}           & false \\
\texttt{DRIVER\_MEMORY}               & 4g                \\
\texttt{EXECUTOR\_MEMORY}             & 8g                \\
\texttt{RAPIDS\_GPU\_ALLOC\_FRACTION} & 1                 \\
\texttt{RAPIDS\_INCOMPATIBLE\_OPS}    & false             \\
\texttt{RAPIDS\_DEBUG}                & NONE              \\
\texttt{RAPIDS\_GPU\_POOL}            & ARENA             \\ \bottomrule
\end{tabular}
\caption{\textit{spark-submit} image environment variables}
\label{table:06_env_depl_submit-envs}
\end{table}


\subsubsection{Executing an Application with the spark-submit Image}
% 
\Lst{lst:06_env_depl_submit_example} provides an example on how to perform an Apache Spark application using the \textit{spark-submit} Docker image.
% envs
The \texttt{SPARK\_MASTER\_URI} and \texttt{EXECUTOR\_MEMORY} environment variables are set using the \texttt{-e} flag.
%
It is important to set the \texttt{--rm} flag which automatically removes the Docker container after the submit script has executed.
% listing
\begin{lstlisting}[label=lst:06_env_depl_submit_example, caption=Example of the spark-submit image, language=bash, numbers=none]
$ docker run \
  --rm \
  --network computing_net \
  -e SPARK_MASTER_URI=spark://spark-master:7077 \
  -e EXECUTOR_MEMORY=16g \
  spark-submit:latest \
  train.py \
  --max-depth=50
\end{lstlisting}


\subsection{Build the Images}
% Intro
A script is used to automate the build process of all images.
% source
The source code of the build script is provided at ANHANG AB.
% params
To specify which Apache Spark version is installed on all images, the script takes the Apache Spark, and Hadoop version as input arguments which are required by the \textit{spark-base} image.
% Taging
Additionally, the build script tags each image with the provided Apache Spark and Hadoop version.
% Example
An example of the usage is provided in \Lst{06_env_depl_build_example}.
% Explain
To build the images, the example uses Apache Spark version 3.0.1 and Hadoop 2.7.
% example
\begin{lstlisting}[label=lst:06_env_depl_build_example, caption=Example of the spark-submit image, language=bash, numbers=none]
$ sh build-images.sh 3.0.1 2.7
\end{lstlisting}


% ===========================================
% ===========================================
\section{GitLab CI/CD Pipeline for Automated Deployment of Machine Learning Applications}
%
The CI pipeline is responsible to test, and submit the application to the Apache Spark cluster.
%
Important to notice, that Python is an interpreted programming language. It is possible to package the Python app as ZIP document or as an egg file. However, to keep the implementation simple, only raw Python files are submitted to the Apache Spark cluster. Therefore, there is no build stage in this CI pipeline architecture.
%
To be able to deploy a \textit{spark-submit} container on the host machine, a GitLab runner (introduced in \Sec{sec:04_background_gitlab_job}) instance has to run on the same machine.


\subsection{GitLab Runner}
GitLab CI/CD jobs are being executed by a GitLab runner.
To deploy a \textit{spark-submit} container in the swarm network of the Apache Spark cluster, the GitLab Runner needs to run on the same host to get access to the Docker engine.


%
As mentioned before, to deploy a \textit{spark-submit} node on the host machine, a GitLab Runner instance is needed which has access to the overlaying Docker engine.


% In this impl
In this implementation, the GitLab Runner is deployed as a Docker container.
% Docker as parent
The Docker container is build from the \texttt{gitlab/gitlab-runner:latest} image. This image provides an installation of the GitLab runner service and command-line-interface.
% Volumes
To enable Docker support in the context of the GitLab Runner Docker container, the Docker socket has to be mounted to the container. This \textit{socket-binding} approach is explained in detail on the GitLab online documentation\footnote{Building Docker images with GitLab CI/CD  - \url{https://docs.gitlab.com/ee/ci/docker/using_docker_build.html\#use-docker-socket-binding} (Accessed: 2021-01-31)}.


% Create a runner
The \texttt{gitlab-runner} command is set as image entrypoint. It initialises a new GitLab runner on the container.
% ENable docker on gitlab runner
To enable Docker access to the GitLab runner, various settings have to be set. First the docker executor has to be activated. This enables the runner to perform jobs with user defined Docker images, which is required to use the \textit{spark-submit} image. Second, the runner needs access to the Docker socket as well. Therefore, the mounted Docker socket has to be set as volume for the runner. Lastly, the \texttt{docker:latest} image is used for the container where jobs are performed on. This gives jobs the ability to perform Docker commands, e.g. the \texttt{docker run} command


% /builds volume
The \texttt{/builds} directory of a runner is the location where the source code of a repository is located. By default, this directory is only available to the job container deployed by the runner. To share this container with container deployed by a job, it has to be shared with the host system. Docker container cannot share their filesystem with other containers, because Docker run in isolation. This is described in detail in SECTION BLA NEXT.
% the listing
\Lst{lst:06_ci_runner_container_cmd} provides the command to create a GitLab runner in a Docker container.
% token and url
In addition to the previously mentioned configuration settings, a GitLab Runner requires a URL to a GitLab server and a registration token.
%
The registration token identifies a specific repository or a GitLab group. Then, the runner can only used from the 
%
The GitLab Runner created in the example is given the \textit{spark-submit} tag. GitLab CI/CD jobs which are tagged using this tag are performed by this runner.
% the example
\begin{lstlisting}[label=lst:06_ci_runner_container_cmd, caption=CLI command to start a GitLab runner in a Docker container, language=bash]
$ docker run -d \
    --name gitlab-runner \
    --restart always \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /builds:/builds \
    gitlab/gitlab-runner:latest \
    gitlab-runner register \
    -n \
    --name "Spark-Cluster Runner" \
    --executor docker \
    --docker-image docker:latest \
    --docker-volumes /var/run/docker.sock:/var/run/docker.sock \
    --docker-volumes /builds:/builds \
    --docker-privileged=true
    --url https://gitlab.com/ \
    --registration-token mpCBWzZzhaaJrdqjXYZq \
    --tag-list spark-submit
\end{lstlisting}


\subsection{Pipeline Architecture}
%
As explained in SECTION AB, a GitLab CI/CD pipeline is configured by a \texttt{.gitlab-ci.yml} file in the root directory of the source code repository.
%
LST XY shows the .gitlab-ci.yml configuration file. The pipeline consists of a \textit{test} and a \textit{train} stage.


\subsubsection{Test Stage}
% test stage
The first stage is the test stage which consists of a single job called \textit{unit-tests} to perform unit tests.
% The image
It uses the \texttt{python:3.8-slim} Docker image, which has already Python 3.8 installed. To perform the unit tests, first a Python virtual environment with all dependencies is created. Lastly, all tests in the \texttt{tests/} directory are performed using the pytest\footnote{pytest - \url{https://docs.pytest.org/en/latest/} (Accessed: 2021-02-01)} library.


\paragraph{}
It is important to mention, that the train stage depends on the application. This implementation focuses on simplicity and provides a general concept of a train stage which can be used and extended for other projects.


\subsubsection{Train Stage}
% train stage
After the test stage has been performed successfully, the train stage is responsible to perform the Apache Spark application on the Apache Spark cluster.
% Jobs
The train stage consists of a single job called \textit{train-model}.
% submit
This jobs executes the \texttt{submit.sh} build script, which is located in the repository root directory. The script executes the \texttt{docker run} command to deploy a \textit{spark-submit} Docker container in the Apache Spark cluster. It takes the path to the application Python file and all needed parameters as input and forwards them to the \texttt{docker run} command. To be able to execute the applications Python file, it has to be mounted to the host filesystem, which is explained in this section.
% Config
To configure the \textit{spark-submit} container, all configuration parameters listed in \Tab{table:06_env_depl_submit-envs} can be set as environment variables in the \texttt{variables} section of the job.
% Tag
The job has to be tagged with \textit{spark-submit} to get executed by the runner on the host machine.


\paragraph{Creating a mount-point:}
% Isolation
As mentioned before, Docker container run in isolation and do not share the same filesystem with each other.
% The runner
Each runner registered on the GitLab Runner Docker container runs its jobs on a separate Docker container on the host system. All commands are being performed in the context of its container.
%
When the \textit{train-job} deploys a \textit{spark-submit} Docker container with the \texttt{docker run} command, it tries to mount the \texttt{/builds} directory from its own filesystem to the \textit{spark-submit} container.
%
To share the source code between the \textit{train-job} container and the \textit{spark-submit} container, the \texttt{/builds} directory is mounted to the host system, as being explained in SECTION OBEN.
%
To overcome this problem, the \textit{train-model} job creates a new folder inside the \texttt{/builds} directory as mount-point and moves the application source code files to this location.
%
This mount-point is available to the \textit{spark-submit} container when given as a volume.
%
The approach of a mount-point is mentioned in a discussion on the GitLab website\footnote{Docker volumes not mounted when using docker:dind - \url{https://gitlab.com/gitlab-org/gitlab-foss/-/issues/41227} (Accessed: 2021-02-01)}.
