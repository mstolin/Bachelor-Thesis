\chapter{Implementation}
\label{sec:06_implementation}
\todo{Describe Chapter}


% ===========================================
% ===========================================
\section{Computing Environment}


\subsection{Swarm}
Vielleicht euch einfach das ganze kapitel Swarm nennen?
- Dockerfile erl채utern


\subsection{Build Script}


% ===========================================
% ===========================================
\section{Apache Spark Cluster with GPU Acceleration}
The Apache Spark cluster is created in standalone mode, see SECTION AB for details. The cluster consists of a single Apache Spark master node, a variant number of Apache Spark worker nodes and for each running application one Apache Spark Submit node. Each node runs in an independent Docker container. The Apache Spark master and worker nodes run as services in the swarm. Each Apache Spark node container is created from a custom Docker image. To simplify the images, a base image is created where all other images can inherit from.

\subsection{Apache Spark Base Image}
% The dockerfile
\begin{lstlisting}[frame=single, label=lst:06_env_apache_base_dockerfile, caption=Dockerfile of the Apache Spark base image, captionpos=b]
FROM nvidia/cuda:11.0-devel-ubuntu16.04
 
LABEL maintainer="marcel.pascal.stolin@ipa.fraunhofer.de"
 
ARG SPARK_VERSION
ARG HADOOP_VERSION
 
# Install all important packages
RUN apt-get update -qy && \
    apt-get install -y openjdk-8-jre-headless procps python3 python3-pip curl
 
# Install Apache Spark
RUN mkdir /usr/bin/spark/ && \
    curl https://ftp-stud.hs-esslingen.de/pub/Mirrors/ftp.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -o spark.tgz && \
    tar -xf spark.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}/* /usr/bin/spark/ && \
    rm -rf spark.tgz && \
    rm -rf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}/
 
# Add GPU discovery script
RUN mkdir /opt/sparkRapidsPlugin/
COPY getGpusResources.sh /opt/sparkRapidsPlugin/getGpusResources.sh
ENV SPARK_RAPIDS_DIR=/opt/sparkRapidsPlugin
 
# Install cuDF and RAPIDS
RUN curl -o ${SPARK_RAPIDS_DIR}/cudf-0.15-cuda11.jar https://repo1.maven.org/maven2/ai/rapids/cudf/0.15/cudf-0.15-cuda11.jar
RUN curl -o ${SPARK_RAPIDS_DIR}/rapids-4-spark_2.12-0.2.0.jar https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/0.2.0/rapids-4-spark_2.12-0.2.0.jar
ENV SPARK_CUDF_JAR=${SPARK_RAPIDS_DIR}/cudf-0.15-cuda11.jar
ENV SPARK_RAPIDS_PLUGIN_JAR=${SPARK_RAPIDS_DIR}/rapids-4-spark_2.12-0.2.0.jar
 
# Set all environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV SPARK_HOME /usr/bin/spark
ENV SPARK_NO_DAEMONIZE true
ENV PYSPARK_DRIVER_PYTHON python3
ENV PYSPARK_PYTHON python3
ENV PATH /usr/bin/spark/bin:/usr/bin/spark/sbin:$PATH
 
WORKDIR ${SPARK_HOME}
\end{lstlisting}


LISTING XY shows the instructions of the Apache Spark base image. The base image installs all requirements needed to run Apache Spark with the RAPIDS accelerator enabled, as listed in \SubSec{subsec:04_rapids_req}. The image uses CUDA as the base image. 


\begin{enumerate}
\item The Apache Spark base image uses CUDA as the base image. It has already CUDA 11.0 and the NVIDIA GPU driver installed.
\item SET SPARK AND HADOOP VERSION
\item First the Ubuntu package repository is being updated. Second all required Ubuntu packages like the JRE 8 will be installed.
\item Apache Spark will be downloaded and installed at \textit{/opts/Spark}.
\item Add the GPU discovery script
\item Download needed \textit{.jar} files for the RAPIDS plugin
\item Set environment variables
\item Set working directory
\end{enumerate}


Each node needs Apache Spark 3.1 installed. A version number of at least 3 is needed to install the NVIDIA RAPIDS plugin (see SECTION AB).


\subsection{Apache Spark Master}


\subsection{Apache Spark Worker}


\subsection{Apache Spark Submit}
- Dockerfile erkl채ren
- Config parameter


\subsection{Building Images}


% ===========================================
% ===========================================
\section{Monitoring-System}


\subsection{cAdvisor}
- Wie ist cAdvisor erreichbar
- Screenshot von cAdvisor 


\subsection{Prometheus}
- Wie ist Prometheu errecihbar
- DIe config von prometheus dateien
- Screenshots von prometheus (target und visualisierung)



%
% WARUM NICHT GPU MIT IN DIE AUTO-SCALER METRICS?
% weil alle worker sich die GPUs teilen ...
%


% ===========================================
% ===========================================
\section{Auto-Scaler}


\subsection{Code Implementation}
- Hier auch Beispiel logs
- Dann scaling heat und KHP algos


\subsection{Configuration}
- Beispiel von der yml Datei


\subsection{Docker Image}
- Dockerfile hier erkl채ren


% ===========================================
% ===========================================
\section{GitLab CI/CD}


\subsection{Automatic Deployment of Apache Spark Applications}
Vielleicht eher das Kapitel so nennen
- gitlab-ci.yml erkl채ren
- Screenshot von webui output


\subsection{Image Registry}
??

































\section{General}
% BLa bla alles in Docker usw.
T create an elastic computing environment, all needed components will be run as Docker container in a Docker network.


\section{Computing environment}
% The environment will be implemented with Docker/docker-compose
To create an elastic environment Docker will be used. With docker Spark worker can be easy scaled. With docker-compose an environment can be created.
% Images used
% network (Communication)
\subsection{Monitoring}
cAdvisor will be uses to monitor metrics from the Docker engine. Prometheus collects the metrics defined in Section XY from cAdvisor and stores them in its time-series database.
% The queries
The query used for getting CPU metrics:

\begin{lstlisting}[language=SQL, caption=Python example]
SUM(SELECT BLA FROM XYZ)
\end{lstlisting}

The query used for getting GPU metrics:


\subsection{Apache Spark Images}
The are 4 different Spark images:

1. Spark-Base
2. Spark-Master
3. Spark-Worker
4. Spark-Submit

Spark-Base is the foundation image for all other Spark images. Spark-Master is the image for the master node. Spark-Worker is the image for all SPark worker nodes. With Spark-Submit, a application will be submitted, the container runs as long as the application runs and exits after the execution automatically. Spark-Submit is used, because Cluster runs in Standalaone mode. Python cann submit cannot run in cluster mode QUELLE SPark.

\subsection{GPU Acceleration}

NVIDIA RAPIDS will be used as the plugin for GPU acceleration.



\section{Auto-Scaler}
The Auto-Scaler consists of a Python module which implements the logic for control-loop and a own Docker image.
% Is implemented in python
The Auto-Scaler is implemented in the Python programming language.
% Is its own Docker image
The Auto-Scaler module will be executed in an individual Docker image.
% UML diagram

\subsection{Configuration}
All configuration properties defined in the concept of the Auto-Scaler in SECTION A will be defined in a YAML file.
% As a cmd-line argument
To set the configuration properties, the configuration file needs to be set as a command-line argument.
% File example

\subsection{Control-Loop}
Control-Loop of the Auto-Scaler implements the Analyze, Plan, and Execute phase of the MAPE architecture.

\subsubsection{Analyze}
% Scaling Heat
Scaling Heat algorithm will be used to estimate if a scaling action is necessary.

\subsubsection{Plan}
% KHPA
The KHPA algorithm will be used to calculate how many worker are needed to reach the target utilization. Since we have two different metrics, the highest number of running worker will be used KUBERNETES QUELLE.

\subsubsection{Execute}
% Python Docker SDK
Via the Python Docker SDK new container will be spawned in the network.
% Check if worker is busy
If worker need to be removed, it is necessary to check if the worker are running any applications at the moment. Spark provides a REST API to check this.
% Cooldown
After a scaling action has been performed, a cooldown period will be applied so all nodes can relax.



\section{Automated Deployment}
Hier die CI/CD von gitlab.
