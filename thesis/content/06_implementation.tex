\chapter{Implementation}
\label{sec:06_implementation}
\todo{Describe Chapter}


% ===========================================
% ===========================================
\section{Computing Environment}


\subsection{Swarm}
Vielleicht euch einfach das ganze kapitel Swarm nennen?
- Dockerfile erl채utern


\subsection{Build Script}


% ===========================================
% ===========================================
\section{Apache Spark Cluster}
The Apache Spark cluster is created in standalone mode, see SECTION AB for details. There is one Apache Spark master node, a variant number of Apache Spark worker nodes and for each running application one Apache Spark Submit node. Each node runs in an independent Docker container. The Apache Spark master and worker nodes run as services in the swarm. Each Apache Spark node container is created from a custom Docker image. To simplify the images, a base image is created where all other images can inherit from.

LISTING XY shows the instructions of the base image. The following requirements are needed for all Apache Spark node images:
\begin{itemize}
\item Use Ubuntu 16.04 as base image
\item \textbf{Install Ubuntu packages:} OpenJDK 8 and Python 3 are needed to run Apache Spark and perform Python applications.
\item Install Apache Spark
\item Add the GPU discovery script
\item Download needed \textit{.jar} files for the RAPIDS plugin
\item Set environment variables
\item Set working directory
\end{itemize}


Each node needs Apache Spark 3.1 installed. A version number of at least 3 is needed to install the NVIDIA RAPIDS plugin (see SECTION AB).


\subsection{Apache Spark Master}


\subsection{Apache Spark Worker}


\subsection{Apache Spark Submit}
- Dockerfile erkl채ren
- Config parameter


\subsection{Building Images}


% ===========================================
% ===========================================
\section{Monitoring-System}


\subsection{cAdvisor}
- Wie ist cAdvisor erreichbar
- Screenshot von cAdvisor 


\subsection{Prometheus}
- Wie ist Prometheu errecihbar
- DIe config von prometheus dateien
- Screenshots von prometheus (target und visualisierung)



%
% WARUM NICHT GPU MIT IN DIE AUTO-SCALER METRICS?
% weil alle worker sich die GPUs teilen ...
%


% ===========================================
% ===========================================
\section{Auto-Scaler}


\subsection{Code Implementation}
- Hier auch Beispiel logs
- Dann scaling heat und KHP algos


\subsection{Configuration}
- Beispiel von der yml Datei


\subsection{Docker Image}
- Dockerfile hier erkl채ren


% ===========================================
% ===========================================
\section{GitLab CI/CD}


\subsection{Automatic Deployment of Apache Spark Applications}
Vielleicht eher das Kapitel so nennen
- gitlab-ci.yml erkl채ren
- Screenshot von webui output


\subsection{Image Registry}
??

































\section{General}
% BLa bla alles in Docker usw.
T create an elastic computing environment, all needed components will be run as Docker container in a Docker network.


\section{Computing environment}
% The environment will be implemented with Docker/docker-compose
To create an elastic environment Docker will be used. With docker Spark worker can be easy scaled. With docker-compose an environment can be created.
% Images used
% network (Communication)
\subsection{Monitoring}
cAdvisor will be uses to monitor metrics from the Docker engine. Prometheus collects the metrics defined in Section XY from cAdvisor and stores them in its time-series database.
% The queries
The query used for getting CPU metrics:

\begin{lstlisting}[language=SQL, caption=Python example]
SUM(SELECT BLA FROM XYZ)
\end{lstlisting}

The query used for getting GPU metrics:


\subsection{Apache Spark Images}
The are 4 different Spark images:

1. Spark-Base
2. Spark-Master
3. Spark-Worker
4. Spark-Submit

Spark-Base is the foundation image for all other Spark images. Spark-Master is the image for the master node. Spark-Worker is the image for all SPark worker nodes. With Spark-Submit, a application will be submitted, the container runs as long as the application runs and exits after the execution automatically. Spark-Submit is used, because Cluster runs in Standalaone mode. Python cann submit cannot run in cluster mode QUELLE SPark.

\subsection{GPU Acceleration}

NVIDIA RAPIDS will be used as the plugin for GPU acceleration.



\section{Auto-Scaler}
The Auto-Scaler consists of a Python module which implements the logic for control-loop and a own Docker image.
% Is implemented in python
The Auto-Scaler is implemented in the Python programming language.
% Is its own Docker image
The Auto-Scaler module will be executed in an individual Docker image.
% UML diagram

\subsection{Configuration}
All configuration properties defined in the concept of the Auto-Scaler in SECTION A will be defined in a YAML file.
% As a cmd-line argument
To set the configuration properties, the configuration file needs to be set as a command-line argument.
% File example

\subsection{Control-Loop}
Control-Loop of the Auto-Scaler implements the Analyze, Plan, and Execute phase of the MAPE architecture.

\subsubsection{Analyze}
% Scaling Heat
Scaling Heat algorithm will be used to estimate if a scaling action is necessary.

\subsubsection{Plan}
% KHPA
The KHPA algorithm will be used to calculate how many worker are needed to reach the target utilization. Since we have two different metrics, the highest number of running worker will be used KUBERNETES QUELLE.

\subsubsection{Execute}
% Python Docker SDK
Via the Python Docker SDK new container will be spawned in the network.
% Check if worker is busy
If worker need to be removed, it is necessary to check if the worker are running any applications at the moment. Spark provides a REST API to check this.
% Cooldown
After a scaling action has been performed, a cooldown period will be applied so all nodes can relax.



\section{Automated Deployment}
Hier die CI/CD von gitlab.
