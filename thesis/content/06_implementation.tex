\chapter{Implementation}
\label{chap:06_implementation}

This chapter explains the implementation process of the conceptual detail introduced in \Chap{chap:05_design}.


% ===========================================
% ===========================================
\section{General}
% Docker swarm
To implement the introduced conceptual design, several requirement on the host machine have to be provided.


\subsection{The Host Machine}
% SHort intro
The host machine is NVIDIA DGX.
% Live system
It is important to mentioned that the machine is a shared live system. Many applications from different departments are performing on the machine. All application are sharing the same resource on the machine.


% Resources
The hardware specification of the machine is the following:
\begin{itemize}
\item 11GB RAM
\item 300 TB DIsk space 
\item 8x NVIDIA Tesla GPU a 32GB
\item 500x Intel CPU COre 29876
\end{itemize}


% Available software
The following software is installed on the machine:
\begin{itemize}
\item Ubuntu 30.5
\item Docker 19.5
\item NVIDIA Docker runtime 1
\end{itemize}


- Docker Swarm 
- Each component as a Docker file to create services
- Nvidia runtime to use GPUs


\subsection{Problem Statement}
% nvidia runtime
To deploy Docker container with access to GPU resources, the NVIDIA runtime for Docker has to be installed.
% What?
The NVIDIA runtime for Docker allows to access GPU resources from inside a Docker container. \todo{besser erklären}
% The problem
The NVIDIA runtime has to be enabled as default runtime for Docker to use it in Docker services.
% How
Just install the following and set it in the file XY.
% Already installed
The NVIDIA runtime 4.x is already installed on the host machine but is not set as the default runtime.
% The problem
Changing the default runtime requires a restart of the Docker service. Restarting the Docker service is not possible because it requires to quit all running application on the system which require Docker. Therefore, components which require access to GPU resources (Apache Spark worker nodes and the dcgm-exporter) cannot be deployed as Docker services.
% The solution
The solution to this problem is, to deploy these components as Docker container. The docker run command provides a runtime attribute to set the NVIDIA runtime as default runtime for this container.
% Example

% Describe example
LST AB provides an example about deploying a Docker container with the NVIDIA runtime enabled.


% Finish
Given this problem statement, the Auto-Scaler has to be implemented to deploy Apache Spark worker Docker container instead of scaling the replicas of the worker service.


%\subsection{Development Steps}
%Vll auch irgendwie ala Scrum etc
%Wasserfall modell ....

%\begin{enumerate}
% 1
%\item Implementing the Auto-Scaler Python module
% 2
%\item Dockerizing the Auto-Scaler
% 3
%\item Creating the computing environment in a Docker swarm
%\begin{itemize}
%\item Create an Apache Spark cluster with GPU acceleration
%\item Create an autonomic manager with a monitoring system and an Auto-Scaler
%\end{itemize}
% 4
%\item Implement a GitLab CI pipeline to deploy Apache Spark applications to the cluster
%\end{enumerate}


% ===========================================
% ===========================================
\section{Auto-Scaler}
% SHort intro 
The Auto-Scaler is a main module of the autonomic manager. It is responsible to analyse performance metrics, plan scaling actions in accordance to the performance metrics and execute scaling actions to adapt the number of Apache Spark worker in the computing environment.
% Mention conceptual design
It will be implement after the conceptual design described in SECTION XY.


\subsection{Technical Background}
% Technical background
The Auto-Scaler is implement in Python 3.8. It consists of different classes, each having different responsibilities.
% A list of the Python libs
The following Python libraries have been used for the implementation:
% The list
\begin{itemize}
\item aiohttp\footnote{\url{https://pypi.org/project/aiohttp/} (Accessed: 2021-01-26)}
\item APScheduler\footnote{\url{https://pypi.org/project/APScheduler/} (Accessed: 2021-01-26)}
\item docker\footnote{\url{https://pypi.org/project/docker/} (Accessed: 2021-01-26)}
\item PyYAML\footnote{\url{https://pypi.org/project/PyYAML/} (Accessed: 2021-01-26)}
\end{itemize}
% Explain libs
To make asynchronous HTTP calls, the aiohttp library was used. Asynchronous HTTP calls are important because the performance metrics have to analysed at runtime.
% apscheduler
The APScheduler is needed to periodically tasks.
% docker
To interact with the Docker engine, Docker provides a Python SDK.
% pyyaml
The configuration for the Auto-Scaler is defined in YAML files. The PyYAML library is used to parse YAML files.


% Auto-Scaler workflow MAPE
%The workflow is the following:
%\begin{enumerate}
%\item Analyse: Fetch performance metrics from the Prometheus API, Next 


%\item Fetch performance metrics from Prometheus
%\item Analyse the performance metrics given specified threshold based rules
%\item Determine if a scaling action is needed using the Scaling-Heat algorithm
%\item Calculate the number of active Apache Spark worker using the KHPA algorithm
%\item Scale the number of Apache Spark worker
%\end{enumerate}

%
% Keine prom alertmanager weil wegen neustart und dann db weg
% Mehr flexibilität, unabhängig von prom
%

%
% WARUM NICHT GPU MIT IN DIE AUTO-SCALER METRICS?
% weil alle worker sich die GPUs teilen ...
%

% Only horizontal
%During this thesis work, the horizontal scaling approach is being used because of its suitability to solve this thesis problems and therefore being %introduced in this section.
% Vertical
%Vertical scaling will not be used due to its limitations explained in \Sec{subsec:02_foundations_scalability_limits}.

\subsection{Configuration}
% Design
\paragraph{}The configuration parameter for the Auto-Scaler have been introduced in \Sec{subsec:05_auto-scaler_configuration}.
% Its yaml
The configuration for the Auto-Scaler will be specified in a YAML file.
% The yaml file
\Lst{lst:06_auto-scaler_config_example} describes an example of an Auto-Scaler configuration.
% Overall structure
Overall, a configuration file is structured in three sections: General, metrics and worker.
% The table
\Tab{table:06_auto-scaler_config_parameter} lists all available configuration parameters. It describes the value type and the default value of each parameter. Some parameters are required to be defined by the administrator and have no default value.

% General
\paragraph{General:}
The general section defines details about the scaling and heat algorithm and the Prometheus URL.


% Metrics
\paragraph{Metrics:}
\todo{Multi-dimensional metrics/queries}
Metrics is a list of performance metrics configuration parameters. The name of a metric (\textit{cpu} and \textit{gpu} in the example \Lst{lst:06_auto-scaler_config_example}) can be set by the administrator. A performance metric requires a query in the PromQL syntax. Additionally a target utilization is needed and the minimum and maximum utilization of the performance metric.


% Worker
\paragraph{Worker:}
To scale the replicas of the Apache Spark worker service, the name of the Docker service needs to be set. In addition, the minimum and maximum number of concurrent worker nodes needs to be defined to prevent an overhead of running worker nodes.


% Example
\begin{lstlisting}[label=lst:06_auto-scaler_config_example, caption=Auto-Scaler configuration YAML file]
general:
  interval_seconds: 5
  cooldown_period_seconds: 180
  recurrence_factor: 3
  prometheus_url: "http://localhost:9090"
 
metrics:
  cpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.5
    thresholds:
      min: 0.2
      max: 0.6
  
  gpu:
    query: 'sum(rate(container_cpu_user_seconds_total{image="spark-worker:3.0.1-hadoop2.7"}[30s]))'
    target_utilization: 0.3
    thresholds:
      min: 0.2
      max: 0.6
 
worker:
  service_name: "computing_spark-worker"
  thresholds:
    min: 1
    max: 30
\end{lstlisting}


% Parameter table
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Name                      & Type    & Default           \\ \midrule
\multicolumn{3}{l}{\textbf{general}}                    \\ \midrule
interval\_seconds         & Integer & 1                 \\
cooldown\_period\_seconds & Integer & 180               \\
recurrence\_factor        & Integer & 1                 \\
prometheus\_url           & String  & \textit{Required} \\
\multicolumn{3}{l}{\textbf{metrics}}                    \\ \midrule
query                     & String  & \textit{Required} \\
target\_utilization       & Float   & 0.5               \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\
\multicolumn{3}{l}{\textbf{worker}}                     \\ \midrule
service\_name             & String  & \textit{Required} \\
\multicolumn{3}{l}{thresholds}                          \\
min                       & Float   & 0.5               \\
max                       & Float   & 0.5               \\ \bottomrule
\end{tabular}
\caption{Auto-Scaler configuration parameter}
\label{table:06_auto-scaler_config_parameter}
\end{table}


\subsection{Scaling Apache Worker Nodes}
% Periodically performing
The Auto-Scaler performs periodically. Each period it fetches performance metrics, analyses the metrics, plans scaling actions, and executes them if necessary.
% Tasks
To perform tasks periodically, it uses the APScheduler library. This allows to perform a tasks each $n$ seconds.
% Explain tasks
In each task, the Analyse, Plan, and Execute phase is performed in order.


\subsubsection{Estimation of Necessary Scaling Actions}

%
% ANALYZE !!!!!
%

The Scaling Heat algorithm, introduced in \Sec{sec:04_scal-heat}, is being used to estimate if a scaling action is necessary.
% Why
The algorithm is being used because it will prevent the Auto-Scaler to perform unnecessary scaling actions.
% For each performance metric
During each interval, after performance metrics have been received from the monitoring system, a heat value will be calculated for each performance metric specified in the configuration under \textit{metrics}.
% Recurrence factor
The algorithm uses a recurrence factor which has to be defined in design time. The Auto-Scaler configuration provides a parameter called \textit{recurrence\_factor} (see \Tab{table:06_auto-scaler_config_parameter} for details). 

% HeatStore
\paragraph{}To store and calculate the heat for each performance metric, a class called \texttt{HeatStore} was created.
% Explain UML
FIGURE XY describes the UML class diagram for the \texttt{HeatStore} Python class.
% For what can it be used
The class can be used to retrieve, update and reset the heat for a list of performance metrics.

% ------

% SHort intro
The Auto-scaler begins with the Analyse phase of the MAPE architecture.
% fetch metrics
First it fetches the performance metrics from the specified Prometheus API.
% How it makes HTTP calls
To make HTTP requests, the Auto-Scaler uses the aiohttp library. This library allow to make asynchronous HTTP requests.
% Why async
Performing asynchronous HTTP calls is important, because this does not block the current thread, the application is running on. This will not stop the loop.
% Performance violations
After receiving the performance metrics, the Auto-Scaler analyses the metric by checking if they violate the defined performance thresholds. If no performance violation has occured, the current task has finished and the Auto-Scaler for the next interval.
% Violation
Otherwise, the Auto-Scaler will perform the Plan phase.
% Conditions
To execute the Plan phase, a set of conditions have to be given:
\begin{enumerate}
\item The Scaling Heat algorithm has to reach the recurrence factor
\item No cooldown period should be activated
\end{enumerate}

% Vll ein http response zeigen


\subsubsection{Calculating the Number of Needed Worker Nodes}

%
% PLAN !!!
%


% Intro KHPA
The KHPA algorithm will be used to calculate how many worker are needed to reach the target utilization (see SECTION XY for algorithm details). 
% More metrics then one
In this project, the calculation is done for the CPU and GPU utilization. The highest number of the desired worker node replicas is chosen.

% --------------

In the Plan phase, the Auto-Scaler calculates the number of needed Apache Spark worker nodes using the KHPA algorithm.
% The python impl
\begin{lstlisting}[label=lst:06_auto-scaler_plan_khpa, caption=KHPA implementation using Python 3.8, language=Python]
def calculate_number_of_needed_worker(active_worker: int,
    utilization: float,
    target_utilization: float):
  return math.ceil(
    active_worker * (utilization / target_utilization))
\end{lstlisting}
% Explain
\Lst{lst:06_auto-scaler_plan_khpa} shows the implementation of the KHPA algorithm in Python.


\subsubsection{Performing a Scaling Action}

%
% EXEVUTE !!!
%


% Python Docker SDK
Docker provides a Python library for the Docker Engine\footnote{Docker SDK for Python 4.4.1 Documentation - \url{https://docker-py.readthedocs.io/en/4.4.1/} (Accessed: 2021-01-05)}. This library will be used to perform the swarm scaling action.


% Check if worker is busy
If worker need to be removed, it is necessary to check if the worker are running any applications. Removing a worker while an application is performing will cause the cancellation of the application.
% How to check if applications are running
To check if applications 


% Cooldown
After a scaling action has been performed, a cooldown period will be applied. The cooldown period is needed because the number of desired worker nodes can keep fluctuating due to the dynamic nature of performance metrics.

% -----------

% short intro
After the number of desired Apache Spark worker has been calculated, the Auto-Scaler adds or removes worker container.
% No romoving while applictions are running
During the execution of Apache Spark applications, it is not possible to remove worker nodes. Because of the bla bla. An alerantive approach is described in SECTION DISCUSSION.
% How ti check if apps are running
In each interval, the Auto-Scaler checks if an application is actively performing. To get this value, the Auto-Scaler uses the Prometheus HTTP API and sends the BBLA BLA PromQL query. Therefore, thr Apache Spark master node has to be defined as Prometheus target (described in SECTION XY).
% Add worker
Docker SDK bla bla


\subsection{Docker Image}
- Dockerfile hier erklären
- Wie den Auto-Scaler per konsole starten (mit config parameter)


% ===========================================
% ===========================================
\section{Deployment of a Docker Swarm}


\subsection{Hardware}
% DGX erklären


\subsection{Software info}
Hier tabelle mit versionen von eingesetzter software


\subsection{Swarm}
Vielleicht euch einfach das ganze kapitel Swarm nennen?
- Dockerfile erläutern
- GPUs (nur die 2 bestimmten)


\subsection{Build Script}


\subsection{Apache Spark Cluster with GPU Acceleration}
% Standalone
\paragraph{}The Apache Spark cluster is created in standalone mode, see \SubSec{subsec:04_spark_standalone} for deployment details.
% Master and worker
The cluster consists of a single Apache Spark master node and a dynamic number of Apache Spark worker nodes. The master and worker container will run as a service in a swarm (see \Sec{subsec:04_docker_swarm}).
% Submit
For submitting an application to the cluster, an individual container performing the spark-submit executable will be deployed.
% Docker
Each node runs in an independent Docker container.


% How many images are needed
\paragraph{}Overall four Docker images are needed to create the Apache Spark cluster introduced in \SubSec{subsec:05_arch_spark}:
\begin{itemize}
\item Base image
\item Master image
\item Worker image
\item Submit image
\end{itemize}
% The base image
The master, worker and submit image require common packages to be installed and a set of common configuration. Therefore, an additional base image will serve as a base image.

\subsubsection{Apache Spark Base Image Installation Details}
\paragraph{}The base image Dockerfile is available at \Lst{lst:appendix_spark_base_dockerfile}.
% Ubuntu base docker image
As parent, the base image uses the \texttt{nvidia/cuda:11.0-devel-ubuntu16.04} Docker image. The parent image runs Ubuntu\footnote{Enterprise Open Source and Linux - \url{https://www.ubuntu.com/} (Accessed: 2021-01-03)} in version 16.04. Additionally the CUDA Toolkit and the NVIDIA GPU driver are already installed.
% Args for spark and hadoop version
Docker provides the ability to set build arguments. To be able to install a specific Apache Spark version, two arguments,  can be set when building the Docker image.
% Apache Spark
Apache Spark will be installed at \texttt{/opts/Spark}.
% Work dir
This directory will be set as the working directory for the Docker image as well.
% Ubuntu packages
Furthermore, required Ubuntu packages will be installed. This includes the Java Runtime Environment Version 8, which is a requirement for Apache Spark.
% RAPIDS
To enable GPU acceleration on all Apache Spark nodes, the base image will install the compiled Java files for the RAPIDS plugin at \texttt{/opt/sparkRapidsPlugin} (for RAPIDS installation requirements, see \SubSec{subsec:04_rapids_req}). The .jar files can be downloaded in the maven repository.
% Discovery script
To enable Apache Spark to discover available GPUs, a GPU discovery script is needed (see \SubSec{subsubsec:04_spark_standalone_res-alloc} for details about resource allocation). This discovery script will be placed at \texttt{/opt/sparkRapidsPlugin} as well. The discovery script is introduced at \Lst{lst:appendix_spark_gpu-discovery}.

%% HETEROGEN


% Building
%\paragraph{}Listing XY demonstrates how the base image can be build 


% Env var table
%The table \ref{table:1} is an example of referenced \LaTeX elements.

%\begin{table}[h!]
%\centering
%\begin{tabular}{||c c||} 
 %\hline
 %Environment variable & Value \\ [0.5ex] 
 %\hline\hline
 %$SPARK\_RAPIDS\_DIR$ & /opt/sparkRapidsPlugin \\ 
 %$SPARK\_CUDF\_JAR$ & opt/sparkRapidsPlugin/cudf-0.15-cuda11.jar \\
 %$SPARK\_RAPIDS\_PLUGIN\_JAR$ & opt/sparkRapidsPlugin/rapids-4-spark_2.12-0.2.0.jar \\
 %$JAVA\_HOME$ & /usr/lib/jvm/java-8-openjdk-amd64 \\
 %$PYSPARK\_PYTHON$ & python3 \\ [1ex] 
 %\hline
%\end{tabular}
%\caption{Table to test captions and labels}
%\label{table:1}
%\end{table}


\subsubsection{Standalone Master and Worker Image}
% Base image
The master and worker image are build on top of the  Apache Spark base image.
% no additional installation
Therefore, no additional installation steps are required.
% standalone
The master and worker nodes will be launched in standalone mode (see \SubSec{subsec:04_spark_standalone} for standalone mode details).


\paragraph{Master image:}
Implementation of the master node Dockerfile is available at \Lst{lst:appendix_spark_master_dockerfile}.
% Ports
The master node image needs two ports configured: The Apache Spark service port and the port for the web user interface.
% web ui port
The Apache Spark service port ist set to 7077 and the web user interface port to 4040. 
% Start in standalone
To start the master node in standalone mode, the start-master.sh launchable will be set as image entrypoint which requires no additional arguments.


\paragraph{Worker image:}
\Lst{lst:appendix_spark_worker_dockerfile} describes the implementation of the worker node Dockerfile.
% web ui port
The port for the worker web interface will be exposed at 4041.
% Port
To start the worker in standalone mode, the start-slave.sh executable will be set as entrypoint for the image.
% Master uri
The launch script requires the master node URI as a parameter. To keep the configuration simple, the environment variable HIER DIE VAR will be set in the compose file.
% Worker configuration
\Lst{lst:06_computing_spark_worker-env} describes the configuration environment for the worker. As mentioned previously (in \Sec{sec:05_restrictions}), for this project two GPUs are available on the DGX workstation. Furthermore, the worker needs to know where to find the GPU resource discovery script.

% Building script
\begin{lstlisting}[label=lst:06_computing_spark_worker-env, caption=Environment configuration for all worker nodes, language=bash]
SPARK_WORKER_OPTS="-Dspark.worker.resource.gpu.amount=2 -Dspark.worker.resource.gpu.discoveryScript=/opt/sparkRapidsPlugin/getGpusResources.sh"
\end{lstlisting}


\subsubsection{Submit Image}
% Pyspark
As mentioned in \Sec{sec:05_restrictions}, a requirements is, that Apache Spark application will be implemented in Python. Accordingly, the pyspark Python module needs to installed on all submit nodes.
% Application workspace
Apache Spark application will be placed at \textit{/opt/spark-apps}. SECTION CI describes how an Apache Spark application will be copied to a submit node.
% Entrypoint
As entrypoint, the image will perform a custom submit script (available at LISTING AB). This script performs the spark-submit executable (usage described in detail in \SubSec{subsubsec:04_spark_standalone_submit}).


\subsection{Autonomic Manager}
As mentioned in \SubSec{subsec:05_arch_am}, the autonomic manager will consist of a monitoring system and the Auto-Scaler to create a complete control loop.

\paragraph{}The monitoring system conceptual design was introduced in SECTION XY.
It consists of a cAdvisor (SECTION XY) service and a Prometheus (SECTION XY) service.
All modules will run as individual Docker services in the overall swarm.
% Docker images
The following DOcker images will be used for the monitoring system:
% List all images
\begin{itemize}
\item \textbf{cAdvisor:} google/cadvisor
\item \textbf{Prometheus:} prom/prometheus
\end{itemize}

\subsubsection{Prometheus Target Configuration}

% ==============
% Hands-On Infrastructure Monitoring with Prometheus -> Comparing CPU usage -> PromQL approach
% ==============



% Pull based
As mentioned in \Sec{sec:04_prom}, Prometheus is a pull-based monitoring tool.
% Target list
It requires a list of targets to pull performance metrics from.


% Describe config
\Lst{lst:06_computing_am_prom-config} specifies the scrape configuration of the Prometheus system.
% cAdvisor target
The cAdvisor service is specified as a target. Prometheus will scrape every 5 seconds performance metrics from cAdvisor. All performance metrics will be labeled with the cAdvisor lable. The cAdvisor service is available at  cadvisor:8080.


% Building script
\begin{lstlisting}[label=lst:06_computing_am_prom-config, caption=Prometheus target configuration in YAML syntax]
scrape_configs:
    - job_name: prometheus
      scrape_interval: 5s
      static_configs:
          - targets: ["localhost:9090"]
 
    - job_name: cadvisor
      scrape_interval: 5s
      static_configs:
          - targets: ["cadvisor:8080"]
            labels:
                group: "cadvisor"
\end{lstlisting}


% ===========================================
% ===========================================
\section{Automatic Deployment of Apache Spark Applications}
Vielleicht eher das Kapitel so nennen
- gitlab-ci.yml erklären
- Screenshot von webui output
