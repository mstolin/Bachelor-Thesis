\chapter{Introduction}
\label{sec:introduction}

\section{Distributed Computing}
% Little intro accorsing to Big Data ML and so on
Machine Learning and Big Data projects consist of a combination of extract-transform-load (ETL) pipelines and compute intensive algorithms to create meaningful informations from massive data sets QULLE DevOps: CON.. .
% Short intro to distributed computing
Performing ETL applications on a single machine, limits the scale and performance of the application. Given this limitation, the idea of distributed computing evolved, where multiple machines with commodity hardware form a cluster to utilize their resources to solve high complex problems \cite{Ganelin2016Spark}.
% Why is distributed computing so useful
% Hier das Zitat von Thomas Davenport, RAPIDS Paper !!!

% Evolution of distributed computing
Several companies utilized the idea of distributed computing to solve some of their business problems. 
% MapReduce
Google developed the MapReduce \cite{Dean2004MapReduce} framework. MapReduce gave the opportunity to solve massive complex problems in parallel on a cluster of single machines.
% Hadoop
Yahoo published an ecosystem platform for distributed computing called Hadoop QUELLE. Hadoop contributed to create a cluster to process massive amounts of data in parallel.
% Apache Spark
Implementing data pipelines requires to chain multiple MapReduce jobs together. This causes a huge amount of writing and reading operation to the disk with bad impact on the overall performance. Another framework called Apache Spark was developed to simplify writing and executing parallel application at scale while keeping the benefits of MapReduce`s scalability and fault-tolerant data processing. Apache Spark provides a performance improve of 10x in iterative Machine Learning algorithms over MapReduce \cite{Zaharia2010Spark} and has evolved as a replacement for MapReduce as the distributed computing framework of choice.


\section{Computing Acceleration with GPUs}
% Why CPUs are limited
Distributed computing frameworks like Apache Spark perform applications on a huge amount of CPU cores to enable parallization. A CPU is build of multiple cores which are optimized for sequential serial processing QUELLE. Performing computationally intensive applications on an Apache Spark cluster, consumes a huge amount of CPU cycles with bad impact on the overall performance QUELLE HETEROSPARK.
% About the growing complexity of computational power
To handle the complexity of Big Data applications, from executing Machine Learning algorithms or training Deep Loearning models, a remaining option of distributed computing clusters is ti scale-up individual nodes. Scaling-up is limited by resource capacity and can be become uneconomically at a specific point.
% What about GPUs
To perform computationaly complex applications with better performance, GPUs have become first class citizens in modern data centers. The architecture of a Graphical Process Unit (GPU) consists of a huge amount of smaller and more efficient cores which are optimized for parallel data processing (handling multiple tasks simoultaneously). In general, GPUs process data at a much faster rate than CPUs are capable.
% How can apps be accelerated with GPUs
Leveraging GPUs for distributed computing frameworks like Apache Spark, can boost the overall performance of performing complex algorithms on large datasets.


\section{Auto-Scaling}
% Short intro to auto-scaling
Adjusting the resources in a computing environment is not an easy task. To do it manually, a system administrator needs deep knowledge about hte environment and has to watch performance spikes regualarly. This is a resource wasting process. In an optimal way, an automized process would watch the computing enviroment, analyze perofmrance metrcis and automatically add or remove resource to optimize the performance and cost. This process is called an Auto-Scaler.


% Why is auto-scaling needed
Hiring experts to manually watching an application and scaling an computing environment is a waste of reources and cost.
% Benefits of auto-scaling
AN Auto-Scaler takes care of watching the environment and adding and removing resources to the computing needs. An Auto-Scaler can be configured to take care of optimal resource allocation and keeping the cost of running low.


% How can auto-scaling be achieved (Horizontal and vertical)
Auto-scaling a computing environment can be achieved with two different scaling approaches: Vertical-scaling and horizontal-scaling.
% Vertical scaling
Vertical scaling refers to adjusting the hardware resources of an individual node in the environment. Hardware adjustments can include adding (scale-up) or removing (scale-down) resources like memory or CPU cores \cite{Wilder2012CloudPatterns}. By adding more powerful resources to a node, a node can take more throughput and perform more specialized tasks \cite{Abbott2015ScalabilityArt}.
% Horizontal scaling
Adjusting the nodes in a computing environment is referred to horizontal scaling \cite{Wilder2012CloudPatterns}. Increasing the number of nodes in an environment, increases the overall computing capacity and in addition, the workload can be distributed across all nodes \cite{Wilder2012CloudPatterns, Abbott2015ScalabilityArt}.
% Not exclusive
Both scaling approaches are not exclusive. A computing environment can be designed to scale vertically, horizontally or both \cite{Wilder2012CloudPatterns}.
% Why horizontal scaling is preferred
Vertical scaling is limited by the maximum hardware capacity. In addition, a point can be reached where more powerful hardware resources become unaffordable or are not available \cite{Abbot2011ScalabilityRules}.  Therefore, horizontal scaling is the preferred approach to enable auto-scaling.


\section{DevOps}
% What is DevOps
DevOps (Development plus Operations) is way that automates process during the development life cycle of a product. Development phases can be testing, monitoring or deployment \cite{Vadapalli2018DevOps}. 
% What benefits has the automation
The automation of development processes leads to an overall improvement of development. It reduces human errors and the downtime for upgrades as well benefits to the improvement of productivity \cite{Vadapalli2018DevOps}.


% DevOps and BigData/ML projects
Machine Learning and Big Data Projects benefits from DevOps \cite{Vadapalli2018DevOps}.


% Intergration of DevOps
To integrate DevOps methodologies into a projects, an optimized strategy is key. The strategy needs to provides automation for each aspect of the projects life cycle. Therefore, it needs to integrate the correct tools, platforms and technologies \cite{Vadapalli2018DevOps}.
% Which purpose does it serve
An integration of DevOps methodologis into the life cycle of a software project can serve several purposes \cite{Vadapalli2018DevOps}:
\begin{itemize}
\item Automating the management of the infrastructure and configuration
\item Automating testing and build workflows of source ode repositories
\item Continuous integration and deployment
\item Virtualization and containirazion
\end{itemize}


\section{Problem Statement}
\subsection{Performance}
% Workload muss effizient verteil werden
For complex machine learning applications working with massive datasets, it is necessary to distribute the workload to enable parallel execution. Therefore, a cluster has to be created to enable nodes to distribute their workload. Apache Spark will be used to create such a cluster.
% GPUs sollen genutzt werden um Performance zu optimieren
Apache Spark distributes the workload on multiple CPU cores on each node to enable parallelism. Machine Learning algorithms benefits from GPUs which enable data parallelism. In this context, the cluster needs to be extendend with GPUs and Apache Spark needs be aware how to distribute workload on GPUs as well.
% Ein problem is, wie GPUs im CLuster verbinden -> Siehe RAPIDS paper für Formulierung
Since 


\subsection{Auto-Scaling}
% Das env muss automatisiert überwacht und analysiert werden
Performing complex Machine Learning algorithms on large datasets requires a huge amount of resources. Theos resources need to be monitored frequently to be able to detect performance spikes. If performance spikes are detected, the system needs to calculate how many nodes to add to keeping the performance on a normal level.
% Nodes müssen automatisch und dynamisch sklaiert werden um performance anspruch gerecht zu werden





%% Struktur
% Möglichkeit IT resourcen automatisch nach bedarf zu skalieren
% Viele ETL operation sind daten intensiv von natur, GPU kann helfen

%To prevent a breakdown of the IT environment while complex operations are being computed, the IT environment should have the ability to scale IT resources dynamically
%when performance peaks and fluctuations occur.

% Dynamisch auf performance spitzen reagieren
%ETL \footnote{Extract transform load} operations are compute-intensive. During the execution of analytic applications, performance thresholds can be reached and
%the computing system can become out-of-order. 

%In addition, many algorithms profit from data-parallelism. % Das ha mehr mit SPark zu tun

%If performance thresholds are reached and the IT environment can`t adapt 
%dynamically to the usage demand, the environment can look temporarily out-of-order. 
%During the execution of operations, performance threshold can be reached and leads to 

% Mit GPU ETLs beschleunigen
%In addition, algorithms do profit from data-parallelism. 

\section{Research Questions and Research Objective}
%!!
% Goals is to create a self-adapting and self-healing system
%!!
% Whats the goal of this thesis
The goal of this thesis is to implement a distributed computing environment to perform machine learning applications. The environment should be able to scale itself automatically in accordance to the workload caused by performing the Machine Learning applications. In addition, the Machine Learning application should be performed automatically in a DevOps process. To address the goal of this research work, the following two research question will be investigated:


%RQs
\begin{enumerate}
\item Is it possible to extend the number of Spark Worker in accordance to performance?
\item Can we extend Apache Spark to accelerate execution with GPU support?
\end{enumerate}


% RQ1
The first research question searches for the possibilities to create an elastic Apache Spark cluster. This question also builds the foundation of the computing environment used for this thesis. To answer this question, state-of-the-art computing architectures have to be investigated. During the investigation, tools have to be evaluated that enable a simple deployment of Apache Spark worker instances and monitoring the performance metrics of the computing environment. In Addition, horizontal auto-scaling approaches that fit into the computing environment have to investigated.


% RQ2
For the second research questions, approaches need to be investigate to extend Apache Spark to perform operation on GPUs. As mentioned in SEC X, Apache Spark only knows about CPUs. The aim to answer this question, is to find a way to add GPUs to the computing cluster and make them available for Apache Spark.


\section{Thesis Structure}

\section{Research Methodologies}
% Read state of the art literature about scaling Apache Spark (elasticity)
The first step is to read state-of-the-art literature. Literature about elastic computing architectures needs to be investigated to compare different tools for monitoring an environment and deploying computing nodes on demand.
% How to enable GPU acceleration for Apache Spark
In addition, literature about extending an Apache Spark cluster with GPU support needs to be investigated.
% Classify literature
The collected literature needs to be classified into different categories. First all architecture related paper should be summarized, second all literature about horizontal auto-scaling needs to be summarized and last all literature about GPU support for Apache Spark needs to be summarized.
% Implementing The (Docker environment)
After gaining knowledge about state-of-the-art technologies and approaches, the Apache Spark cluster needs to be implemented.
% Impementing an auto-scaler
After an Apache Spark cluster is available, the Auto-Scaler has to be implemented.
% Testing
If the Auto-Scaler is available, it needs to be tested.
% Enabling GPU acceleration
GPU support has to bee added to the cluster.
% Evaluation
With GPU support, the scalable environment can be evaluated with Machine Learning algorithms.
