\chapter{Introduction}
\label{sec:01_introduction}


% ===========================================
% ===========================================
\section{Distributed Computing}
% Little intro accorsing to Big Data ML and so on
Machine Learning and Big Data projects consist of a combination of extract-transform-load (ETL) pipelines and compute intensive algorithms to create meaningful informations from massive data sets QULLE DevOps: CON.. .
% Short intro to distributed computing
Performing ETL applications on a single machine, limits the scale and performance of the application. Given this limitation, the idea of distributed computing evolved, where multiple machines with commodity hardware form a cluster to utilize their resources to solve high complex problems \cite{Ganelin2016Spark}.
% Why is distributed computing so useful
% Hier das Zitat von Thomas Davenport, RAPIDS Paper !!!

% Evolution of distributed computing
Several companies utilized the idea of distributed computing to solve some of their business problems. 
% MapReduce
Google developed the MapReduce \cite{Dean2004MapReduce} framework. MapReduce gave the opportunity to solve massive complex problems in parallel on a cluster of single machines.
% Hadoop
Yahoo published an ecosystem platform for distributed computing called Hadoop QUELLE. Hadoop contributed to create a cluster to process massive amounts of data in parallel.
% Apache Spark
Implementing data pipelines requires to chain multiple MapReduce jobs together. This causes a huge amount of writing and reading operation to the disk with bad impact on the overall performance. Another framework called Apache Spark was developed to simplify writing and executing parallel application at scale while keeping the benefits of MapReduce`s scalability and fault-tolerant data processing. Apache Spark provides a performance improve of 10x in iterative Machine Learning algorithms over MapReduce \cite{Zaharia2010Spark} and has evolved as a replacement for MapReduce as the distributed computing framework of choice.


% ===========================================
% ===========================================
\section{Computing Acceleration with GPUs}
% Why CPUs are limited
Distributed computing frameworks like Apache Spark perform applications on a huge amount of CPU cores to enable parallization. A CPU is build of multiple cores which are optimized for sequential serial processing QUELLE. Performing computationally intensive applications on an Apache Spark cluster, consumes a huge amount of CPU cycles with bad impact on the overall performance QUELLE HETEROSPARK.
% About the growing complexity of computational power
To handle the complexity of Big Data applications, from executing Machine Learning algorithms or training Deep Loearning models, a remaining option of distributed computing clusters is ti scale-up individual nodes. Scaling-up is limited by resource capacity and can be become uneconomically at a specific point.
% What about GPUs
To perform computationally complex applications with better performance, GPUs have become first class citizens in modern data centers. The architecture of a Graphical Process Unit (GPU) consists of a huge amount of smaller and more efficient cores which are optimized for parallel data processing (handling multiple tasks simultaneously). In general, GPUs process data at a much faster rate than CPUs are capable.
% How can apps be accelerated with GPUs
Leveraging GPUs for distributed computing frameworks like Apache Spark, can boost the overall performance of performing complex algorithms on large datasets.


% ===========================================
% ===========================================
\section{Auto-Scaling}
% Short intro to auto-scaling
Adjusting the resources in a computing environment is not an easy task. To do it manually, a system administrator needs deep knowledge about the environment and has to watch performance spikes regularly. This is a resource wasting process. In an optimal way, an automized process would watch the computing environment, analyze performance metrics and automatically add or remove resource to optimize the performance and cost. This process is called auto-scaling.


% Why is auto-scaling needed
Hiring experts to manually watching an application and scaling an computing environment is a waste of resources and cost.
% Benefits of auto-scaling
An Auto-Scaler takes care of watching the environment by adding and removing resources to adapt to the computing needs. The Auto-Scaler can be configured to take care of optimal resource allocation and keeping the cost of running low.


% How can auto-scaling be achieved (Horizontal and vertical)
Auto-scaling a computing environment can be achieved with two different scaling approaches: Vertical-scaling and horizontal-scaling.
% Vertical scaling
Vertical scaling refers to adjusting the hardware resources of an individual node in the environment. Hardware adjustments can include adding (scale-up) or removing (scale-down) resources like memory or CPU cores \cite{Wilder2012CloudPatterns}. By adding more powerful resources to a node, a node can take more throughput and perform more specialized tasks \cite{Abbott2015ScalabilityArt}.
% Horizontal scaling
Adjusting the nodes in a computing environment is referred as horizontal scaling \cite{Wilder2012CloudPatterns}. Increasing the number of nodes in an environment, increases the overall computing capacity and in addition, the workload can be distributed across all nodes \cite{Wilder2012CloudPatterns, Abbott2015ScalabilityArt}.
% Not exclusive
Both scaling approaches are not exclusive. A computing environment can be designed to scale vertically, horizontally or both \cite{Wilder2012CloudPatterns}.
% Why horizontal scaling is preferred
Vertical scaling is limited by the maximum hardware capacity. Furthermore, a point can be reached where more powerful hardware resources become unaffordable or are not available \cite{Abbot2011ScalabilityRules}.  Therefore, horizontal scaling is the preferred approach to enable auto-scaling.


% ===========================================
% ===========================================
\section{Automated Deployment Pipeline}
% Whats the problem
Building, testing and releasing software manually is a time-consuming and error-prone process.
% Overcoming this issue
To overcome this issue, a pattern called deployment pipeline automates the build, test, deploy and release processes of an applications development cycle.
% How does it work
The concept of deployment pipelines is based on automation scripts which will be performed on every changes on an applications source code, environment, data or configuration \cite{Farley2010CI}.
% Goals of the deployment pipeline
A fully automated deployment pipeline has many improvements over deploying applications manually:
% The goals
\begin{itemize}
\item Makes every process until release visible to all developers \cite{Farley2010CI}
\item Errors can be identified and resolved at an early stage \cite{Farley2010CI}
\item The ability to deploy and release any version of an application to any environment \cite{Farley2010CI}
\item A non automated deployment process is not repeatable and reliable \cite{Farley2010CI}
\item The automation scripts can serve as documentation \cite{Farley2010CI}
\item If an application has been deployed manually, there is no guarantee that the documentation has been followed \cite{Farley2010CI}
\end{itemize}
% CI
The automated deployment pipeline is based on the Continuous Integration (CI) process. Furthermore, the deployment pipeline is the logical implementation of CI \cite{Farley2010CI}.
% How is it used nowadays
Nowadays, 


% ===========================================
% ===========================================
\section{Research Objective and Research Questions}
% Fraunhofer
The thesis work be implemented for the Cyber Cognitive Intelligence department at Fraunhofer IPA\footnote{Fraunhofer Institute for Manufacturing Engineering and Automation IPA - \url{https://www.ipa.fraunhofer.de/} (Accessed: 2021-01-07)}.
% Apache Spark cluster
At the IPA, developers deploy ML applications using Apache Spark.
% Goal
Executing these applications should be accelerated using GPUs on the Apache Spark cluster.
% Scaling
If the point is reached where the performance utilization of the Apache Spark cluster is too high, the environment should adapt to the computing needs and scale the Apache Spark cluster by adding more workers.
% Deployment
In addition, developers should have the ability to submit an application to the Apache Spark cluster automatically which will be triggered by pushing changes to the code base of the application.


%Rqs
To address the goal of this thesis, the following three research question will be investigated:
%RQs
\begin{itemize}
\item \textbf{RQ1:} Is it possible to scale the number of Apache Spark Worker in accordance to performance utilization?
\item \textbf{RQ2:} How can Apache Spark be extended to accelerate application execution with GPU support?
\item \textbf{RQ3:} Is it possible to automate the deployment process of applications to a running Apache Spark cluster?
\end{itemize}


% RQ1
\paragraph{}
% Self-adapting
The first research question searches for concepts to create a self-adapting computing environment.
% How to answer this questions
To answer this question, state-of-the-art computing architectures have to be investigated.
% Evaluating tools
Monitoring tools to collect performance metrics need to evaluated. Additionally, tools which enable fast deployment of computing units.
% Scaling
Furthermore, a suitable scaling approach has to be investigated.


% RQ2
\paragraph{}
% Main goal
The main goal of this research question is enable Apache Spark to perform algorithms on the GPU instead of the CPU.
% Tools
Therefore, a concept needs to be investigated to extend Apache Spark to use GPUs for suitable algorithms in addition to the CPUs.


%RQ3
\paragraph{}
% More applied
The last research question has a more applied nature.
% Automating
Automating the development cycle of an application is a well investigated topic.
% Try and error
The IPA is using a platform called GitLab (will be introduced in \Sec{sec:04_background_gitlab}) which provides an API to build automated pipelines.
% Anser
To answer this research question, GitLabs functionality will be investigated to find a solution that fits the need of this project work.


% ===========================================
% ===========================================
\section{Problem Statement}
\label{sec:01_introduction_problem}
%
% Hier wegen Fraunhofer
%

% Short intro
The previously mentioned concepts distributed computing, GPU acceleration, auto-scaling and automated deployment are each trying to solve a specific problem.
% Which problems
In this work, these concepts will be used 
Given the previously introduced RQs and the research objective of this thesis, the problems that need to be solved using the concepts are the following:
% The problems
\begin{enumerate}
\item Create a dynamic environment
\item Accelerating the computing performance of the environment
\item Automating the deployment process to save time and resources
\end{enumerate}

\subsection{Performance}
% Workload muss effizient verteil werden
For complex machine learning applications working with massive datasets, it is necessary to distribute the workload to enable parallel execution. Therefore, a cluster has to be created to enable nodes to distribute their workload. Apache Spark will be used to create such a cluster.
% GPUs sollen genutzt werden um Performance zu optimieren
Apache Spark distributes the workload on multiple CPU cores on each node to enable parallelism. Machine Learning algorithms benefits from GPUs which enable data parallelism. In this context, the cluster needs to be extendend with GPUs and Apache Spark needs be aware how to distribute workload on GPUs as well.
% Ein problem is, wie GPUs im CLuster verbinden -> Siehe RAPIDS paper für Formulierung
Since 


\subsection{Dynamic Computing Environment}
% Needs huge amounts of computing resources
Performing complex Machine Learning algorithms on large datasets requires a huge amount of computing resources.
% The environment needs to adapt
The computing environment should be able to automatically scale to meet an increasing demand of application computing resources.


\subsection{Automated Deployment}
% Time and resource wasting





%% Struktur
% Möglichkeit IT resourcen automatisch nach bedarf zu skalieren
% Viele ETL operation sind daten intensiv von natur, GPU kann helfen

%To prevent a breakdown of the IT environment while complex operations are being computed, the IT environment should have the ability to scale IT resources dynamically
%when performance peaks and fluctuations occur.

% Dynamisch auf performance spitzen reagieren
%ETL \footnote{Extract transform load} operations are compute-intensive. During the execution of analytic applications, performance thresholds can be reached and
%the computing system can become out-of-order. 

%In addition, many algorithms profit from data-parallelism. % Das ha mehr mit SPark zu tun

%If performance thresholds are reached and the IT environment can`t adapt 
%dynamically to the usage demand, the environment can look temporarily out-of-order. 
%During the execution of operations, performance threshold can be reached and leads to 

% Mit GPU ETLs beschleunigen
%In addition, algorithms do profit from data-parallelism. 


% ===========================================
% ===========================================
\section{Thesis Structure}
% 02 Theoretical Foundation
\Chap{chap:02_foundation} provides the theoretical foundation about concepts which have been introduced in this chapter.
% 03 Related Work
\Chap{chap:03_related-work} focuses on related work which provides solutions to solve the given problems of this thesis introduced in \Sec{sec:01_introduction_problem}.
% 04 Technical Background
In \Chap{chap:04_background} all technologies which are being used to implement the objective of this thesis are being introduced.
% 05 Design
Afterwards in \Chap{chap:05_design}, a conceptual design of a dynamic computing environment and an automated deployment pipeline is being described.
% 06 Implementation
\Chap{chap:06_implementation} describes the implementation of the computing environment and how the deployment pipeline is being used to automate the deployment of applications to the computing environment.
% 07 Evaluation
In \Chap{chap:07_evaluation} the results of the implementation are being presented and analysed.
% 08 Outlook
\Chap{chap:08_outlook} introduces further work, which has been discovered during the work of this thesis, as well as improvements for the implementation.
% 09 Conclusion
Finally \Chap{chap:09_conclusion} ...


% ===========================================
% ===========================================
\section{Research Methodologies}
% Read state of the art literature about scaling Apache Spark (elasticity)
The first step is to read state-of-the-art literature. Literature about elastic computing architectures needs to be investigated to compare different tools for monitoring an environment and deploying computing nodes on demand.
% How to enable GPU acceleration for Apache Spark
In addition, literature about extending an Apache Spark cluster with GPU support needs to be investigated.
% Classify literature
The collected literature needs to be classified into different categories. First all architecture related paper should be summarized, second all literature about horizontal auto-scaling needs to be summarized and last all literature about GPU support for Apache Spark needs to be summarized.
% Implementing The (Docker environment)
After gaining knowledge about state-of-the-art technologies and approaches, the Apache Spark cluster needs to be implemented.
% Impementing an auto-scaler
After an Apache Spark cluster is available, the Auto-Scaler has to be implemented.
% Testing
If the Auto-Scaler is available, it needs to be tested.
% Enabling GPU acceleration
GPU support has to bee added to the cluster.
% Evaluation
With GPU support, the scalable environment can be evaluated with Machine Learning algorithms.
