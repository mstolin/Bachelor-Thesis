\chapter{Introduction}
\label{chap:01_introduction}

% Concepts
This chapter introduces the background and problem statement of this thesis.
% research and problems
Next, the research objective and the research questions are described.
% structure
Finally, the thesis structure is explained.


% ===========================================
% ===========================================
\section{Background}
% Little intro accorsing to Big Data ML and so on
Machine Learning (\hyperlink{abbr:ml}{ML}) and Big Data projects consist of combining Extract-Transform-Load (\hyperlink{abbr:etl}{ETL}) pipelines and compute-intensive algorithms to create meaningful information from large datasets \cite{Vadapalli2018DevOps}.
% Problem with large datasets
To meet the demands of compute-intensive applications, the underlying environment needs high scalability, which can be improved by distributing the workload in parallel across multiple nodes.
% Distributed computing
This approach is referred to as distributed computing, which allows executing applications in parallel on a cluster of commodity hardware \cite{Ganelin2016Spark}. A distributed cluster can be scaled in accordance with the applications demands by adding or removing nodes. Increasing the number of nodes in an environment increases the overall computing capacity \cite{Wilder2012CloudPatterns, Abbott2015ScalabilityArt}.


% Intro of tools
Various tools and frameworks such as MapReduce\cite{Dean2004MapReduce}, Apache Hadoop\footnote{Apache Hadoop - \url{https://hadoop.apache.org/} (Accessed: 2020-01-08)}, and Apache Spark\cite{Zaharia2010Spark} have been created to facilitate distributed computing.
% MapReduce
The MapReduce framework allows solving massive complex problems in parallel on a cluster of single machines.
% Hadoop
Apache Hadoop is an ecosystem platform for distributed computing. It contributes to creating a cluster to process massive amounts of data in parallel by implementing the MapReduce processing framework \cite{Khattak2016BigData}.
% Apache Spark
Implementing data pipelines with MapReduce requires chaining multiple MapReduce jobs together. This causes a huge amount of writing and reading operations to the disk, with a bad impact on the overall performance. The Apache Spark framework was developed to simplify writing and executing parallel applications at scale while keeping the benefits of MapReduce's scalability and fault-tolerant data processing. Apache Spark provides a performance improvement of 10x in iterative Machine Learning algorithms over MapReduce \cite{Zaharia2010Spark} and has evolved as a replacement for MapReduce as the distributed computing framework of choice.
% Spark
To enable parallelism, Apache Spark allows performing applications on a huge amount of Central Processing Unit (\hyperlink{abbr:cpu}{CPU}) cores. A \hyperlink{abbr:cpu}{CPU} is build of multiple cores that are optimized for sequential serial processing. Performing computationally intensive applications on an Apache Spark cluster consumes many \hyperlink{abbr:cpu}{CPU} cycles with a negative impact on the overall performance \cite{Li2015HeteroSpark}.
% About the growing complexity of computational power
To handle the complexity of Big Data applications, from executing Machine Learning algorithms or training Deep Learning models, distributed computing clusters are used to scale-up individual nodes. Scaling-up is limited by resource capacity and can be become uneconomically at a specific point.


% What about GPUs
Graphical Process Units (\hyperlink{abbr:gpu}{GPUs}) have become first class citizens in modern data centers to perform computationally complex applications with better performance. A \hyperlink{abbr:gpu}{GPU} architecture consists of many smaller and more efficient cores, which are suitable for data-parallel processing (handling multiple tasks simultaneously) \cite{Yuan2016SparkGPU}. In general, \hyperlink{abbr:gpu}{GPUs} process data at a much faster rate than \hyperlink{abbr:cpu}{CPUs} are capable.
% How can apps be accelerated with GPUs
Apache Spark applications have a data-parallel nature. Therefore, enabling Apache Spark to leverage \hyperlink{abbr:gpu}{GPUs} to perform complex \hyperlink{abbr:ml}{ML} algorithms on big datasets positively impacts the performance \cite{Yuan2016SparkGPU}.


% Short intro to auto-scaling
Adjusting the resources in a computing environment is not an easy task. To do it manually, a system administrator needs deep knowledge about the environment and regularly watches performance spikes. This is a resource-wasting process. Optimally, an automised process would watch the computing environment, analyse performance metrics and automatically add or remove resources to optimize the performance and cost of running. This process is called auto-scaling.
% Benefits of auto-scaling
An \textit{Auto-Scaler} takes care of watching the environment by adding and removing resources to adapt to the computing needs and can be configured to take care of optimal resource allocation to keep the cost of running at a low point.


% Automation
Building, testing, and releasing software manually is a time-consuming and error-prone process. Additionally, a non-automated application development cycle is not repeatable nor reliable.
% Overcoming this issue
To overcome this issue, a pattern called deployment pipeline automates the build, test, deploy, and release processes of an application development cycle.
% How does it work
By integrating a fully automated deployment pipeline, an application development cycle can be simplified, and errors can be identified and resolved at an early stage \cite{Farley2010CI}.


% ===========================================
% ===========================================
\section{Problem Statement}
\label{sec:01_introduction_problem}
% Intro
This thesis provides a solution to the following three problem statements:

\begin{enumerate}
% Problem 1
\item
The performance of Apache Spark can be optimized using \hyperlink{abbr:gpu}{GPUs}.
However, Apache Spark has no built-in support to enable \hyperlink{abbr:gpu}{GPU}-acceleration.

% Problem 2
\item
To enable \hyperlink{abbr:gpu}{GPU} acceleration for Apache Spark alone is not sufficient to increase the performance.
At some point, an Apache Spark cluster can reach the limit of its available computing resources.
If this point is reached, the environment should automatically scale the number of Apache Spark workers to increase the computing capacity.

% Problem 3
\item
To execute an Apache Spark application on the cluster, developers must submit the application manually.
With an automated deployment pipeline, developers can submit an application by pushing changes to the code base.
Additionally, a deployment pipeline will contribute to the reliability of executing applications and reduces the development time.
\end{enumerate}


% ===========================================
% ===========================================
\section{Research Objective and Research Questions}
% 1.
\hyperlink{abbr:cpu}{CPUs} and \hyperlink{abbr:gpu}{GPU} resources have only been combined to a limited extent in order to optimize Machine Learning model training. Therefore, a Spark cluster prototype is created, which automatically allocates resources depending on the load and thus scales itself. Furthermore, the Apache Spark cluster can leverage \hyperlink{abbr:gpu}{GPUs} in order to increase its performance.
% 2.
Additionally, the Machine Learning model training process is further simplified with an automated deployment pipeline.
% RQs
This results in the following research questions:
%RQs
\begin{itemize}
\item RQ1: Is it possible to scale the number of Apache Spark Workers according to performance utilization?
\item RQ2: How can Apache Spark be extended to accelerate application execution with GPU support?
\item RQ3: Is it possible to automate the deployment process of Apache Spark applications?
\end{itemize}


% RQ1
\paragraph{}
% Self-adapting
The first research question searches for concepts to create a self-adapting computing environment.
% How to answer this questions
To answer this question, state-of-the-art computing architectures have to be investigated.
% Evaluating tools
Monitoring tools to collect performance metrics need to evaluated. Additionally, tools that enable fast deployment of computing units.
% Scaling
Furthermore, a suitable scaling approach has to be investigated.


% RQ2
\paragraph{}
% Main goal
The main goal of the second research question is to enable Apache Spark to perform algorithms with \hyperlink{abbr:gpu}{GPU} acceleration included.
% Tools
Therefore, a concept needs to be investigated to extend Apache Spark to use \hyperlink{abbr:gpu}{GPUs} for suitable algorithms in addition to the available \hyperlink{abbr:cpu}{CPU}.


%RQ3
\paragraph{}
% More applied
The last research question has a more applied nature.
% Automating
Automating the development cycle of an application is a well-investigated topic.
% Try and error
The IPA uses a platform called GitLab (introduced in \Sec{sec:04_background_gitlab}), which provides an Application Programming Interface (\hyperlink{abbr:api}{API}) to build automated pipelines.
% Anser
To answer this research question, GitLabs functionality will be investigated to find a solution that fits the need of this project work.


% ===========================================
% ===========================================
\section{Thesis Structure}
% 02 Theoretical Foundation
\Chap{chap:02_foundation} provides the theoretical foundation about concepts that have been introduced in this chapter.
% 03 Related Work
\Chap{chap:03_related-work} focuses on related work that provides solutions to solve this thesis's given problems introduced in \Sec{sec:01_introduction_problem}.
% 04 Technical Background
In \Chap{chap:04_background}, all used technologies to implement the objective of this thesis are being introduced.
% 05 Design
Afterward, in \Chap{chap:05_design}, a conceptual design of a dynamic computing environment and an automated deployment pipeline is being described.
% 06 Implementation
\Chap{chap:06_implementation} contains the implementation of the computing environment and how the deployment pipeline is being used to automate the deployment of applications to the computing environment.
% 07 Evaluation
In \Chap{chap:07_evaluation}, the results of the implementation are being presented and analysed.
% 08 Outlook
Finally, \Chap{chap:08_conclusion-outlook} concludes this thesis and introduces further work, which has been discovered during this thesis's work, as well as improvements for the implementation.
