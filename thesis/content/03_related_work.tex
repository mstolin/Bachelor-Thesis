\chapter{Related Work}
\label{chap:03_related-work}


This chapter provides an overview of related literature for this thesis. Furthermore, the surveyed literature is built on the theoretical foundation introduced in \Chap{chap:02_foundation}. This chapter introduces work about auto-scaling computing environments, GPU accelerated Apache Spark cluster, and the implementation of an automated deployment pipeline. These topics are related to the choice of technologies (\Chap{chap:04_background}), the proposed conceptual design of this thesis (\Chap{chap:05_design}), and the resulting implementation (\Chap{chap:06_implementation}).


% ===========================================
% ===========================================
\section{Auto-Scaling Computing Environments}
% State of the art monitoring
In recent years, container technologies have been used efficiently in complex computing environments. Dynamic scaling of containerized applications is an active area of research.
% WHich is important for this thesis
To accommodate this thesis research objective, the literature research according to auto-scaling environments was focused on two topics: Concepts of \textit{Auto-Scalers}  and auto-scaling algorithms.


\subsection{Auto-Scaler Concepts}
% A review of auto-scaling techniques for elastic application in cloud environments
\paragraph{}
Lorido-Botrán et al. \cite{Lorido2014Review} reviewed state-of-the-art literature about auto-scaling and explain auto-scaling process proposals in a cloud environment.
% SLA and costs
It is mentioned that an \textit{Auto-Scaler} is responsible to find a trade-off between meeting the Service Level Agreement (\hyperlink{abbr:sla}{SLA}) and keeping the cost of renting resources low.
% SLA
Two types of \hyperlink{abbr:sla}{SLA} exists while maintaining an acceptable trade-off: The application \hyperlink{abbr:sla}{SLA} and the resource \hyperlink{abbr:sla}{SLA}. The former is a contract between the application owner and the end-users (e.g., a certain response time). The resource \hyperlink{abbr:sla}{SLA} is agreed upon by the infrastructure provider and the application owner (e.g., 99.9\% availability).
% Auto-scaling problems
They introduced three problems \textit{Auto-Scaler} faces while scaling an environment and meeting the \hyperlink{abbr:sla}{SLA}:
% The problems
\begin{enumerate}
%1
\item Under-provisioning:
An application is under-provisioned if it needs more resources to process the incoming workload.
To make resources available and return the application to its normal state may take some time, which causes \hyperlink{abbr:sla}{SLA} violations.
% 2
\item Over-provisioning:
Applications with more resources available than needed will lead to unnecessary costs for the client.
% 3
\item Oscillation:
If scaling-actions are executed too quickly before the impact is available, a combination of over-provisioned and under-provisioned applications can occur.
A cooldown period after a scaling-action allows preventing oscillation.
\end{enumerate}
% MAPE
To prevent the mentioned problems from occurring, the authors introduced and explained the \hyperlink{abbr:mape}{MAPE} architecture (cf. \Sec{sec:02_ac}).
% MAPE phases
\hyperlink{abbr:mape}{MAPE} consists of four different phases: Monitor, Analyse, Plan, and Execute.
% Only AP
There exist \textit{Auto-Scaler} proposals which only focus on the Analyse, and Planning phase architecture of the \hyperlink{abbr:mape}{MAPE} architecture.
% Analysing and planning techniques
Several techniques for the Analyse phase are being introduced: Queuing theory and time-series analysis.
As well as for the planning phase: Threshold-based rules, reinforcement learning, and control theory.
% Active proactive grouping
There exist \textit{Auto-Scaler} which uses techniques to predict the future state of the environment (e.g., reinforcement learning). These are called proactive \textit{Auto-Scalers}.
Reactive \textit{Auto-Scalers} use techniques to respond to the environment's current status (e.g., threshold-based rules).


% Application deployment using containers with auto-scaling for microservices in cloud environment
\paragraph{}
Srirama et al. \cite{Srirama2020AppDeplyCont} designed a heuristic-based auto-scaling strategy for container-based microservices in a cloud environment. The purpose of the auto-scaling strategy is to balance the overall resource utilization across microservices in the environment.
% Results
The proposed auto-scaling strategy performed better than state-of-the-art algorithms in processing time, processing cost, and resource utilization. The processing cost of microservices was reduced by 12-20\%. The \hyperlink{abbr:cpu}{CPU} and memory utilization of cloud-servers were maximized by 9-15\% and 10-18\%, respectively.


% Comparison of auto-scaling techniques for cloud environments
\paragraph{}
Lorido-Botrán et al.  \cite{Botran2013AutoScalingComp} compared different representative auto-scaling techniques in a simulation in terms of cost and \hyperlink{abbr:sla}{SLA} violations. They compared load balancing with static threshold-based rules, reactive and proactive techniques based on \hyperlink{abbr:cpu}{CPU} load.
Load balancing is based on static rules defining the upper and lower thresholds of a specific load (e.g., \textit{if CPU > 80\% then scale-out; if CPU < 20\% then scale-in}). The difficulty of this technique is to set the ideal rules. False rules can lead to bad performance. Proactive techniques try to predict the future values of performance metrics based on historical data. Reactive techniques are based on control theory to automate the system management. To overcome the difficulties of static thresholds, the authors proposed a new auto-scaling technique using rules with dynamic thresholds. The results showed that for auto-scaling techniques to scale well, it highly depends on parameter tuning. The best result was achieved with proactive results with a minimum threshold of 20\% and a maximum of 60\%.


\subsection{Auto-Scaling Algorithms}
% Delivering elastic ...
\paragraph{}Barna et al. \cite{Barna2017ElasticContainerApps} proposed an autonomic scaling architecture approach for containerized microservices. Their approach focused on creating an autonomic management system, following the autonomic computing concept \cite{Kephart2003VisionComputing}, using a self-tuning performance model. The demonstrated architecture frequently monitors the environment and gathers performance metrics from components. It can analyze the data and dynamically scale components. In addition, to determine if a scaling action is needed, they proposed the \textit{Scaling Heat Algorithm}. The Scaling Heat algorithm is used to prevent unnecessary scaling actions, which can throw the environment temporarily off.
% Is ised in this thesis
The Scaling Heat algorithm will be used for decision making in this thesis and is explained in detail in \Sec{sec:04_background_scaling-heat}.


% Auto-scaling of Containers: the Impact of Relative and Absolute Metrics
\paragraph{}Casalicchio et al. \cite{Casalicchio2017AutoScaleCont} focused on the difference of absolute and relative metrics for container-based auto-scaling algorithms. They analysed the mechanism of the \textit{Kubernetes Horizontal Pod Auto-Scaling} (\hyperlink{abbr:khpa}{KHPA}) algorithm and proposed a new auto-scaling algorithm based on \hyperlink{abbr:khpa}{KHPA} using absolute metrics called \textit{KHPA-A}. The results showed that KHPA-A could reduce response time between 0.5x and 0.66x compared to \hyperlink{abbr:khpa}{KHPA}. In addition, their work proposed an architecture using cAdvisor for collecting container performance metrics, Prometheus for monitoring, alerting, storing time-series data, and Grafana for visualizing metrics. 
% KHPA
Absolute metrics are more appropriate when it comes to efficient resource allocation. Therefore, the KHPA-A algorithm is more efficient in vertical scaling of resources.
% This thesis
In this thesis, the focus for scaling strategies is based on the horizontal scaling approach. Therefore, the \hyperlink{abbr:khpa}{KHPA} algorithm will be used throughout this thesis and is explained in detail in \Sec{sec:04_background_khpa}.


% ===========================================
% ===========================================
\section{GPU accelerated Apache Spark Cluster}
% Short intro
This thesis aims to enable \hyperlink{abbr:gpu}{GPU} acceleration for Apache Spark.
% Research
In research, many solutions have been proposed which try to solve the problem in similar ways.
% Following
In the following, three different approaches are introduced.


% HeteroSpark
\paragraph{}
Li et al. \cite{Li2015HeteroSpark} developed a middleware framework called \textit{HeteroSpark} to enable \hyperlink{abbr:gpu}{GPU} acceleration on Apache Spark worker nodes. HeteroSpark listens for function calls in Apache Spark applications and invokes the \hyperlink{abbr:gpu}{GPU} kernel for acceleration. For communication between \hyperlink{abbr:cpu}{CPU} and \hyperlink{abbr:gpu}{GPU}, HeteroSpark implements a \hyperlink{abbr:cpu}{CPU}-\hyperlink{abbr:gpu}{GPU} communication layer for each worker node using the Java Remote Method Invocation (\hyperlink{abbr:rmi}{RMI}) \hyperlink{abbr:api}{API}. To execute operations on the \hyperlink{abbr:gpu}{GPU}, the \hyperlink{abbr:cpu}{CPU} Java Virtual Machine (\hyperlink{abbr:jvm}{JVM}) will send serialized data to the \hyperlink{abbr:gpu}{GPU} JVM using the \hyperlink{abbr:rmi}{RMI} communication interface. The \hyperlink{abbr:gpu}{GPU} \hyperlink{abbr:jvm}{JVM} will deserialize the received data for execution.
% Design
The design provides a plug-n-play approach and an \hyperlink{abbr:api}{API} for the user to call functions with \hyperlink{abbr:gpu}{GPU} support.
% Results
Overall, HeteroSpark is able to achieve an 18x speed-up for various Machine Learning applications running on Apache Spark.


% HetSpark
\paragraph{}
Klodjan et al. \cite{Klodjan2018HetSpark} introduced \textit{HetSpark}, a modification of Apache Spark.
% The goal
HetSpark extends Apache Spark with two executors, a \hyperlink{abbr:gpu}{GPU} accelerated executor and a commodity class. 
% accelerated
The accelerated executor uses VineTalk\cite{Mavridis2017VineTalk} for \hyperlink{abbr:gpu}{GPU} acceleration.
% VineTalk
VineTalk contributes as a transport layer between the application and accelerator devices (\hyperlink{abbr:cpu}{CPU} or \hyperlink{abbr:gpu}{GPU}).
% Byte code analysis
To detect suitable tasks for \hyperlink{abbr:gpu}{GPU} acceleration, HetSpark uses the ASM\footnote{ASM - \url{https://asm.ow2.io/} (Accessed: 2021-01-11)} framework to analyse the byte code of Java binaries.
% Results
The authors observed that for compute-intensive tasks, \hyperlink{abbr:gpu}{GPU} accelerated executers are preferable, while for linear tasks, \hyperlink{abbr:cpu}{CPU}-only accelerators should be used.


% Spark-GPU
\paragraph{}
% SHort intro
Yuan et al. \cite{Yuan2016SparkGPU} proposed \textit{SparkGPU} a \hyperlink{abbr:cpu}{CPU}-\hyperlink{abbr:gpu}{GPU} hybrid system built on top of Apache Spark.
% The goal
The goal of SparkGPU is to utilize \hyperlink{abbr:gpu}{GPUs} to achieve high performance and throughput.
% Problems trying to solve
SparkGPU tries to solve the following problems statements:
% The problems
\begin{enumerate}
% Iterator
\item The iterator model of Apache Spark executes one element at a time.
This approach does not match the \hyperlink{abbr:gpu}{GPU} architecture and underutilizes \hyperlink{abbr:gpu}{GPU} resources.

% JVM
\item Apache Spark runs on the \hyperlink{abbr:jvm}{JVM} and therefore stores its data on the heap memory.
\hyperlink{abbr:gpu}{GPU} programs are usually implemented with \hyperlink{abbr:gpu}{GPU} programming models like Compute Unified Device Architecture (\hyperlink{abbr:cuda}{CUDA}), which cannot access data on the heap.
Therefore, data must be copied between the heap and native memory frequently. These copy operations are expensive.

% Cluster manager
\item Existing cluster manager of Apache Spark manage \hyperlink{abbr:gpu}{GPUs} in a coarse-grained fashion.
This can lead to crashes because of insufficient memory when multiple programs run on the \hyperlink{abbr:gpu}{GPU} concurrently.
\end{enumerate}
% Extension
To solve the mentioned problem statements, SparkGPU extends Apache Spark in the following ways:
\begin{itemize}
\item Enable block processing on \hyperlink{abbr:gpu}{GPUs} by extending the Apache Sparks iterator model. Therefore Apache Spark can better utilize \hyperlink{abbr:gpu}{GPUs} to accelerate application performance.

\item To offload queries to the \hyperlink{abbr:gpu}{GPU}, SparkGPU extends the query optimizer. The query optimizer will create query plans with both \hyperlink{abbr:cpu}{CPU} and \hyperlink{abbr:gpu}{GPU} operators.

\item To manage \hyperlink{abbr:gpu}{GPUs} efficiently, SparkGPU extends the cluster manager and the task scheduler.
\end{itemize}
% Intro GPU-RDD
To extend the programming \hyperlink{abbr:api}{API}, SparkGPU provides a new Resilient Distributed Dataset (\hyperlink{abbr:rdd}{RDD}) type called GPU-RDD.
% OPtimized for GPU
A GPU-RDD is optimized to utilize the \hyperlink{abbr:gpu}{GPU}.
SparkGPU utilizes native memory on the \hyperlink{abbr:gpu}{GPU} instead of the Java heap to buffer data in GPU-RDDs.
% Operations
Operations performed on a GPU-RDD can be performed on the \hyperlink{abbr:gpu}{GPU}. Several built-in operators on the GPU-RDD are provided, which support data-parallelism.


% The query optimizer
SparkGPU can execute Structured Query Language (\hyperlink{abbr:sql}{SQL}) queries on both \hyperlink{abbr:cpu}{CPU} and \hyperlink{abbr:gpu}{GPU}.
By adding a set of \hyperlink{abbr:gpu}{GPU} rules and strategies, SparkGPU extends the query optimizer to find the best execution plan for \hyperlink{abbr:gpu}{GPU} scheduling.

% Cluster manager
To manage \hyperlink{abbr:gpu}{GPU} memory on shared \hyperlink{abbr:gpu}{GPUs}, SparkGPU provides a user-level \hyperlink{abbr:gpu}{GPU}-management library.
% Mem contention
When memory contention happens, the library will ensure that SparkGPU will stop scheduling new tasks to the Apache Spark cluster.

% End
SparkGPU accomplished to improve the performance of machine learning algorithms up to 16.13x and \hyperlink{abbr:sql}{SQL} query execution performance up to 4.83x.


% ===========================================
% ===========================================
\section{Implementation of an Automated Deployment Pipeline}
Implementing an automated deployment pipeline is a more applied topic and well described in many literatures. In this sections the primary literature used throughout the implementation of this thesis work is being introduced.


% continuous delivery
\paragraph{}The conceptual design and implementation of an automated deployment pipeline in this thesis were mostly inspired by the proposed solution of the book \textit{Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation} by Humble et al. \cite{Farley2010CI}.
% Short intro about the content of this book
The authors explain the theoretical idea behind an automated deployment pipeline and explaining an example implementation.
% Implementation
The proposed implementation covers the software lifecycle from compiling source code to delivering the software to a production environment.
The commit stage, which covers the build and test part of the software, can be applied in parts for this thesis work.
