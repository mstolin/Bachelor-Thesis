\chapter{Conceptual Design}
\label{sec:design}
%
\todo{Describe Chapter}


% ===========================================
% ===========================================
\section{Design Restrictions}
The design of the comuting environment will be restricted by several factors.

\begin{enumerate}
\item Apache Spark for distributed computing
\item Docker for containerization
\item NVIDIA GPUs (Running on a DGX)
\item Python applications
\item Homogeneous nodes (worker)
\item Worker auto-scaling
\end{enumerate}

% ===========================================
% ===========================================
\section{CI/CD}


% ===========================================
% ===========================================
\section{Identification of suitable Metrics}
% Welche metrics gibt es
% Welche metrics werden gesammelt (nur von den Worker)
% Wie werden die metrics berechnet
% Welche metriken sind suitable

% Which metrics are suitable
To determine if the Apache Spark needs to be scaled, suitable performance metrics needs to be identified. 
% CPU
As described in SECTION XY, Apache SPark performs the workload on CPU cores. Therefore, CPU utilization is a suitable metric to look out for scaling.
% GPU
As mentioned in SECTION XY, the computing environment will run on a DGX server with multiple GPUs. Each Apache Spark worker will get a GPU to accelerate its applications. Therefore, GPU utilization will be a suitable metric.


\subsection{CPU Performance}
% Evtl ist hier der faösche platz -> Besser eine Metrics section wo erklärt wird, wie das berechnet werden soll.
\todo{Hier auf Tabelle verweisen (Anhang) Metriken die von Prometheus + cAdvisor bereitgestellt werden}

% How is the CPU Utilization being calculated
To adapt to business needs, the CPU percentage of each Spark Worker will be calculated. Prometheus provides several metrics to calculate the CPU percentage. The CPU percentage of all Worker can be calculated as follows:

\begin{equation}
CPUUtilization = \dfrac{\sum SparkWorkerCPUUtilization}{NumberOfSparkWorker}
\label{eq:formel}
\end{equation}

\subsection{GPU Performance}
\begin{equation}
GPUUtilization = \dfrac{\sum SparkWorkerGPUUtilization}{NumberOfSparkWorker}
\label{eq:formel}
\end{equation}


% ===========================================
% ===========================================
\section{Computing environment Architecture}


\subsection{Requirements}
1. self-healing
2. self-adapting
3. distributed computing
4. monitoring


\subsection{Overall}

% Figure
\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{images/05_conceptual_design/cluster_architecture/overall_architecture}
\caption{Overall cluster architecture - Source: Authors own model.}
\label{fig:ca-overall_architecture}
\end{figure}

\Fig{ig:ca-overall_architecture} illustrates the overall architecture with all components of the computing environment. The two main components in the environment are an Apache Spark cluster and an autonomic manager.
% The components
The autonomic manager will be implemented according to the MAPE architecture (SECTION XY). It is responsible for monitoring and auto-scaling the Apache Spark cluster. To enable distributed computing, an Apache Spark cluster will be set up to execute machine learning applications. 
% The nodes and virtualization
The autonomic manager and the Apache Spark cluster consist each of multiple nodes. Each node of a component will run as a Docker container.
% Self healing
To fullfill the requirements of a self-healing system, Docker Swarm (SECTION AB) will be used to orchestrate all nodes in the system and make sure each is healthy.


\subsection{Apache Spark Cluster}


\subsubsection{Master and Worker}
The Apache Spark cluster will consist of a Spark master node and a dynamic number of Spark worker nodes.
% Master
The Spark master node is responsible to distribute the application workload across available Spark worker nodes.
% Worker
A Spark worker node will execute the workload given by the master node. Each Spark worker is homogeneous. 
% Homegeneous spark worker
Homegenneosity is important to scale the number of worker nodes. To enable homogeneous nodes, each SPark worker node is a Docker container running the same Docker image. In addition, each worker is given the same computing resources. With homogeneous Spark worker nodes, each worker will respond as all other nodes.
% GPU acceleration
To enable GPU acceleration, each WORKER/MASTER will have the RAPIDS plugin installed.
% Standalone mode
The cluster will be deployed in standalone mode. To be able to run Python applications


\subsubsection{Spark Submit}
Because the Apache Spark cluster ill be executed in standalone mode, a node inside the cluster is required to run Spark applications. When a Spark application will be executed, a Spark Submit container will be deployed in the cluster. When the appliccation has finished, the container will be automatically removed.
Each app will be executed by a unique Spark Submit node.
The Spark Submit node will be deployed via the CI pipeline (SECTION XY). The prupose of this 


\subsection{Autonomic Manager}
% Auto scaling
% self adapting
% monitoring system (database, cadvisor, metrics, aggregation, ....)

\begin{figure}[h]
\centering
\includegraphics[scale=0.85]{images/05_conceptual_design/autonomic_manager/autonomic_manager_overview}
\caption{Autonomic manager component design - Source: Authors own model.}
\label{fig:am-design-component}
\end{figure}

% MAPE architecture
The design of the autonomic manager needs to fullfill all requirements given by the MAPE architecture (SECTION XY). It will be responsible to monitor the performance of Apache Spark worker nodes in the environment, analyze the metrics and plan and execute scaling actions in accordance to fullfill the performance goals.
% Figure
As illustrated in \Fig{fig:am-design-component}, the autonomic manager consists of a cAdvisor, Prometheus and Auto-Scaler node. Together, all three nodes build a complete autonomic manager in accordance to the MAPE architecture. Each node will run as a Docker container.
% Explain figure
For monitoring, cAdvisor collects metrics from all available Docker container in the computing environment. Prometheus pulls metrics from cAdvsior for streage. The Auto-Scaler will fetch metrics from Prometheus and send scaling instructions to the Docker engine.


\subsubsection{Monitoring System}
% bEsser nochmal erläutern
To create a monitoring system that fullfills the requirements (SECTION AB), cAdvisor and Prometheus will be used.
% Why cadvisor and why prometheus


\subsubsection{Workflow}

\paragraph{} The workflow of the autonomic manager is implemented as a loop.

% Workflow
\begin{figure}[h]
\centering
\includegraphics[scale=0.50]{images/05_conceptual_design/autonomic_manager/autonomic_manager_workflow}
\caption{UML activity model of the autonomic manager process - Source: Authors own model.}
\label{fig:am-workflow}
\end{figure}
% Explain figure
\paragraph{} \Fig{fig:am-workflow} illustrates all steps of each component of the autonomic manager process according to the MAPE architecture.


% Monitor
The workflow starts with collecting metrics in the monitor phase. cAdvisor is responsible to collect metrics from the Docker engine. After that, Prometheus stores the collected metrics as time-series data in its database.
% Analyze
Next, in the analyze phase, the Auto-Scaler needs to determine if a scaling action is needed. If scaling the Apache Spark worker nodes is not needed, the process has finished and will repeat from the beginning in the next period.
% Plan
If the performance is over- or under-utilized, a scaling action is needed. Then, the Auto-Scaler needs to determine how many Apache Spark worker nodes are needed to reach the performance goal.
% Execute
Lastly, the Auto-Scaler is responsible to send the scaling instructions to the Docker engine.


% ===========================================
% ===========================================
\section{Auto-Scaler}
% What is the Auto-Scaler
The Auto-Scaler is a component of the the Autonomic Manager and is responsible for the Analyze, Plan and Execution phase.
% Complete autonomic manager
Together with cAdvisor and Prometheus, the Auto-Scaler builds a complete Autonomic Manager according to the architecture demonstrated in \SubSec{subsec:background-autonomic_computing-autonomic_manager}.
% controll loop
In addition, the Auto-Scaler implements the control-loop which is responsible to make adjustments in the environment.
% Communication
Since Prometheus collects and stores available metrics from cAdvisor,  the Auto-Scaler has not to communicate with cAdvisor.

% HIER NOCH BILD VON SCALER WIE MIT DER MAPE ARCH


\subsection{Configuration}
\label{subsec:design-auto_scaler-configuration}
The Auto-Scaler needs specific configuration properties to be able to collect the correct metrics from Prometheus and deploy new Apache Spark worker container in the environment. The following are properties that have to be defined to ensure that the Auto-Scaler is able to collect meaningful metrics and scale Apache Spark worker as expected.

\subsubsection{General properties}

\begin{itemize}
\item \textbf{Interval seconds:} The number of seconds when the loop has to repeat needs to be defined.

\item \textbf{Cooldown period:} The duration in seconds,  the Auto-Scaler has to wait after a scaling action was performed.

\item \textbf{Recurrence factor:} To prevent to many scaling actions,  the autonomic manager should only execute a scaling action,  if the utilization thresholds is violated \textit{n} times.

\item \textbf{Prometheus URL:} The Auto-Scaler will fetch the configured metrics from the Prometheus REST API.
\end{itemize}

\subsubsection{Metrics}

To support to analyze multiple metrics, the user should be able to create a dynamic list if metrics. Each metric needs to have a variety of properties configured.

\begin{itemize}
\item \textbf{Target utilization:} The relative target utilization of a metrics needs to be defined to calculate the number of Spark worker to add or to remove to reach the defined goal.

\item \textbf{Utilization thresholds:} To determine if a scaling action is needed, the scaling heat algorithm needs the minimum and maximum utilization defined by an administrator.

\item \textbf{Query:} A PromQL query needs to be defined to collect the metric for all Spark Worker.
\end{itemize}

\subsubsection{Apache Spark worker properties}

\begin{itemize}
\item \textbf{Worker image:} To guarantee that each Spark worker is homogeneous, all worker container should be created with the same image.

\item \textbf{Worker network:} To establish communication between all Spark worker and the Spark master, all new Spark worker container should be in the same network.

\item \textbf{Worker thresholds:} The minimum and maximum number of concurrent Spark worker should be defined. To avoid the cold start effect, the minimum amount of worker should be 1. 

\item \textbf{Apache Spark master URI:} To distribute the workload across all Spark Worker, all Spark Worker need to communicate with the Spark master.
\end{itemize}


\subsection{Analyze}
% Short intro to analyze phase
In order to determine if a scaling-action is necessary, the Auto-Scaler has to process the collected metrics. 
% How to analyze
During each period, the Auto-scaler queries the Prometheus time-series database with the configured queries to get all needed metrics. 
% Determine if scaling is needed
After the metrics are received, the Auto-Scaler determines if a scaling action is needed using the Scaling Heat algorithm (introduced in Section AB). If scaling is not necessary, the Auto-Scaler continues to collect metrics from Prometheus.


\subsection{Plan}
% Short intro
If a scaling-action is necessary, the Auto-Scaler is responsible to plan how to scale the number of Spark worker to satisfy the defined utilization goals.
% What is a scaling plan
A scaling plan consists of instructions to add or remove Spark worker which will be send to the Docker engine.
% Calculating Spark worker
To calculate the number of Spark worker, needed to accomplish the defined target utilization, the Auto-Scaler uses the \textit{Kubernetes Horizontal Pod Auto-Scaling} algorithm. In addition, the Auto-Scaler needs to check if the estimated number of Spark worker fall bellow the minimum threshold or exceed the maximum threshold of concurrent Spark worker.


\subsection{Execute}
% Short intro
After a scaling plan has been created, the Auto-Scaler needs to send the instructions to the Docker engine.
% Cooldown
After scaling the environment, it needs time for changes to take effect. Therefore a cooldown period will be activated after each scaling action.
% Whats happens
During the cooldown period, no scaling actions will be forwarded to the Docker engine.
