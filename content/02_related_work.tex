\chapter{Related Work}
\label{sec:related}
%

This chapter provides background information about ...
\todo{Describe Chapter}


% ===========================================
% ===========================================
% Scalable container
% ===========================================
% ===========================================
\section{Scalable container architectures}
% State of the art monitoring
In recent years, container technologies have been used efficiently in complex IT environments. Dynamic scaling of containerized applications is an active area of research. The studied research can be divided in two parts. First, Barna et al. \cite{Barna2017ElasticContainerApps} primarily focused on a design for an auto-scaling architecture of containers with state-of-the-art technologies. Second, Srirama et al. \citep{Srirama2020AppDeplyCont} researched auto-scaling algorithm and policies for efficient resource utilization and cost reducing.


% Delivering elastic ...
Barna et al. \cite{Barna2017ElasticContainerApps} proposed an autonomic scaling architecture approach for containerized microservices. Their approach focused on creating an autonomic management system, following the autonomic computing concept \cite{Kephart2003VisionComputing}, using a self-tuning performance model. The demonstrated architecture frequently monitors the environment and gathers performance metrics from components. It has the ability to analyze the data and dynamically scale components. In addition, to determine if a scaling action is needed, they proposed the \textit{Scaling Heat Algorithm}. The Scaling Heat Algorithm is used to prevent unnecessary scaling actions, which can throw the environment temporarily off.


% Application deployment using containers with auto-scaling for microservices in cloud environment
Srirama et al. \citep{Srirama2020AppDeplyCont} designed a heuristic-based auto-scaling strategy for container-based microservices in a cloud environment. The purpose of the auto-scaling strategy was to balance the overall resource utilization across microservices in the environment.
% Results
The proposed auto-scaling strategy performed better results than state-of-the-art algorithms in processing time, processing cost and resource utilization. The processing cost of microservices could be reduced by 12-20\% and the CPU and memory utilization of cloud-servers have been maximized by 9-15\% and 10-18\%.


% Auto-scaling of Containers: the Impact of Relative and Absolute Metrics
Casalicchio et al. \cite{Casalicchio2017AutoScaleCont} focused on the difference of absolute and relative metrics for container-based auto-scaling algorithms. They analysed the mechanism of the \textit{Kubernetes Horizontal Pod Auto-scaling} (KHPA) algorithm and proposed a new auto-scaling algorithm based on KHPA using absolute metrics called \textit{KHPA-A}. The results showed, that KHPA-A can reduce response time between 0.5x and 0.66x compared to KHPA. In addition, their work proposed an architecture using cAdvisor for collecting container performance metrics, Prometheus for monitoring, alerting and storing time-series data and Grafana for visualizing metrics.


% ===========================================
% ===========================================
% GPU Stuff
% ===========================================
% ===========================================
\section{Heterogenous GPU aware Spark systems}
% Why is GPU needed
Apache Spark is a computing framework that distributes tasks between CPU cores. Data and compute intensive applications profit from GPU acceleration. Therefore, various research projects took effort to bring GPU acceleration to Apache Spark.


% HeteroSpark
Li et al. \cite{Li2015HeteroSpark} developed a middleware framework called \textit{HeteroSpark} to enable GPU acceleration on Apache Spark worker nodes. HeteroSpark listens for function calls in Spark applications and invokes the GPU kernel for acceleration. For communication between CPU and GPU, HeteroSpark uses the Java RMI\footnote{Java Remote Method Invocation} API to send data from the CPU JVM to the GPU JVM for execution.
% Design
The design provides a plug-n-play approach and an API for the user to call functions with GPU support.
% Results
Overall, HeteroSpark is able to achieve a 18x speed-up for various Machine Learning applications running on Apache Spark.


% HetSpark
Klodjan et al. \cite{Klodjan2018HetSpark} introduced HetSpark a heterogeneous modification of Apache Spark.
% The goal
HetSpark extends Apache Spark with two executors, a GPU accelerated executor and a commodity class. The GPU accelerated executor is based on VineTalk\cite{Mavridis2017VineTalk} for GPU acceleration.
% Results
The authors observed, that for compute intensive tasks GPU accelerated executers are preferable while for linear tasks CPU-only accelerators should be used.


% Spark-GPU
Yuan et al. \cite{Yuan2016SparkGPU} proposed SparkGPU to enable parallel processing with GPUs in Apache Spark and contributes to achieve high performance and high throughput in Apache Spark applications.
SparkGPU extends Apache Sparks to determine the suitability of parallel-processing for a task to enable task scheduling between CPU and GPU. 
SparkGPU accomplished to improve the performance of machine learning algorithms up to 16.13x and SQL query execution performance up to 4.83x.


